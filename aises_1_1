<!-- Overview of Catastrophic AI Risks -->

</head>
<body>
<p>— title: Introduction —</p>
<h1 id="introduction">Introduction</h1>
<p>The world as we know it is not normal. We take for granted that we
can talk instantaneously with people thousands of miles away, fly to the
other side of the world in less than a day, and access vast mountains of
accumulated knowledge on devices we carry around in our pockets. These
realities seemed far-fetched decades ago, and would have been
inconceivable to people living centuries ago. The ways we live, work,
travel, and communicate have only been possible for a tiny fraction of
human history.<br />
Yet, when we look at the bigger picture, a broader pattern emerges:
accelerating development. Hundreds of thousands of years elapsed between
the time Homo sapiens appeared on Earth and the agricultural revolution.
Then, thousands of years passed before the industrial revolution. Now,
just centuries later, the artificial intelligence (AI) revolution is
beginning. The march of history is not constant—it is rapidly
accelerating.<br />
</p>
<figure id="fig:gwp">
<embed src="images/overview of catastrophic risks/gwp.pdf" />
<figcaption>World production has grown rapidly over the course of human
history. AI could further this trend, catapulting humanity into a new
period of unprecedented change.</figcaption>
</figure>
<p>We can capture this trend quantitatively in , which shows how
estimated gross world product has changed over time <span
class="citation" data-cites="Roodman2020OnTP Davidson2021">(Roodman
2020; Davidson 2021)</span>. The hyperbolic growth it depicts might be
explained by the fact that, as technology advances, the rate of
technological advancement also tends to increase. Empowered with new
technologies, people can innovate faster than they could before. Thus,
the gap in time between each landmark development narrows.<br />
It is the rapid pace of development, as much as the sophistication of
our technology, that makes the present day an unprecedented time in
human history. We have reached a point where technological advancements
can transform the world beyond recognition within a human lifetime. For
example, people who have lived through the creation of the internet can
remember a time when our now digitally-connected world would have seemed
like science fiction.<br />
From a historical perspective, it appears possible that the same amount
of development could now be condensed in an even shorter timeframe. We
might not be certain that this will occur, but neither can we rule it
out. We therefore wonder: what new technology might usher in the next
big acceleration? In light of recent advances, AI seems an increasingly
plausible candidate. Perhaps, as AI continues to become more powerful,
it could lead to a qualitative shift in the world, more profound than
any we have experienced so far. It could be the most impactful period in
history, though it could also be the last.<br />
Although technological advancement has often improved people’s lives, we
ought to remember that, as our technology grows in power, so too does
its destructive potential. Consider the invention of nuclear weapons.
Last century, for the first time in our species’ history, humanity
possessed the ability to destroy itself, and the world suddenly became
much more fragile.<br />
Our newfound vulnerability revealed itself in unnerving clarity during
the Cold War. On a Saturday in October 1962, the Cuban Missile Crisis
was cascading out of control. US warships enforcing the blockade of Cuba
detected a Soviet submarine and attempted to force it to the surface by
dropping low-explosive depth charges. The submarine was out of radio
contact, and its crew had no idea whether World War III had already
begun. A broken ventilator raised the temperature up to <span
class="math inline">140<sup>∘</sup></span>F in some parts of the
submarine, causing crew members to fall unconscious as depth charges
exploded nearby.<br />
</p>
<figure id="fig:splash">
<embed src="figures/splash.pdf" />
<figcaption>In this chapter we cover four categories of AI risks and
discuss how to mitigate them.</figcaption>
</figure>
<p>The submarine carried a nuclear-armed torpedo, which required consent
from both the captain and political officer to launch. Both provided it.
On any other submarine in Cuban waters that day, that torpedo would have
launched—and a nuclear third world war may have followed. Fortunately, a
man named Vasili Arkhipov was also on the submarine. Arkhipov was the
commander of the entire flotilla and by sheer luck happened to be on
that particular submarine. He talked the captain down from his rage,
convincing him to await further orders from Moscow. He averted a nuclear
war and saved millions or billions of lives—and possibly civilization
itself.</p>
<p>Carl Sagan once observed, “If we continue to accumulate only power
and not wisdom, we will surely destroy ourselves” <span class="citation"
data-cites="sagan1994pale">(Sagan 1994)</span>. Sagan was correct: The
power of nuclear weapons was not one we were ready for. Overall, it has
been luck rather than wisdom that has saved humanity from nuclear
annihilation, with multiple recorded instances of a single individual
preventing a full-scale nuclear war.<br />
AI is now poised to become a powerful technology with destructive
potential similar to nuclear weapons. We do not want to repeat the Cuban
Missile Crisis. We do not want to slide toward a moment of peril where
our survival hinges on luck rather than the ability to use this
technology wisely. Instead, we need to work proactively to mitigate the
risks it poses. This necessitates a better understanding of what could
go wrong and what to do about it.<br />
Luckily, AI systems are not yet advanced enough to contribute to every
risk we discuss. But that is cold comfort in a time when AI development
is advancing at an unprecedented and unpredictable rate. We consider
risks arising from both present-day AIs and AIs that are likely to exist
in the near future. It is possible that if we wait for more advanced
systems to be developed before taking action, it may be too late.<br />
In this chapter, we will explore various ways in which powerful AIs
could bring about catastrophic events with devastating consequences for
vast numbers of people. We will also discuss how AIs could present
existential risks—catastrophes from which humanity would be unable to
recover. The most obvious such risk is extinction, but there are other
outcomes, such as creating a permanent dystopian society, which would
also constitute an existential catastrophe. We outline many possible
catastrophes, some of which are more likely than others and some of
which are mutually incompatible with each other. This approach is
motivated by the principles of risk management. We prioritize asking
“what could go wrong?” rather than reactively waiting for catastrophes
to occur. This proactive mindset enables us to anticipate and mitigate
catastrophic risks before it’s too late.<br />
To help orient the discussion, we decompose catastrophic risks from AIs
into four risk sources that warrant intervention:</p>
<ul>
<li><p><strong>Malicious use</strong>: Malicious actors using AIs to
cause large-scale devastation.</p></li>
<li><p><strong>AI race</strong>: Competitive pressures that could drive
us to deploy AIs in unsafe ways, despite this being in no one’s best
interest.</p></li>
<li><p><strong>Organizational risks</strong>: Accidents arising from the
complexity of AIs and the organizations developing them.</p></li>
<li><p><strong>Rogue AIs</strong>: The problem of controlling a
technology more intelligent than we are.</p></li>
</ul>
<p>These four sections—, , , and —describe causes of AI risks that are
<em>intentional</em>, <em>environmental/structural</em>,
<em>accidental</em>, and <em>internal</em>, respectively <span
class="citation" data-cites="Yampolskiy2016TaxonomyOP">(Yampolskiy
2016)</span>. The risks that are briefly outlined in this chapter are
discussed in greater depth in the rest of this book.<br />
In this chapter, we will describe how concrete, small-scale examples of
each risk might escalate into catastrophic outcomes. We also include
hypothetical stories to help readers conceptualize the various processes
and dynamics discussed in each section. We hope this survey will serve
as a practical introduction for readers interested in learning about and
mitigating catastrophic AI risks.<br />
</p>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-Davidson2021" class="csl-entry" role="listitem">
Davidson, Tom. 2021. <span>“Could Advanced AI Drive Explosive Economic
Growth?”</span>
</div>
<div id="ref-Roodman2020OnTP" class="csl-entry" role="listitem">
Roodman, David Malin. 2020. <span>“On the Probability Distribution of
Long-Term Changes in the Growth Rate of the Global Economy: An Outside
View.”</span>
</div>
<div id="ref-sagan1994pale" class="csl-entry" role="listitem">
Sagan, Carl. 1994. <em>Pale Blue Dot: A Vision of the Human Future in
Space</em>. New York: Random House.
</div>
<div id="ref-Yampolskiy2016TaxonomyOP" class="csl-entry"
role="listitem">
Yampolskiy, Roman V. 2016. <span>“Taxonomy of Pathways to Dangerous
Artificial Intelligence.”</span> In <em>AAAI Workshop: AI, Ethics, and
Society</em>.
</div>
</div>
</body>
</html>
