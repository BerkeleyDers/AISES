<h1 id="law"> 6.2 Law</h1>
<p><strong>Why not have AIs follow the law?</strong> In this textbook,
we have often turned to ethics to answer the question of how we can
ensure AI is safe and beneficial. But some argue that simply getting AIs
to follow the law is a better solution.<br />
The law has three features which give it an advantage over ethics as a
model for safe and beneficial AI. In democratic countries, the law is a
<em>legitimate</em> representation of our moral codes: at least in
theory, it is a democratically endorsed record of our shared values. Law
is <em>time-tested</em> and <em>comprehensive</em>; it has developed
over many generations to adjudicate the areas where humans have
consequential disagreements. Finally, legal language can be
<em>specific</em> and <strong>adaptable</strong> to new contexts,
comparing favorably to ethical language, which can often be interpreted
in diverging ways.<br />
The next subsection will expand on these features of the law. We will
see how these features contrast favorably with ethics before arguing
that we do need ethics after all but alongside the law.</p>
<h2 id="the-case-for-law"> 6.2.1 The Case For Law</h2>
<h3 id="legitimate-aggregation-of-values">Legitimate Aggregation of
Values</h3>
<h4
id="in-a-democratic-country-the-law-is-influenced-by-the-opinions-of-the-populace.">In
a democratic country, the law is influenced by the opinions of the
populace.</h4>
<p>Democratic citizens are able to elect judges and vote on new laws,
directly or through representatives. If they don’t like part of an
existing law, they have a range of legal means, such as advocating,
protesting, and voting, to change it. Even though the law at any given
time won’t perfectly reflect the values of the citizenry, the method of
arriving at law is usually <em>legitimate</em>. In other words, the
citizens have input into the law.</p>
<h4
id="legitimacy-provides-the-law-with-a-clear-advantage-over-ethics.">Legitimacy
provides the law with a clear advantage over ethics.</h4>
<p>The law provides a collection of rules and standards that enable us
to differentiate illegal from legal actions. Ethics, on the other hand,
isn’t standardized or codified. To determine ethical and unethical
actions, we have to either pick an ethical theory to follow, or decide
how to weigh the differing opinions of multiple theories that we think
might be true. But any of these options are likely to be more
controversial than simply following the law. Ethics has no in-built
method of democratic agreement.<br />
However, just following the law isn’t a perfect solution: there will
always be an act of interpretation between the written law and its
application in a particular situation. There is often no agreement over
the procedure for this interpretation (for example, in US constitutional
cases). Therefore, even if AI systems were created in a way that bound
them to follow the law, a legal system with human decision-makers would
have to remain part of the process. The law is only legitimate when
interpreted by someone democratically appointed or subject to democratic
critique.</p>
<h3 id="time-tested-and-comprehensive">Time-Tested and
Comprehensive</h3>
<p><strong>Systems of law have evolved over generations.</strong> With
each generation, new people are given the job of creating, enforcing,
and interpreting the law. The law covers a huge range of issues and
incorporates a wide range of distinctions. Because these bodies of law
have been evolving for so long, the information encoded in the law is a
record of what has worked for many people and is often considered an
approximation of their values. This makes the law a particularly
promising resource for aligning AI systems with the values of the
population.</p>
<h3 id="rules-and-standards">Rules and Standards</h3>
<h4
id="naively-we-might-think-of-the-law-as-a-system-of-rules.">Naively, we
might think of the law as a system of rules.</h4>
<p>“Law” seems almost synonymous with “rule” in our language. When we
talk about “the laws of physics” or “natural laws” in general, we mean
something rigid and inflexible—when X happens, Y will follow. This is
partly true: inflexible rules are a part of the law. For instance, take
the rule: “If someone drives faster than the speed limit of 70mph, they
will be fined $200.” In this case, there is an objective trigger
(driving faster than 70mph) and a clear directive (a $200 fine). There
isn’t much room for the judge to interpret the rule differently. This
gives the lawmaker predictable control over how the law will be carried
out.</p>
<h4 id="a-law-based-on-rules-alone-would-be-flawed.">A law based on
rules alone would be flawed.</h4>
<p>However, a fixed speeding rule would mean fining someone who was
accelerating momentarily to avoid hitting a pedestrian and not fining
someone who continued to drive at the maximum speed limit around a blind
turn, creating a danger for other drivers. Rules are always
over-inclusive (they will apply to some cases we would rather not be
illegal) and under-inclusive (they won’t apply to all cases we would
like to be illegal).</p>
<h4
id="to-remedy-this-problem-a-law-can-instead-be-based-on-a-standard.">To
remedy this problem, a law can instead be based on a standard.</h4>
<p>In the speeding case, a standard could be “when someone is driving
unreasonably, they will be fined in proportion to the harm they pose.” A
judge could apply this standard to get the correct result in both cases
above (speeding to avoid an accident and going full speed around a blind
turn). Standards have their own problems: with standards rather than
rules, the judge is empowered to interpret the standards based on their
own opinion, allowing them to act in ways that diverge from the
lawmaker’s intentions.</p>
<h4 id="the-law-uses-rules-and-standards.">The law uses rules and
standards.</h4>
<p>Using rules and standards alongside each other, the law can find the
best equilibrium between carrying out the lawmaker’s intentions and
accounting for situations they didn’t foresee <span class="citation"
data-cites="clermont2020rules"></span>. This gives the law an advantage
in the problem of maintaining human control of AI systems by displaying
the right level of ambiguity.<br />
Law is less ambiguous than ethical language, which can be very
ambiguous. Phrases like “do the right thing”, “act virtuously” or “make
sure you are acting consistently” can mean different things to different
people. In contrast, it is more flexible than programming languages,
which are brittle and designed to only fit into particular contexts.
Legal language can maintain a middle ground between rigid rules and more
sensible standards.<br />
</p>
<figure>
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/formatting.png" class="tb-img-full"/>
<p class="tb-caption">Ambiguity vs. flexibility in languages</p>
<!--<figcaption>Ambiguity vs. flexibility in languages</figcaption>-->
</figure>
<h3 id="specific-and-adaptable">Specific and Adaptable</h3>
<p>We can apply these insights about rules and standards in law to two
core problems in controlling AI systems: <em>misinterpretation</em> and
<em>gaming</em>. The law is specific enough to avoid misinterpretation
and adaptable enough to prevent many forms of gaming.</p>
<h4
id="given-commands-in-natural-language-ais-might-interpret-them-literally.">Given
commands in natural language, AIs might interpret them literally.</h4>
<p>The misinterpretation problem arises when the literal interpretation
of our commands differs from our intended interpretation. In AI safety
discourse, we see this problem raised by many thinkers; for example,
Stuart Russell raised the concern that an AI, if asked to develop a cure
for cancer, might experiment on people, giving them tumors as part of
carrying out the request <span class="citation"
data-cites="Russell2019HumanCA"></span>. A narrow, literal
interpretation of “develop a cure for cancer,” which doesn’t take any of
our typical considerations into account, could lead to this outcome.
Misinterpretation risks are like wishes to a genie: we might get what we
ask for but not what we want.</p>
<h4
id="the-capabilities-of-llms-give-us-some-reason-to-see-this-risk-as-unlikely.">The
capabilities of LLMs give us some reason to see this risk as
unlikely.</h4>
<p>Fortunately, there are good reasons not to be concerned about this
particular risk from AI. Our experience with large language models has
shown that by being trained on human-generated language data, AIs can
act on the meaning of our sentences in a way that is approximately
similar to the way that a human speaker of the language would. The
Happiness section below discusses two other systems that can predict
human responses to video and text. Suppose similar systems are used in
the future. In that case, we should not worry about extreme actions
emerging from a misinterpretation of perfectly normal requests—although
we might still worry that these emerge from optimization pressures.</p>
<h4
id="however-we-still-face-the-risk-of-ais-gaming-our-commands-to-get-what-they-want.">However,
we still face the risk of AIs gaming our commands to get what they
want.</h4>
<p>Stuart Russell raises a different concern with AI: gaming <span
class="citation" data-cites="Russell2019HumanCA"></span>. An AI system
may ‘play’ the system or rules, akin to strategizing in a game, to
achieve its objectives in unexpected or undesired ways. He gives the
example of tax codes. Humans have been designing tax laws for 6000
years, yet many still avoid taxation. Creating a tax code not
susceptible to gaming is particularly difficult because individuals are
incentivized to avoid paying taxes wherever possible. If our track
record with creating rules to constrain each other is so bad, then we
might be pessimistic about constraining AI systems that might have goals
that we don’t understand.</p>
<h4 id="a-partial-answer-is-found-in-the-use-of-standards.">A partial
answer is found in the use of standards.</h4>
<p>A standard can cover much more ground than a rule. A well-formulated
standard can lead to the law finding the right answer, even in new and
unique cases. Such approaches are sometimes applied in the case of
taxes. In the UK, for example, there is a rule against tax arrangements
that “are abusive.” This is not an objective trigger: it is up to a
judge to decide what is “abusive.” An AI system trained to follow the
law will be accountable to rules and standards.<br />
Nothing we have considered will be able to eliminate the possibility of
gaming. However, the problem of misinterpretation is less concerning
now, and we can be hopeful about using standards to maintain control of
AI systems.</p>
<h2 id="the-need-for-ethics"> 6.2.2 The Need for Ethics</h2>
<p>This subsection will discuss why ethics is still indispensable in
creating safe and beneficial AI, even though law is a powerful tool.
Firstly, though the law is comprehensive, there are important areas of
human life where it gives no advice. Secondly, it is common for laws to
be immoral, even by the standards of the residents of the country that
made them. Finally, the law is unlikely to be the most legitimate
conceivable system, even if it is our best one.</p>
<h3 id="silent-law">Silent Law</h3>
<p><strong>The law is a set of constraints, not a complete guide to
behavior.</strong> The law doesn’t cover every aspect of human life, and
certainly not everything we need to constrain AI. Sometimes, this is
accidental, but sometimes, the law is intentionally silent. We can call
these zones of discretion: areas of behavior that the law doesn’t
constrain; for example, a state’s economic policy and the content of
most contracts between private individuals. The law puts some limits on
these areas of behavior. Still, lawmakers intentionally leave free space
to enable actors to make their own choices and to avoid the law becoming
too burdensome.</p>
<h4
id="the-law-often-stops-short-of-compelling-us-to-do-clearly-good-things.">The
law often stops short of compelling us to do clearly good things.</h4>
<p>It is generally seen as good if someone steps in to rescue a stranger
who is in danger. However, under US common law, the duty to rescue only
applies in certain cases where we have special responsibility for
someone. We face a penalty if we fail to rescue our spouse or employee,
but there is no law against failing to rescue a stranger, even when it
would be at no cost to us. Giving humans this kind of discretion may not
have terrible outcomes, because many people have strong ethical
intuitions anyway, and would carry out the rescue. But an AI that was
only following the law would not unless it had a strong ethical drive as
well. In cases like rescue, where AI systems may be <em>more</em>
capable than humans to help, we could be passing up a major benefit by
asking the AI to only follow the law. Likewise, in the US, AIs may be
required to avoid uttering copyrighted content and libelous claims, but
doxing, instructions for building bombs, or advice for how to break the
law can be legal, even if it is not ethical.<br />
Conversely, by constraining AI with laws rather than guiding it with
ethics, we risk it acting in undesirable ways in zones of discretion. AI
could recommend potentially harmful economic policies, trick humans into
regrettable contracts, and pursue legal but harmful business practices.
Without ethics, a law-abiding AI could carry out great harm because the
law is silent in situations where we may want to guide behavior
nonetheless.</p>
<h3 id="immoral-law">Immoral Law</h3>
<h4
id="the-law-can-be-immoral-even-if-it-is-created-legitimately-and-interpreted-correctly.">The
law can be immoral even if it is created legitimately and interpreted
correctly.</h4>
<p>Democratic majorities can pass laws that a large number of fellow
citizens think are immoral, or that will seem immoral to future
populations of that country, such as the legalization of wars later
regretted, legal slavery, legal discrimination on the basis of race and
gender, and various other controversial laws which are later deemed
morally wrong. There is also often a significant time delay between the
moral opinions of a population changing, and the law changing to reflect
them <span class="citation" data-cites="nay2023law"></span>. This means
that laws formed in democracies can fall short of being moral, even in
the eyes of the citizens of the country that made them.<br />
If solely constrained by a country’s fallible democratic laws, future AI
systems could behave in ways most of the world would consider immoral.
To mitigate this risk, it is necessary to train AI to respect the
ethical perspectives of those affected by its actions.</p>
<h3 id="unrepresentative-law">Unrepresentative Law</h3>
<p><strong>Law isn’t the only way, or even necessarily the best way, to
arrive at an aggregation of our values.</strong> Not all judges and
legal professionals agree that the law <em>should</em> capture the
values of the populace. Many think that legal professionals know better
than the public, and that laws should be insulated from the changing
moral opinions of the electorate. This suggests that we might be able to
find, or conceive of, more representative ways of capturing the values
of the populace. Current alternatives like surveys or citizens’
assemblies are useful for some purposes, such as determining preferences
on specific issues or arriving at informed, representative policy
proposals. However, they aren’t suited to the general task of
summarizing the values of the entire population across areas as
comprehensive as those covered by the law.</p>
<p>Perhaps for this general task we could turn to solutions in the moral
uncertainty literature. The problem of moral uncertainty is precisely
the problem of figuring out how to act, given the range of differing
views on ethics. A solution from the moral uncertainty literature which
best embodies the democratic ideal is the <em>moral parliament</em>
approach, which we will discuss later in this chapter.</p>
<div class="visionbox">
<legend style="border-bottom-style: solid; border-bottom-color: #3585d4;
margin-left: 0em; margin-right: 0em; padding: 0.2em 0.8em; color: #ffffff; background-color: #3585d4">
    <p><span><b>A Note On The Three Laws of Robotics</b></span></p>
</legend>
<p>Some propose that
Isaac Asimov’s “Three Laws of Robotics” provide a useful set of rules
for resolving problems in machine ethics <span class="citation"
data-cites="asimov1942run Shulman2021"></span>. These laws are as
follows:</p>
<ol>
<li><p>“A robot may not injure a human being, or through inaction, allow
a human being to come to harm,”</p></li>
<li><p>“A robot must obey the orders given to it by a human being except
where such orders would conflict with the First Law,” and</p></li>
<li><p>“A robot must protect its own existence as long as such
protection does not conflict with the First or Second Laws.”</p></li>
</ol>
<p>The idea that these laws provide a solution to challenges in machine
ethics is a common misconception. Asimov himself frequently tested the
adequacy of these laws throughout his writing, showing that they are, in
fact, limited in their ability to resolve machine ethics problems.
Below, we explore some of these limitations.</p>
<h4
id="asimovs-laws-are-insufficient-for-guiding-ethical-behavior-in-ai-systems.">Asimov’s
laws are insufficient for guiding ethical behavior in AI systems<span
class="citation" data-cites="stokes2018"></span>.</h4>
<p>The three laws use under-defined terms like “harm” and “inaction.”
Because they’re under-defined, they could be interpreted in multiple
ways. It’s not clear precisely what “harm” means to humans, and it would
be even more difficult to encode the same meaning in AI systems.<br />
Harm is a complex concept. It can be physical or psychological. Would a
robot following Asimov’s first laws be required to intervene when humans
are about to hurt each other’s feelings? Would it be required to
intervene to prevent a human from behaving in ways that are self-harming
but deliberate, like smoking? Consider the case of amputating a limb in
order to stop the spread of an infection. An AI programmed with Asimov’s
laws would be forbidden from amputating the limb, as that would
literally be an instance of injuring a human being. However, the AI
would also be forbidden from allowing the harmful spread of an infection
through its inaction. These scenarios illustrate that the first law
fails, and therefore, that the following two do not follow. The laws may
need to be much more specific in order to reliably guide ethical
behavior in future AI systems.<br />
Philosophy has yet to produce a sufficient set of rules to determine
moral conduct. The safety of future AI systems cannot be guaranteed
simply through a set of rules or axioms. Numerous factors, such as proxy
gaming and competitive pressures, cannot be adequately captured in a set
of rules. Rules may be useful, but AI safety will require a more dynamic
and comprehensive approach that can address existing technical and
sociotechnical issues.<br />
Overall, Asimov’s Three Laws of Robotics fail to reliably guide ethical
behavior in AI systems, even if they serve as a useful starting point
for examining certain questions and problems in AI safety.<br />
</p>
</div>
<h3 id="conclusions-about-law-and-ethics">Conclusions About Law and
Ethics</h3>
<h4 id="summary.">Summary.</h4>
<p>The law is comprehensive, but not comprehensive enough to ensure that
the actions of an AI system are safe and beneficial. AI systems must
follow the law as a baseline, but we must also develop methods to ensure
that they follow the demands of ethics as well. Relying solely on the
law would leave many gaps that the AI could exploit, or make ethical
errors within. To create beneficial AI that acts in the interests of
humanity, we need to understand the ethical values that people hold over
and above the law.</p>
