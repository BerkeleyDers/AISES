<style>
    .visionbox{
    border-radius: 15px;
    border: 2px solid #3585d4;
    background-color: #ebf3fb;
    text-align: left;
    padding: 10px;
    }
</style>

<style>
    .visionboxlegend{
    border-bottom-style: solid;
    border-bottom-color: #3585d4;
    border-bottom-width: 0px;
    margin-left: -12px;
    margin-right: -12px; margin-top: -13px;
    padding: 0.01em 1em; color: #ffffff;
    background-color: #3585d4;
    border-radius: 15px 15px 0px 0px}
</style>

<h1 id="law">6.2 Law</h1>
<p><strong>Why not have AIs follow the law?</strong> We often argue that
for AIs to be safe and beneficial, we need to align them with human
values. However, some argue that simply getting AIs to follow the law is
a better solution.<p>
The law has three features that give it an advantage over ethics as a
model for safe and beneficial AI. In democratic countries, the law is a
<em>legitimate</em> representation of our moral codes: at least in
theory, it is a democratically endorsed record of our shared values. Law
is <em>time-tested</em> and <em>comprehensive</em>; it has developed
over many generations to adjudicate the areas where humans have
consequential disagreements. Finally, legal language can be
<em>specific</em> and <em>adaptable</em> to new contexts, comparing
favorably to ethical language, which can often be interpreted in
diverging ways.<p>
The next subsection will expand on these features of the law. We will
see how these features contrast favorably with ethics before arguing
that we do need ethics after all but alongside the law.</p>
<h2 id="the-case-for-law">6.2.1 The Case For Law</h2>
<h3 id="legitimate-aggregation-of-values">Legitimate Aggregation of
Values</h3>
<strong>In
a democratic country, the law is influenced by the opinions of the
populace.</strong>
<p>Democratic citizens vote on new laws, directly or through
representatives. If they don’t like part of an existing law, they have a
range of legal means, such as advocating, protesting, and voting, to
change it. Even though the law at any given time won’t perfectly reflect
the values of the citizenry, the method of arriving at law is usually
<em>legitimate</em>. In other words, the citizens have input into the
law.</p>
<strong>Legitimacy
provides the law with a clear advantage over ethics.</strong>
<p>The law provides a collection of rules and standards that enable us
to differentiate illegal from legal actions. Ethics, on the other hand,
isn’t standardized or codified. To determine ethical and unethical
actions, we have to either pick an ethical theory to follow, or decide
how to weigh the differing opinions of multiple theories that we think
might be true. But any of these options are likely to be more
controversial than simply following the law. Ethics has no in-built
method of democratic agreement.<p>
However, just following the law isn’t a perfect solution: there will
always be an act of interpretation between the written law and its
application in a particular situation. There is often no agreement over
the procedure for this interpretation (for example, in US constitutional
cases). Therefore, even if AI systems were created in a way that bound
them to follow the law, a legal system with human decision-makers would
have to remain part of the process. The law is only legitimate when
interpreted by someone democratically appointed or subject to democratic
critique.</p>
<h3 id="time-tested-and-comprehensive">Time-Tested and
Comprehensive</h3>
<p><strong>Systems of law have evolved over generations.</strong> With
each generation, new people are given the job of creating, enforcing,
and interpreting the law. The law covers a huge range of issues and
incorporates a wide range of distinctions. Because these bodies of law
have been evolving for so long, the information encoded in the law is a
record of what has worked for many people and is often considered an
approximation of their values. This makes the law a particularly
promising resource for aligning AI systems with the values of the
population.</p>
<h3 id="rules-and-standards">Rules and Standards</h3>
<strong>Naively, we
might think of the law as a system of rules.</strong>
<p>“Law” seems almost synonymous with “rule” in our language. When we
talk about “the laws of physics” or “natural laws” in general, we mean
something rigid and inflexible—when X happens, Y will follow. This is
partly true: inflexible rules are a part of the law. For instance, take
the rule: “If someone drives faster than the speed limit of 70mph, they
will be fined $200.” In this case, there is an objective trigger
(driving faster than 70mph) and a clear directive (a $200 fine). There
isn’t much room for the judge to interpret the rule differently. This
gives the lawmaker predictable control over how the law will be carried
out.</p>
<strong>A law based on
rules alone would be flawed.</strong>
<p>However, a fixed speeding rule would mean fining someone who was
accelerating momentarily to avoid hitting a pedestrian and not fining
someone who continued to drive at the maximum speed limit around a blind
turn, creating a danger for other drivers. Rules are always
over-inclusive (they will apply to some cases we would rather not be
illegal) and under-inclusive (they won’t apply to all cases we would
like to be illegal).</p>
<strong>To
remedy this problem, a law can instead be based on a standard.</strong>
<p>In the speeding case, a standard could be “when someone is driving
unreasonably, they will be fined in proportion to the harm they pose.” A
judge could apply this standard to get the correct result in both cases
above (speeding to avoid an accident and going full speed around a blind
turn). Standards have their own problems: with standards rather than
rules, the judge is empowered to interpret the standards based on their
own opinion, allowing them to act in ways that diverge from the
lawmaker’s intentions.</p>
<strong>The law uses rules and
standards.</strong>
<p>Using rules and standards alongside each other, the law can find the
best equilibrium between carrying out the lawmaker’s intentions and
accounting for situations they didn’t foresee <span class="citation"
data-cites="clermont2020rules">[1]</span>. This gives the law an
advantage in the problem of maintaining human control of AI systems by
displaying the right level of ambiguity.<p>
Law is less ambiguous than ethical language, which can be very
ambiguous. Phrases like “do the right thing”, “act virtuously” or “make
sure you are acting consistently” can mean different things to different
people. In contrast, it is more flexible than programming languages,
which are brittle and designed to only fit into particular contexts.
Legal language can maintain a middle ground between rigid rules and more
sensible standards.<p>
</p>
<figure>
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/formatting.png" class="tb-img-full" style="width=70%"/>
<p class="tb-caption">Figure 6.1: Legal language balances being less ambiguous than ethical language while being more
flexibly formatted than programming language.</p>
</figure>
<h3 id="specific-and-adaptable">Specific and Adaptable</h3>
<p>We can apply these insights about rules and standards in law to two
core problems in controlling AI systems: <em>misinterpretation</em> and
<em>gaming</em>. The law is specific enough to avoid misinterpretation
and adaptable enough to prevent many forms of gaming.</p>
<strong>Given
commands in natural language, AIs might interpret them literally.</strong>
<p>The misinterpretation problem arises when the literal interpretation
of our commands differs from our intended interpretation. In AI safety
discourse, we see this problem raised by many thinkers; for example,
Stuart Russell raised the concern that an AI, if asked to develop a cure
for cancer, might experiment on people, giving them tumors as part of
carrying out the request <span class="citation"
data-cites="Russell2019HumanCA">[2]</span>. A narrow, literal
interpretation of “develop a cure for cancer,” which doesn’t take any of
our typical considerations into account, could lead to this outcome.
Misinterpretation risks are like wishes to a genie: we might get what we
ask for but not what we want.</p>
<strong
id="the-capabilities-of-llms-give-us-some-reason-to-see-misinterpretation-risks-as-unlikely.">The
capabilities of LLMs give us some reason to see misinterpretation risks
as unlikely.</strong>
<p>Before LLMs, some AI researchers were concerned that most AI models
would be trained in ways that would mean they had no understanding of
many ordinary human concepts or values. However, this now seems less
likely. Our experience with large language models has shown that by
being trained on human-generated language data, AIs can respond to the
meaning of our sentences in a way that is approximately similar to the
way that a human speaker of the language would; for instance, the
Happiness section below discusses two other systems that can predict
human responses to video and text. If similar systems are used in the
future, it seems plausible that AIs can apply laws in a sensible way,
and we may not need to worry about extreme actions emerging from a
misinterpretation of perfectly normal requests.<p>
That said, we might still worry that these emerge from optimization
pressures. Additionally, while AIs might usually interpret human
concepts sensibly, they may still have representations that have rare
quirks. This poses problems; for instance, it creates vulnerabilities
that can be exploited by adversarial attacks. There is more work to be
done before we can feel comfortable that systems will reliably be able
to interpret laws and other commands in practice.</p>
<strong
id="however-we-still-face-the-risk-of-ais-gaming-our-commands-to-get-what-they-want.">However,
we still face the risk of AIs gaming our commands to get what they
want.</strong>
<p>Stuart Russell raises a different concern with AI: gaming <span
class="citation" data-cites="Russell2019HumanCA">[2]</span>. An AI
system may ‘play’ the system or rules, akin to strategizing in a game,
to achieve its objectives in unexpected or undesired ways. He gives the
example of tax codes. Humans have been designing tax laws for 6000
years, yet many still avoid taxation. Creating a tax code not
susceptible to gaming is particularly difficult because individuals are
incentivized to avoid paying taxes wherever possible. If our track
record with creating rules to constrain each other is so bad, then we
might be pessimistic about constraining AI systems that might have goals
that we don’t understand.</p>
<strong>A
partial solution to misinterpretation and gaming is found in rules and
standards.</strong>
<p>If we are concerned about misinterpretation, we might choose to rely
on laws that are specific. Rules such as “any generated image must have
a digital watermark” are specific enough that they are difficult to
misinterpret. We might prefer using such laws rather than relying on
abstract ethical principles, which are vaguer and easier to
misinterpret.<p>
Conversely, if we are concerned about AIs gaming rules, we might prefer
to have standards, which can cover more ground than a rule. A
well-formulated standard can lead to an agent consistently finding the
right answer, even in new and unique cases. Such approaches are
sometimes applied in the case of taxes. In the UK, for example, there is
a rule against tax arrangements that are “abusive.” This is not an
objective trigger: it is up to a judge to decide what is “abusive.” An
AI system trained to follow the law can be accountable to rules and
standards.</p>
<h2 id="the-need-for-ethics">6.2.2 The Need for Ethics</h2>
<p>This subsection will discuss why ethics is still indispensable in
creating safe and beneficial AI, even though law is a powerful tool.
Firstly, though the law is comprehensive, there are important areas of
human life where it gives no advice. Secondly, it is common for laws to
be immoral, even by the standards of the residents of the country that
made them. Finally, the law is unlikely to be the most legitimate
conceivable system, even if it is our best one.</p>
<h3 id="silent-law">Silent Law</h3>
<p><strong>The law is a set of constraints, not a complete guide to
behavior.</strong> The law doesn’t cover every aspect of human life, and
certainly not everything we need to constrain AI. Sometimes, this is
accidental, but sometimes, the law is intentionally silent. We can call
these zones of discretion: areas of behavior that the law doesn’t
constrain; for example, a state’s economic policy and the content of
most contracts between private individuals. The law puts some limits on
these areas of behavior. Still, lawmakers intentionally leave free space
to enable actors to make their own choices and to avoid the law becoming
too burdensome.</p>
<strong>The
law often stops short of compelling us to do clearly good things.</strong>
<p>It is generally seen as good if someone steps in to rescue a stranger
who is in danger. However, in some jurisdictions like the US, the duty
to rescue only applies in certain cases where we have special
responsibility for someone. We face a penalty if we fail to rescue our
spouse or employee, but there is no law against failing to rescue a
stranger, even when it would be at no cost to us. Giving humans this
kind of discretion may not have terrible outcomes, because many people
have strong ethical intuitions anyway, and would carry out the rescue.
But an AI that was only following the law would not unless it had a
strong ethical drive as well. In cases like rescue, where AI systems may
be <em>more</em> capable than humans to help, we could be passing up a
major benefit by asking the AI to only follow the law. Likewise, in the
US, AIs may be required to avoid uttering copyrighted content and
libelous claims, but doxing, instructions for building bombs, or advice
for how to break the law can be legal, even if it is not ethical.<p>
Conversely, by constraining AI with laws rather than guiding it with
ethics, we risk it acting in undesirable ways in zones of discretion. AI
could recommend potentially harmful economic policies, trick humans into
regrettable contracts, and pursue legal but harmful business practices.
Without ethics, a law-abiding AI could carry out great harm because the
law is silent in situations where we may want to guide behavior
nonetheless.</p>
<h3 id="immoral-law">Immoral Law</h3>
<strong>The
law can be immoral even if it is created legitimately and interpreted
correctly.</strong>
<p>Democratic majorities can pass laws that a large number of fellow
citizens think are immoral, or that will seem immoral to future
populations of that country, such as the legalization of wars later
regretted, legal slavery, legal discrimination on the basis of race and
gender, and various other controversial laws which are later deemed
morally wrong. There is also often a significant time delay between the
moral opinions of a population changing, and the law changing to reflect
them <span class="citation" data-cites="nay2023law">[3]</span>. This
means that laws formed in democracies can fall short of being moral,
even in the eyes of the citizens of the country that made them.<p>
If solely constrained by a country’s fallible democratic laws, future AI
systems could behave in ways most of the world would consider immoral.
To mitigate this risk, it is necessary to train AI to respect the
ethical perspectives of those affected by its actions.</p>
<h3 id="unrepresentative-law">Unrepresentative Law</h3>
<p><strong>Law isn’t the only way, or even necessarily the best way, to
arrive at an aggregation of our values.</strong> Not all judges and
legal professionals agree that the law <em>should</em> capture the
values of the populace. Many think that legal professionals know better
than the public, and that laws should be insulated from the changing
moral opinions of the electorate. This suggests that we might be able to
find, or conceive of, more representative ways of capturing the values
of the populace. Current alternatives like surveys or citizens’
assemblies are useful for some purposes, such as determining preferences
on specific issues or arriving at informed, representative policy
proposals. However, they aren’t suited to the general task of
summarizing the values of the entire population across areas as
comprehensive as those covered by the law.</p>
<p>Perhaps for this general task we could turn to solutions in the moral
uncertainty literature. The problem of moral uncertainty is precisely
the problem of figuring out how to act, given the range of differing
views on ethics. A solution from the moral uncertainty literature which
best embodies the democratic ideal is the <em>moral parliament</em>
approach, which we will discuss later in this chapter.</p>
<div class="visionbox">
<legend class="visionboxlegend">
    <p><span><b>A Note On The Three Laws of Robotics</b></span></p>
</legend>
Some propose that Isaac Asimov’s “Three Laws of Robotics” provide a useful set of rules
for resolving problems in machine ethics <span class="citation"
data-cites="asimov1942run Shulman2021">[4], [5]</span>. These laws are
as follows:</p>
<ol>
<li><p>“A robot may not injure a human being, or through inaction, allow
a human being to come to harm,”</p></li>
<li><p>“A robot must obey the orders given to it by a human being except
where such orders would conflict with the First Law,” and</p></li>
<li><p>“A robot must protect its own existence as long as such
protection does not conflict with the First or Second Laws.”</p></li>
</ol>
<p>The idea that these laws provide a solution to challenges in machine
ethics is a common misconception. Asimov himself frequently tested the
adequacy of these laws throughout his writing, showing that they are, in
fact, limited in their ability to resolve machine ethics problems.
Below, we explore some of these limitations.</p>
<strong>Asimov’s
laws are insufficient for guiding ethical behavior in AI systems<span
class="citation" data-cites="stokes2018">[6]</span>.</strong>
<p>The three laws use under-defined terms like “harm” and “inaction.”
Because they’re under-defined, they could be interpreted in multiple
ways. It’s not clear precisely what “harm” means to humans, and it would
be even more difficult to encode the same meaning in AI systems.<p>
Harm is a complex concept. It can be physical or psychological. Would a
robot following Asimov’s first laws be required to intervene when humans
are about to hurt each other’s feelings? Would it be required to
intervene to prevent a human from behaving in ways that are self-harming
but deliberate, like smoking? Consider the case of amputating a limb in
order to stop the spread of an infection. An AI programmed with Asimov’s
laws would be forbidden from amputating the limb, as that would
literally be an instance of injuring a human being. However, the AI
would also be forbidden from allowing the harmful spread of an infection
through its inaction. These scenarios illustrate that the first law
fails, and therefore, that the following two do not follow. The laws may
need to be much more specific in order to reliably guide ethical
behavior in future AI systems.<p>
Philosophy has yet to produce a sufficient set of rules to determine
moral conduct. The safety of future AI systems cannot be guaranteed
simply through a set of rules or axioms. Numerous factors, such as proxy
gaming and competitive pressures, cannot be adequately captured in a set
of rules. Rules may be useful, but AI safety will require a more dynamic
and comprehensive approach that can address existing technical and
sociotechnical issues.<p>
Overall, Asimov’s Three Laws of Robotics fail to reliably guide ethical
behavior in AI systems, even if they serve as a useful starting point
for examining certain questions and problems in AI safety.<p>
</p>
</div>
<h3 id="conclusions-about-law">Conclusions About Law</h3>
<p>The law is comprehensive, but not comprehensive enough to ensure that
the actions of an AI system are safe and beneficial. AI systems must
follow the law as a baseline, but we must also develop methods to ensure
that they follow the demands of ethics as well. Relying solely on the
law would leave many gaps that the AI could exploit, or make ethical
errors within. To create beneficial AI that acts in the interests of
humanity, we need to understand the ethical values that people hold over
and above the law.</p>

<br>
<br>
<h3>References</h3>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-clermont2020rules" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] K.
M. Clermont, <span>“Rules, standards, and such,”</span> <em>Buffalo Law
Review</em>, 2020.</div>
</div>
<div id="ref-Russell2019HumanCA" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] S.
Russell, <span>“Human compatible: Artificial intelligence and the
problem of control,”</span> 2019.</div>
</div>
<div id="ref-nay2023law" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] J.
J. Nay, <span>“Law informs code: A legal informatics approach to
aligning artificial intelligence with humans.”</span> 2023. Available:
<a
href="https://arxiv.org/abs/2209.13020">https://arxiv.org/abs/2209.13020</a></div>
</div>
<div id="ref-asimov1942run" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] I.
Asimov, <em>Runaround</em>. 1942.</div>
</div>
<div id="ref-Shulman2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] C.
Shulman and N. Bostrom, <span>“<span class="nocase">306C18Sharing the
World with Digital Minds</span>,”</span> in <em><span>Rethinking Moral
Status</span></em>, Oxford University Press, 2021. doi: <a
href="https://doi.org/10.1093/oso/9780192894076.003.0018">10.1093/oso/9780192894076.003.0018</a>.</div>
</div>
<div id="ref-stokes2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] C.
Stokes, <span>“Why the three laws of robotics do not work,”</span>
<em>International Journal of Research in Engineering and
Innovation</em>, vol. 2, 2018.</div>
</div>
</div>
