<h1 id="sec:moral-parliament">6.8 Moral Parliament</h1>
<p>In previous sections, we have explored various frameworks to embed
ethics into AIs, from the law to social welfare functions. In the next
chapter, we will consider a few of the most influential ethical theories
developed by moral philosophers to explain how we ought to behave.
However, each of these presents some serious problems that leave them
unsuitable to be the sole solution to machine ethics. Most theories seem
to have at least some counterintuitive implications that we would be
uncomfortable endorsing, and many seem to capture something valuable
about morality. Determining how to act given uncertainty about which
theory of ethics is correct is the problem of <em>moral
uncertainty</em>.</p>
<p><strong>Should we use a moral parliament to guide AI
decision-making?</strong> In this section, we will explore using
<em>moral parliaments</em> to help AIs make robust ethical decisions in
the face of moral uncertainty. In the chapter, we will examine the
philosophical problem of moral uncertainty and the moral parliament
solution in detail. Here, we will first briefly describe the solution
and how we might use AIs to implement a moral parliament. Then, we will
consider five advantages such an approach has over giving AIs specific
human values.</p>
<h2 id="implementing-a-moral-parliament">Implementing a Moral
Parliament</h2>
<strong>We can use AIs to
simulate moral parliaments.</strong>
If we want AIs to be guided by moral theories while accounting for
moral uncertainty, we can use a moral parliament. The basic idea is to
represent a set of moral theories that we think are plausible, place
representatives of these theories into a parliament, and use democratic
processes to make decisions <span class="citation"
data-cites="newberry2021parliamentary">[1]</span>.<p>
</p>
<figure id="fig:parliament">
    <p><img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/parliament.png" alt="image" class="tb-img-full" style="width: 90%"/> </p>
<p class="tb-caption">Figure 6.8: Different AIs, each programmed to represent a unique moral theory, engage in negotiation
and voting to decide on a course of action, simulating parliamentary processes.</p>
</figure>
<p>We can use advanced AIs to emulate these representatives by training
AIs to act in accordance with a specific moral theory. This allows us to
run moral parliaments artificially, permitting real-time
decision-making. Just like in real parliaments, we would have
delegates—AIs instructed to represent certain moral theories—get
together, discuss what to do, negotiate favorable outcomes, and then
vote on what to recommend. We could use the output of this moral
parliament to instruct a separate AI in the real world to take certain
actions over others.<p>
This is speculative and might still face problems; for instance, AIs
might have insufficient understanding of our moral theories. However,
these problems could become more tractable with advanced AI systems.
Assuming we have this ability, using a moral parliament might be an
attractive solution to getting AIs to act in accordance with human
values in real time.</p>
<strong>We
can decide who we want represented in our generalized moral
parliament.</strong>
While we have explored the traditional moral parliament method of
representing moral theories, we can generalize beyond this. Instead of
representing theories, it might be more appropriate to represent
stakeholders; for instance, in a decision about public transport, we
could emulate representatives for local residents, commuters, and
environmental groups, all of whom have an interest in the outcome. Using
a generalized moral parliament for decision-making in AI is an approach
that ensures all relevant perspectives are taken into account. In
contrast to traditional methods that focus on representing different
moral theories, <em>stakeholder representation</em> prioritizes the
views of those directly affected by the AI’s decisions. This could
enhance the AI’s understanding of the intricate human social dynamics
involved in any given situation.</p>
Let’s consider an AI suggesting new regulations for data privacy. By
emulating the perspectives of users, advertisers, and developers, it can
obtain a comprehensive understanding of the potential implications of
new regulations. Users may prioritize privacy and usability, advertisers
may focus on visibility and click-through rates, and developers may be
concerned with feasibility and profitability. By considering these
diverse views, the AI can make a more balanced decision that better
aligns with overall societal interests in real time.<p>
</p>
<h2 id="advantages-of-a-moral-parliament">6.8.1 Advantages of a Moral
Parliament</h2>
<p>Using moral parliaments presents a wide array of benefits relative to
just giving AIs certain sets of values directly. In this subsection, we
will explore how they are customizable, transparent, robust to bugs and
errors, adaptable to changing human values, and pro-negotiation.</p>
<strong>Customizable
moral parliaments are diverse and scalable.</strong>
The generalized moral parliament can accommodate a wide variety of
stakeholders, ranging from individual users to large corporations, and
from local communities to global societies. By emulating a large set of
stakeholders, we can ensure that a diverse set of views are represented.
This allows AIs to effectively respond to a wide range of scenarios and
contexts, providing a robust framework for ensuring AIs decisions
reflect the values, interests, and expectations of all relevant
stakeholders. Additionally, moral parliaments are scalable: if we are
concerned about a lack of representation, we can simply emulate more
stakeholders. By grounding AI decision-making in human perspectives and
experiences, we can create AI systems that are not only more ethical and
fair but also more effective and beneficial for society as a whole.</p>
<strong>Transparency
is another key benefit of a moral parliament.</strong>
As it stands, automated decision making is opaque: we rarely
understand why AIs make the decisions they do. However, since the moral
parliament gives us a clear mechanism of representing and weighting
different perspectives, it allows stakeholders to understand the basis
of an AI’s decision-making. We could, for instance, enforce that AIs
keep records of simulated negotiations in human languages and then view
the transcripts. Using moral parliaments provides insights into how
different moral considerations have been weighed against each other,
making the decision-making process of an AI more transparent,
explainable, and accountable.</p>
<strong>Moral
parliaments tend to be less fragile and less prone to bugs.</strong>
If we are sure that utilitarianism is the correct moral view, we
might be tempted to create AIs that maximize wellbeing-—this seems clean
and elegant. However, having a diverse moral parliament would make AIs
less likely to misbehave. By having multiple parliament members, we
would achieve <em>redundancy</em>. This is a common principle in
engineering: to always include extra components that are not strictly
necessary to functioning, in case of failure in other components (and is
explored further in the chapter). We would do this to avoid failure
modes where we were overconfident that we knew the correct moral theory,
such as lying and stealing for the greater good, or just to avoid poor
implementation from AIs optimizing for one moral theory. For example, a
powerful AI told that utilitarianism is correct might implement
utilitarianism in a particular way that is likely to lead to bad
outcomes.</p>
Imagine an AI that has to evaluate millions of possibilities for every
decision it makes. Even with a small error rate, the cumulative effect
could lead the AI to choose risky or unconventional actions. This is
because, when evaluating so many options, actions with high variance in
moral value estimation may occasionally appear to have significant
positive value. The AI could be more inclined to select these high-risk
actions based on the mistaken belief that they would yield substantial
benefits. For instance, an AI following some form of utilitarianism
might use many resources to create happy digital minds—at the expense of
humanity-—even if that is not what we humans think is morally
good.</p>
This is similar to the Winner’s Curse in auction theory: those that win
auctions of goods with uncertain value often find that they won because
they overestimated the value of the good relative to everyone else; for
instance, when bidding on a bag of coins at a fair, people who
overestimate how many coins there are will be more likely to win.
Similarly, the AI might opt for actions that, in hindsight, were not
truly beneficial. A moral parliament can make this less likely, because
actions that would be judged morally extreme by most humans also
wouldn’t be selected by a diverse moral parliament.</p>
The process of considering a range of theories inherently embeds
redundancy and cross-checking into the system, reducing the probability
of catastrophic outcomes arising from a single point of failure. It also
helps ensure that AI systems are robust and resilient, capable of
handling a broad array of ethical dilemmas.</p>
<strong>Moral
parliaments encourage compromise and negotiation.</strong>
In real-life parliaments, representatives who hold different opinions
on various issues often engage in bargaining, compromise, and
cooperation to reach agreeable outcomes and find common ground. We want
our AIs to achieve similar outcomes, such as ones that are moderate
instead of extreme. Ideally, we want AIs to select outcomes that many
moral theories and stakeholders all like, rather than being forced to
trade off between them.</p>
In particular, we might want to design our moral parliaments in specific
ways to encourage this. One such feature is proportional chances voting,
in which each option then gets a chance of winning that’s proportional
to the number of votes it gets—if a parliament is 60/40 split on a
proposal, then the AI would do what’s recommended 60% of the time rather
than just going with the majority. This setup motivates the
representatives to come together on options that are compromises rather
than sticking to their own viewpoints rigidly. They want to do this to
prevent any option they see as extremely bad from having any chance of
winning. This ensures a robust high-level principle guiding AI behavior,
reducing the risk of extreme outcomes, and fostering a more balanced,
nuanced approach to ethical decision-making.</p>
<strong>Using
a moral parliament reduces the risk of overlooking or locking in certain
values.</strong>
The moral parliament represents an approach to ethical
decision-making in AI that is distinctively cosmopolitan, in that it
encompasses a broad range of moral theories or stakeholders. It ensures
that many ethical viewpoints are considered. This wider view is helpful
in dealing with moral problems and tough decisions AI systems may run
into, by making sure that all important considerations are thought over
in a balanced way. AIs using moral parliaments are less likely to ignore
values that matter to different groups in society.</p>
Further, the moral parliament allows the representation of human values
to grow and change over time. We know that moral views change over time,
so we should be humble about how much we know about morality: there
might be important things we don’t yet understand that could help us get
closer to the truth about what is right and wrong. By regularly using
moral parliaments, AI systems can keep up with current human values,
rather than sticking to the old values that were defined when the AI was
created. This keeps AI up-to-date and flexible, and prevents it from
acting based on outdated or irrelevant values that are locked into the
system.</p>
<strong>Challenges.</strong>
Deciding which ethical theories to include in the moral parliament
could be a challenging task. There are numerous ethical frameworks, and
selecting a representative set may be subjective and politically
charged. The decision procedure used to assign appropriate weights to
different ethical theories and aggregate their recommendations in ways
that reflect their importance could also be contentious. Different
stakeholders may have varying opinions on how to prioritize these
theories. Moreover, ethical theories can be subject to interpretation
and may have nuanced implications. Advanced AI systems would need to be
able to accurately understand and apply these theories in order to use a
moral parliament.</p>
<h3 id="conclusions-about-moral-parliament">Conclusions About Moral
Parliament</h3>
<strong>Summary.</strong>
In this section, we explored using AIs to operationalize moral
parliaments, whether in the original form of representing moral theories
or generalized to representing stakeholders for any given issue. We
highlighted the advantages of using a moral parliament, such as reducing
the risk of overlooking or locking in certain values, allowing for the
representation of changing human values over time, and increasing
transparency and accountability in AI decision-making. We also noted
that moral parliaments encourage compromise and negotiation, leading to
more balanced and nuanced ethical decisions.</p>

<br>
<br>
<h3>References</h3>

<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-newberry2021parliamentary" class="csl-entry"
role="listitem">
<div class="csl-left-margin">[1] T.
Newberry and T. Ord, <span>“The parliamentary approach to moral
uncertainty,”</span> Future of Humanity Institute, University of Oxford,
2021. Available: <a
href="https://www.fhi.ox.ac.uk/wp-content/uploads/2021/06/Parliamentary-Approach-to-Moral-Uncertainty.pdf ">https://www.fhi.ox.ac.uk/wp-content/uploads/2021/06/Parliamentary-Approach-to-Moral-Uncertainty.pdf
</a></div>
</div>
</div>
