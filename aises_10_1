<!-- Appendix A - Normative Ethics -->

<h1 id="introduction">A.1 Introduction</h1>
<p>Ethics is the branch of philosophy concerned with questions of right
and wrong, good and bad, and how we ought to live our lives. We make
ethical choices every day. When we decide whether to tell the truth or
lie, help someone in need or ignore them, treat others with respect or
act in a discriminatory manner, we are making moral decisions that
reflect our values, beliefs, and moral principles. Philosophical ethics
seeks to provide a systematic framework for making these
decisions.<p>
In this chapter, we will explore some of the key concepts and theories
in philosophical ethics. This branch of research is also commonly called
<em>moral philosophy</em>. We use the terms <em>ethics</em> and
<em>morality</em> interchangeably. The subfield of ethics dedicated to
developing moral theories is called normative ethics. Normative ethics
is the consideration of questions like “Which actions are right and
wrong?”<p>
This chapter outlines some of the main reasons why it’s important for AI
researchers to learn about ethics. We then turn to the basic building
blocks of moral theories, examining various moral considerations like
intrinsic goods, constraints, and special obligations. Then we will
explore some of the most prominent ethical theories, like
utilitarianism, deontology, and virtue ethics, evaluating their
strengths and weaknesses. Throughout, our
key focus is on the ethical concepts that are most relevant to the
development, implementation, and governance of AI.</p>

<h1 id="why-learn-about-ethics">A.2 Why Learn About Ethics?</h1>
<p>This chapter will help you understand ethics—both in the context of
this book and in public discourse about AI safety. Here, we cover the
most prominent theories in the history of ethical discourse. After
reading this chapter, you should have a solid foundation for
understanding ethics in AI discussions.<p>
Ethics is relevant to the field of AI for two key reasons. First, AI
systems are increasingly being integrated into various aspects of human
life, such as healthcare, education, finance, and transportation, and
they have the potential to significantly impact our lives and wellbeing.
As AI systems become increasingly intelligent and powerful, it is
crucial to ensure that they are designed, developed, and deployed in
ways that promote widely shared values and do not amplify existing
social biases or cause needless harms. Unfortunately, there are already
numerous examples of AI systems being designed in ways that failed to
adequately consider such risks, such as racially biased facial
recognition systems. In order to wisely manage the growing power of AI
systems, developers and users of AI systems need to understand the
ethical challenges that AI systems introduce or exacerbate.<p>
Second, AI systems raise a range of new ethical questions that are
unique to their technological nature and capabilities. For instance, AI
systems can generate, process, and analyze vast amounts of data—much
more than was previously possible. In what ways does this new technology
challenge traditional notions of privacy, consent, intellectual
property, and transparency? Another important set of questions relates
to the moral status of AI systems. This is likely to become more
pressing if AI systems become increasingly autonomous and able to
interact with human beings in ways that convince their users that they
have their own preferences and feelings. What should we do if AI systems
appear to meet some of the potential criteria for sentience or other
morally relevant features?<p>
Thirdly, as further explored in the Single Agent Safety and Machine Ethics chapters, it is challenging to
specify objectives or goals for highly powerful AI systems in ways that
do not lead in a predictable way to highly undesirable consequences. In
order to grasp why it is so challenging to specify these objectives, it
is helpful to understand the ethical theories that have been proposed.
Questions of what it means to act rightly or to live a good life have
been debated by many thinkers over several millennia, with strong
arguments advanced for a number of competing positions. These debates
can provide us with greater insight into the challenges that AI
developers will need to overcome in order to build increasingly powerful
AI systems in a beneficial way. Rather than attempting to bypass or
ignore such controversies, AI developers should accept that their design
decisions may raise difficult ethical questions that need to be
considered carefully.</p>
<h2 id="is-ethics-relative">A.2.1 Is Ethics “Relative?”</h2>
<p><strong>Even after millennia of deliberation, we do not agree on all
of morality.</strong> Philosophers have been thinking about and debating
moral principles for millennia, yet they have not achieved consensus on
many moral issues. Widespread disagreements remain in both philosophical
and public discourse, including about important topics like abortion,
assisted suicide, capital punishment, animal rights, and the effects of
human activity on natural ecosystems. One troubling idea is that these
disagreements are irresolvable because no moral principles or judgments
are absolutely or universally correct. In the case of AI, this may lead
AI developers to believe that they have no role to play in shaping how
AI systems behave.</p>
<p><strong>Cultural relativism claims there is no objective, culturally
independent standard of morality.</strong> Consider the principle that
consensual relationships between adults are acceptable regardless of
whether they are heterosexual or homosexual. A moral relativist would
suggest this principle is correct for people who belong to some cultures
where homosexuality is accepted, but incorrect for people who belong to
other cultures where homosexuality is criminalised or socially
stigmatized. These differences are systemic: many cultures have moral
standards that seem incompatible with others’ ideals, such as different
views on marriage, divorce, gender roles, freedom of speech, or
religious tolerance. These differences form the basis for arguments for
cultural relativism.</p>
<p><strong>Normative moral relativism vs. descriptive moral relativism
<span class="citation" data-cites="gowans2021moral">[1]</span>.</strong>
Moral relativism has various forms, but here we discuss two: descriptive
moral relativism and normative moral relativism. Descriptive moral
relativism is straightforward: it means that different societies around
the world have different sets of rules about what’s right and wrong,
much like they have unique cuisines, customs, and traditions.
Descriptive moral relativism makes no claims about which, if any, of
these rules is right or wrong. Normative moral relativism suggests that
one cannot say that something is right or wrong in general, but only
relative to a particular culture or set of norms. Normative moral
relativists conclude that morality itself is not something universal or
absolute. Strictly speaking, descriptive moral relativism and normative
moral relativism are independent of each other, although in practice
descriptive moral relativism is often treated as if it provides evidence
for normative moral relativism.</p>
<h3 id="objections-to-moral-relativism">Objections to Moral
Relativism</h3>
<p>A number of arguments can be advanced against descriptive and
normative moral relativism <span class="citation"
data-cites="gowans2021moral">[1]</span>, which we explore in this
subsection. We will explore the argument that cultural differences might
be overstated, which makes descriptive moral relativism harder to
uphold. Another argument is that proponents of normative moral
relativism often face challenges when confronted with instances of
extreme harm. For instance, while many would unequivocally agree that
torturing a child for entertainment is morally wrong, a normative moral
relativist might be required to argue that its morality is contingent
upon the cultural context. Extreme examples such as this suggest few
people are willing to be thoroughgoing moral relativists. We further
explore arguments for and against moral relativism in this section.</p>
<p><strong>Human moral systems appear to share some common
features.</strong> Some have argued that most or all societies share
some norms. For example, prohibitions against lying, stealing, or
killing human beings are common across cultures. Many cultures have some
form of reciprocity, which is the idea that people have a moral
obligation to repay the kindness or generosity they have received from
others or that people should treat others the way they wish to be
treated <span class="citation"
data-cites="curry2019cooperate">[2]</span>. This can be seen in the
widespread practice of exchanging gifts and in moral codes that
emphasize fairness and justice. Additionally, human cultures have
typically some concept of parenthood, which often involves a moral
obligation to care for one’s children, as well as broader obligations to
one’s family and group. These common features suggest that there are at
least a few universal aspects of morality that transcend cultural
boundaries.</p>
<p><strong>Moral relativism conflicts with common-sense morality <span
class="citation" data-cites="gowans2021moral">[1]</span>.</strong>
Consider controversial practices still prevalent in some cultures, such
as honor killings in parts of the Middle East. The honor of a family
depends on the “purity” of its women. If a woman is raped or is deemed
to have compromised her chastity in some way, the profound shame brought
upon her family may lead them to kill her in response. According to the
normative moral relativist, if such a practice is in line with the moral
standards of the society where it takes place, there is nothing wrong
with it. Even more disturbingly, on some versions of relativism, men in
these societies may even be considered morally in the wrong if they fail
to kill their wives, daughters or sisters for having worn the wrong
clothing, having premarital sex or being raped. Similarly, normative
moral relativism would require us to believe that the morality of owning
slaves was entirely dependent on the societal context. Moral
iconoclasts, such as early anti-slavery campaigners, would by definition
always be morally wrong. In practice, if required to accept that moral
standards that endorse honor killings or slavery are not wrong in a
general sense, many moral relativists may recoil from this.</p>
<p><strong>Cultural moral relativism denies the possibility of
meaningful moral debate or moral progress <span class="citation"
data-cites="gowans2021moral">[1]</span>.</strong> Moral relativism seems
to require us to accept contradictory claims. For example, moral
relativists might say that a supporter of gay marriage is correct in
saying that homosexuality is morally acceptable, while someone from a
different culture might be correct in saying that homosexuality is
morally wrong, provided that both claims are in line with the moral
standards of the cultures they respectively belong to. If moral
relativism requires assert to simultaneously assert and deny that
homosexuality is morally acceptable, and any theory that generates
contradictions should be rejected, this would appear to mean that we
should reject moral relativism. In order to resist this, moral
relativists typically reinterpret the way we usual moral language in a
way that can save it from contradiction. The relativist would say that
when we say “homosexuality is wrong”, what we really mean is
“Homosexuality is not approved by my society’s norms”. This means that
relativists have to deny the possibility of moral disagreement and claim
that anyone who engages in such debates does not understand the meaning
of what they are saying.</p>
<p><strong>Moral relativism does not necessarily promote tolerance <span
class="citation" data-cites="gowans2021moral">[1]</span>.</strong> Some
have argued that one of the attractions of moral relativism is that it
promotes tolerance. By recognizing cultural differences (descriptive
moral relativism), they may assert that everyone ought to do what their
culture says is right (normative moral relativism). However, in a
society that is deeply intolerant, cultural moral relativism cannot
support tolerance, as it cannot claim that this has any universal or
objective value. Moral relativism only recommends tolerance to cultures
where it is already accepted. Indeed, to be tolerant, one need not be a
normative moral relativist. There are alternatives views which can
accommodate tolerance and multiple perspectives, such as
cosmopolitanism, liberal principles, and value pluralism.</p>
<p><strong>In practice, moral relativism can shut down ethics
discussions <span class="citation"
data-cites="gowans2021moral">[1]</span>.</strong> It is important to
note that different cultures have different moral standards. However, AI
developers sometimes invoke this observation and side with normative
moral relativism to avoid considering the ethics of their AI design
choices. Moreover, suppose AI developers do not analyze the ethical
implications of their choices and avoid ethical discussions by noting
the lack of cross-cultural consensus. In that case, the default is for
AI development to be driven by amoral forces, such as self-interest or
what makes the most sense in a competitive market. Decisions driven by
other forces, such as commercial incentives, will not necessarily be
aligned with the broader interests of society. Moral relativism can be
unattractive from a pragmatic point of view, as it limits our ability to
engage in discussions that may sometimes lead to convergence on shared
principles. This quietist stance de-emphasizes moral arguments to the
benefit of economic incentives and self-interest.<p>
Why are these debates about moral relativism relevant to AI? People
commonly observe that different cultures have different beliefs when
discussing how to ensure that AIs promote human values. It is essential
not to conflate this observation with normative moral relativism and
conclude that AI developers have no ethical responsibilities. Instead,
they are responsible for ensuring that the values embodied in their AI
systems are beneficial. Rather than a barrier, cultural variation means
that making AIs ethical requires a broad, globally representative
approach.</p>
<h2 id="is-ethics-determined-by-religion">A.2.2 Is Ethics Determined by
Religion?</h2>
<p>Moral relativists may believe that studying ethics is futile because
ethical questions are irresolvable. On the other hand, some people
believe that studying ethics is futile because moral questions are
already solved. This position is most common among those who say that
religion is the source of morality.</p>
<h3 id="divine-command-theory">Divine Command Theory</h3>
<p><strong>Many believe morality depends on God’s will and
commands.</strong> The view called <em>divine command theory</em> says
whether an action is moral is determined solely by God’s commands rather
than any qualities of the action or its consequences. (We use the term
“God” inclusively to refer to the god or gods of any religion.) This
theory suggests that God has the power to create moral obligations and
can change them at will.<p>
While this book does not argue for or against any particular religion,
we do suggest that there are severe problems with equating religion and
morality. One problem is that it creates a problematic understanding of
God.<p>
If you believe there is a god, you likely believe he is more than just
an arbitrary authority figure. Many religious traditions view God as
inherently good. It is precisely because God is good that religion
compels us to follow God’s word. However, if you believe that we should
follow God’s word because God is good, then there must be some moral
qualities (like goodness) that exist independently of God’s rules—thus,
divine command theory is false <span class="citation"
data-cites="plato2004euthyphro">[3]</span>.<p>
To be clear, this is not an argument against believing in God or
religion. It is an argument against equating God or faith with morality.
Both religious people and irreligious people can behave morally or
immorally. That’s why everyone needs to understand the factors that
might make our actions right or wrong.</p>

<h1 id="moral-considerations">A.3 Moral Considerations</h1>
<p>How can we determine whether an action is right or wrong? What are
the kinds of principles and values that should guide our moral
decisions? There are many factors to consider. Here, we’ll focus on a
few—-goodness, constraints, special obligations, and options-—that very
commonly enter into moral decision making.</p>
<h2 id="the-goodness-of-actions-and-their-consequences">A.3.1 The “Goodness”
of Actions and Their Consequences</h2>
<p>Moral decision making often involves considering the values, or
“goods,” that are at stake. These may be intrinsic goods or instrumental
goods.</p>
<p><strong>Intrinsic goods are things that are valuable for their own
sake.</strong> Philosophers disagree about what, if anything, is
intrinsically good, but many argue for the intrinsic value of things
like happiness, love, and knowledge. We value such things simply because
they are valuable—not because they necessarily lead to anything
else.</p>
<p><strong>Instrumental goods are things that are valuable because of
the benefits they provide or the outcomes they achieve.</strong> We
pursue instrumental goods as a means to an end, but not for their own
sake. Money, power, and education are examples of instrumental goods. We
value them because they can lead to other things we value, like
security, influence, career opportunities, or intrinsic goods.</p>
<p><strong>Intrinsically good things are not necessarily instrumentally
good.</strong> Sometimes, intrinsically bad things can be instrumentally
good and intrinsic goods can be instrumentally bad. For instance, many
people believe that honesty is intrinsically good. However, it’s easy to
imagine cases in which honesty can lead to bad outcomes, like hurt
feelings. Suppose a friend has confided in you that they are staying at
a shelter to hide from an abusive partner. If that abusive partner asks
you for your friend’s location, you may think that that honesty is
intrinsically good. However, revealing your friend’s location would be
instrumentally bad, as it may lead to further violence and perhaps even
a risk to your friend’s life. On the other hand, consider medical
treatments like chemotherapy. Chemotherapy is instrumentally good
because it can prolong cancer patients’ lives. Yet, as it requires the
administration of highly toxic drugs into a patient’s body, it could be
seen as harmful, or intrinsically bad. For many people, exercise is
painful, and pain is intrinsically bad, but exercise can be
instrumentally good.</p>
<p><strong>There is no consensus about what is intrinsically
good.</strong> Some philosophers believe that there are many intrinsic
goods. Others believe there is only one value. One common view is that
the only intrinsic good is wellbeing, and everything else is valuable
only insofar as it promotes wellbeing.<p>
Value pluralists believe that there are many intrinsic goods. These
values may include justice, rights, autonomy, and virtues such as
courage. Other philosophers believe there is only one fundamental value.
Among these, one common view is that the only intrinsic good is
wellbeing, and everything else is valuable only insofar as it promotes
wellbeing.</p>
<h3 id="sec:wellbeing">Wellbeing</h3>
<p><strong>Wellbeing is how well a person’s life is going for
them.</strong> It is commonly considered to be intrinsically good,
though there are different accounts of precisely what wellbeing is and
how we can evaluate it. Generally, a person’s wellbeing seems to depend
on the extent to which that person is happy, healthy, and fulfilled.
Three common accounts of wellbeing characterize it as 1) net pleasure
over pain, 2) preference satisfaction, or 3) a collection of objective
goods. Each account is elaborated below.</p>
<p>Some philosophers, known as <em>hedonists</em>, argue that wellbeing
is the achievement of the greatest balance of pleasure and happiness
over pain and suffering. (For simplicity we do not distinguish, in this
chapter, between “pleasure” and “happiness” or between “pain” and
“suffering,” though neither pair is interchangeable.) All else equal,
individuals who experience more pleasure have higher wellbeing. All else
equal, individuals who experience more pain have lower wellbeing.</p>
<p><strong>According to hedonism, pleasure is the only intrinsic
good.</strong> Goods like health, knowledge, and love are instrumentally
valuable. That is, they are only good insofar as they lead to pleasure.
It may feel as though other activities are intrinsically valuable. For
instance, someone who loves literature may feel that studying classic
works is valuable for its own sake. Yet, if the literature lover were
confronted with proof that reading the classics makes them less happy
than they otherwise would be, they might no longer value studying
literature. Hedonists believe that when we think we value certain
activities, we actually value the pleasure they bring us, not the
activities themselves.<p>
Hedonism is a relatively clear and intuitive account of wellbeing. It
seems to apply equally to everyone. That is, while we all may have
different preferences and desires, pleasure seems to be universally
valued. However, some philosophers argue that hedonism is an incomplete
account of wellbeing. They argue there may be other factors that
influence wellbeing, such as the pursuit of knowledge.</p>
<p>Some philosophers claim that what really matters for wellbeing is
that our preferences are satisfied, even if satisfying preferences does
not always lead to pleasurable experiences.<p>
One difficulty for preference-based theories is that there are different
kinds of preferences, and it’s unclear which ones matter. Preferences
can be split into three categories: stated preferences, revealed
preferences, and idealized preferences. Each of these categories can be
informative in different contexts.<p>
To illustrate different kinds of preferences, consider voter preferences
in a democratic election.<p>
In a democratic election, citizens choose which candidate they want to
elect by casting their vote on a ballot. Their choice to vote for a
given candidate can be impacted by a number of different factors.
Perhaps they have an existing political affiliation, are influenced by
social pressures, believe in the candidate’s policies, or maybe they
just like one candidate’s demeanor and personality. Importantly,
citizens may not always vote for the candidate they outwardly support,
and the choice to vote for a specific candidate can change when voters
discover new information.<p>
A voter’s <em>stated preference</em> is the candidate that they state
they support. Voters may express their stated preferences in
conversations, polls, and while campaigning.<p>
When a voter casts their vote on a ballot, they express their
<em>revealed preference</em>. Generally, a voter’s revealed preferences
align with their interests. For example, a voter who supports increased
funding for education might vote for a candidate who wants to increase
budgets for local public schools. A revealed preference is expressed by
your actions, not your words.<p>
People change their preferences upon learning new information.
Uninformed preferences can be reached quickly. For example, a voter
might have an uninformed preference based on a “gut reaction” to a
candidate. Voters can arrive at more <em>idealized preferences</em> once
they have gathered and evaluated all relevant information. They might
not actually do this—few people have the time or ability to perfectly
gather and evaluate all the relevant information that they would require
to find their idealized preferences. However, their preferences can
become more idealized over time. A voter might have an uninformed
preference for Candidate A and, after learning new information about
each candidate’s platform, they may arrive at a more informed preference
for a different candidate. In other words, preferences can change, and
they often do change as people become more informed.</p>
<p><strong>It is easy to learn about people’s stated preferences–—simply
ask them.</strong> Political polls and surveys, for example, are an easy
way to gather information about people’s stated preferences. However,
stated preferences may not always predict what people will actually
choose. A voter may outwardly express support for Candidate X, but when
it comes to casting their ballot, they may vote for Candidate Y.
Similarly, someone might express a stated preference to eat healthier,
but that doesn’t necessarily mean that they will. Their behavior (such
as eating only chocolate for a week) may indicate that their revealed
preference is for unhealthy food.<p>
Revealed preferences can be harder to observe, but they are generally
more useful for predicting people’s behavior. Someone with a stated
preference for vegetables but a revealed preference for chocolate is
more likely to purchase and consume chocolate than vegetables. When
researching consumer behavior, economists often prefer to study
consumers’ revealed preferences (i.e. what they buy) rather than stated
preferences (i.e. what they say they’d like to buy).</p>
<p>Others believe that wellbeing is the achievement of an objective set
of “goods” or “values.” These goods are considered necessary for living
a good life regardless of a person’s individual experiences or
preferences. There is disagreement about which particular goods are
necessary for wellbeing. Commonly proposed goods include happiness,
health, relationships, knowledge, and more. Objective goods theorists
consider these values to be important for wellbeing independently of
individual beliefs and preferences.<p>
There is no uncontroversial way to determine which goods are important
for living a good life. However, this uncertainty is not a unique
problem for objective goods theory. It can be difficult for hedonists to
explain why happiness is the only value that’s important for wellbeing
and for preference satisfaction theorists to determine which preferences
matter most.<p>
While people disagree about which account of wellbeing is correct, most
people agree that wellbeing is an important moral consideration. All
else equal, taking actions that promote wellbeing is generally
considered morally superior to taking actions that reduce
wellbeing.<p>
In the future, it is conceivable that AIs might be conscious and have
preferences but not experience pleasure, which would mean they could
have wellbeing according to the preference satisfaction theorists but
not hedonists. It is also possible that in the future AIs may have
wellbeing according to all three accounts of wellbeing. This would
require that we dramatically reassess our relationship with AIs.</p>
<h2 id="constraints-and-special-obligations">A.3.2 Constraints and Special
Obligations</h2>
<p>We’ve covered the moral consideration of intrinsic goods, and focused
on the intrinsic good wellbeing. Special obligations and constraints are
key considerations when we make ethical decisions.</p>
<p><strong>Special obligations are duties arising from
relationships.</strong> We can incur special obligations when we promise
someone to do something, take a professional position with
responsibilities, have a child, make a romantic commitment to a partner,
and so on. Sometimes we can have special obligations that we did not
volunteer for—a child to its parents, or our duties to fellow
citizens.</p>
<p><strong>Constraints are actions that we are morally prohibited from
taking.</strong> A constraint is something that places limits on our
actions. For example, many people think we’re morally prohibited from
lying, stealing, cheating, harming others, and more.</p>
<p><strong>Constraints often come in the form of rights.</strong> Rights
are claims that individuals may have over their community. For instance,
many people believe that humans have the rights to life, freedom,
privacy, and so on. Some people argue that any individual with the
capacity for experiencing pleasure and pain has rights. Non-human
individuals (including animals and AI systems) might also have certain
rights.<p>
An individual’s rights may require that society intervene in certain
ways to ensure that those rights are fulfilled. For instance, an
individual’s right to food, shelter, or education may require the rest
of society to pay taxes so that the government can ensure that
everyone’s rights are fulfilled. Rights that require certain actions
from others are called positive rights.<p>
Other rights may require that society abstain from certain actions. For
instance, an individual’s right to free speech, privacy, or freedom from
discrimination may require the rest of society to refrain from
censorship, spying, and discriminating. Rights that require others to
abstain from certain behaviors are called negative rights.<p>
Many AI researchers think that, for now, we should avoid accidentally
creating AIs that deserve rights <span class="citation"
data-cites="sebo2022chatbot">[1]</span>; for instance, perhaps all
entities that can experience suffering have natural rights to protect
them from it. Some think we should especially avoid giving them positive
rights; it might be fine to give them rights against being tortured but
not the right to vote. If they come to deserve rights, this would create
many complications and undermine our claim to control.</p>
<h2 id="what-does-it-mean-for-an-action-to-be-right-or-wrong">A.3.3 What does
it mean for an action to be right or wrong?</h2>
<p>Some of the first questions we might ask about ethics are: Are all
actions either right or wrong? Are some simply neutral? Are there other
distinctions we might want to draw between the morality of different
actions?<p>
The answers to these questions, like most moral questions, are the
subject of much debate. Here, we will simply examine what it might mean
for an action to be right or wrong. We will also draw some other useful
distinctions, like the distinction between obligatory and non-obligatory
actions, and between permissible and impermissible actions. These
distinctions will be useful in the following section, when we discuss
the considerations that inform our moral judgments.</p>
<h3 id="options">Options</h3>
<p>Special obligations and constraints tell us what we should not do,
and sometimes, what we must do. Intrinsic goods tell us about things
that would be good, should they happen. But philosophers debate how much
good we are required to do.</p>
<p><strong>Options are moral actions which we are neither required to do
nor forbidden from doing.</strong> Even though it would be good to
donate money, many people do not think people are morally required to
donate. This is an ethical option. If we believe in options, not all
actions are either required or forbidden.<p>
We now break down actions onto a spectrum on which we will simply
examine what it might mean for an action to be right or wrong. We will
also draw some other useful distinctions, like the distinction between
obligatory and non-obligatory actions and between permissible and
impermissible actions.</p>
<p><strong>Obligatory actions are those that we are morally obligated or
required to perform.</strong> We have a moral duty or obligation to
carry out obligatory actions, based on ethical principles. For example,
it is generally considered obligatory to help someone in distress, or
refrain from hurting others.</p>
<p><strong>Non-obligatory actions are actions that are not morally
required or necessary.</strong> Non-obligatory actions may still be
morally good, but they are not considered to be obligatory. For example,
volunteering at a charity organization or donating to a good cause may
be good, but most people don’t consider them to be obligatory.</p>
<p><strong>Permissible actions may be morally good or simply neutral
(i.e. not good or bad).</strong> In general, any action that is not
impermissible is permissible. Moral obligations, of course, are
permissible. We can consider four other actions: volunteering, donating
to charity, eating a sandwich, and taking a walk. These seem
permissible, and can be classified into two categories.<p>
One class of permissible actions is called <em>supererogatory
actions</em>. These may include volunteering or giving to charity. They
are generally considered good; in fact, we tend to believe that the
people who do them deserve praise. On the other hand, we typically don’t
consider the failure to do these actions to be bad. We might think of
supererogatory actions as those that are morally good, but optional;
they go “above and beyond” what is morally required.<p>
Another class of permissible actions is called <em>morally neutral
actions</em>. These may include seemingly inconsequential activities
like eating a sandwich or taking a walk. Most people probably believe
that actions like these are neither right nor wrong.</p>
<p><strong>Impermissible actions are those that are morally prohibited
or unacceptable.</strong> These actions violate moral laws or principles
and are considered wrong. Stealing or attacking someone are generally
considered to be impermissible actions.<p>
</p>
<figure id="fig:action-types">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/action_types.png" class="tb-img-full"/>
<p class="tb-caption">Classes of permissible and non-obligatory actions</p>
<!--<figcaption>Classes of permissible and non-obligatory actions-->
<!--</figcaption>-->
</figure>
<p>Some philosophers believe that all actions fit on a scale like the
one above. At one end of the scale are impermissible actions, like
murder, theft, or exploitation. At the other end are obligatory actions,
like honesty, respect, and not harming others. In between are neutral
and supererogatory actions. These are neither impermissible nor
obligatory. Many people believe that the vast majority of our actions
fall into these two categories. Crucially, in designing ethical AI
systems that operate in the real world, it is important to determine
which actions are obligatory and which actions are impermissible.<p>
However, some philosophers do not believe in options; rather that
actions are all on a spectrum from the least moral to the most moral. We
will learn more about these positions, and others, when we discuss moral
theories later in this chapter.</p>
<h3 id="from-considerations-to-theories">From Considerations to
Theories</h3>
<p><strong>Moral considerations can guide our day-to-day decision
making.</strong> Understanding which factors are morally relevant can
help us think more clearly about what we should do. Of course, we don’t
always stop to consider every factor before making a decision. Rather,
we tend to draw broader conclusions or moral principles based on our
evaluations of specific cases. For instance, once we consider a few
examples of the ways in which stealing can harm others, we might draw
the conclusion that we shouldn’t steal.<p>
The considerations discussed in this section provide a basis on which we
can develop more practical, action-guiding theories about how we should
behave. The types of fundamental considerations in this section comprise
a subfield of ethics called <em>metaethics</em>. Metaethics is the
consideration of questions like “What makes an action right or wrong?”
and “What does it mean to say that an action is right or wrong?” <span
class="citation" data-cites="fisher2014metaethics">[2]</span><p>
These considerations are important in the context of designing AI
systems. In order to respond to situations in an appropriate way, AI
systems need to be able to identify morally relevant features and detect
situations where certain moral principles apply. They would also need to
be able to evaluate and compare the moral worth of potential actions,
taking into account various purported intrinsic goods as well as
normative factors such as special obligations and constraints. The
challenges of designing objectives for AI systems that respect moral
principles are further discussed in the Machine Ethics chapter.<p>
In the following section, we will discuss some popular moral
theories.</p>

<br>
<br>

<h3>References</h3>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-gowans2021moral" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] C.
Gowans, <span>“<span>Moral Relativism</span>,”</span> in <em>The
<span>Stanford</span> encyclopedia of philosophy</em>,
<span>S</span>pring 2021., E. N. Zalta, Ed., <a
href="https://plato.stanford.edu/archives/spr2021/entries/moral-relativism/"
class="uri">https://plato.stanford.edu/archives/spr2021/entries/moral-relativism/</a>;
Metaphysics Research Lab, Stanford University, 2021.</div>
</div>
<div id="ref-curry2019cooperate" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] O.
S. Curry, D. A. Mullins, and H. Whitehouse, <span>“Is it good to
cooperate?: Testing the theory of morality-as-cooperation in 60
societies,”</span> <em>Current Anthropology</em>, vol. 60, no. 1, pp.
47–69, 2019, doi: <a
href="https://doi.org/10.1086/701478">10.1086/701478</a>.</div>
</div>

<div id="ref-plato2004euthyphro" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] C.
Plato, <em>Euthyphro</em>. Kessinger Publishing, 2014.</div>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-sebo2022chatbot" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] J.
Sebo, <span>“Op-ed: What should we do if a chatbot has thoughts and
feelings?”</span> [Online]. Available: <a
href="https://www.latimes.com/opinion/story/2022-06-16/artificial-intelligence-morals-ethics-sentience-thinking">https://www.latimes.com/opinion/story/2022-06-16/artificial-intelligence-morals-ethics-sentience-thinking</a></div>
</div>
<div id="ref-fisher2014metaethics" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] A.
Fisher, <em>Metaethics: An introduction</em>. Routledge, 2011.</div>
</div>
</div>
