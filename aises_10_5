<style type="text/css">
    table.tableLayout{
        margin: auto;
        border: 1px solid;
        border-collapse: collapse;
        border-spacing: 1px;
        caption-side: bottom;
    }

    table.tableLayout tr{
        border: 1px solid;
        border-collapse: collapse;
        padding: 5px;
    }

    table.tableLayout th{
        border: 1px solid;
        border-collapse: collapse;
        padding: 3px;
    }

    table.tableLayout td{
        border: 1px solid;
        padding: 5px;
    }
</style>

<h1 id="beyond-expected-utility-theories">B.5 Beyond Expected Utility
Theories</h1>
<p><strong>Overview.</strong> von Neumann-Morgenstern utility functions
are supposed to capture how an agent chooses among different options,
but they do not always explain how humans actually behave. In reality,
humans are not ideally rational agents. We’d like to model how people,
like those leading the development and regulation of AI, will behave,
and they will often behave in ways that perfect expected utility
maximizers would not. If AIs learn from and interact with humans, they
too might exhibit some aspects of human irrationality. Therefore, we
like to model human behavior more accurately.<p>
Economists have proposed alternative theories, such as Daniel Kahneman
and Amos Tversky’s Prospect theory, to explain why humans deviate from
rationality, as defined under the von Neumann-Morgenstern axioms. In
this section, we examine some major ways that humans break rationality,
as defined under the von Neumann-Morgenstern model, and how non-expected
utility theories help us better understand human choices. Having a
stronger model of human behavior will ultimately help us design AIs that
behave in ways more aligned with humans.</p>
<h2 id="humans-and-rationality">A.5.1 Humans and Rationality</h2>
<p><strong>Humans are not ideally rational agents.</strong> As we
discussed before, rational agents have a thorough understanding of their
own preferences, have complete and stable preferences, and are able to
make which decisions will help them maximally satisfy these preferences.
Human decision-making deviates from ideal rationality. For example, we
prioritize fairness, may have incommensurable values, and value the
desires of others even when these mean we must compromise self-interest.
Human preferences are also unstable, susceptible to persuasion, and can
change over the course of our lives or in light of new information.</p>
<p><strong>Humans often satisfice rather than maximize.</strong>
According to the theory of <em>bounded rationality</em>, humans often
make choices that result in outcomes that are “good enough” rather than
the most ideal. Human rationality is limited by cognitive abilities,
time, and available information, meaning we must frequently make
decisions without considering all possible scenarios and outcomes. Take
an everyday example: suppose a group of friends is deciding what
restaurant to dine at. They will likely choose the first satisfactory
option they come across, rather than methodically consider all possible
places in the city. Thus, humans are said to <em>satisfice</em>—choose
the first option that is satisfactory—rather than exhaustively
maximize.</p>
<p><strong>AIs that are not ideally rational can have varying
safety.</strong> It is plausibly safer for AI systems to be satisficers,
since maximizers may behave in undesirable ways while trying to
relentlessly maximize some metric <span class="citation"
data-cites="taylor2016quantilizers">[1]</span>. As discussed in Proxies,
maximizers may optimize proxies in ways that differ from the idealized
result. However, when AI agents are trained on human data and trained to
interact with humans, they may pick up some of our biases and thereby
not be ideally rational. AIs with many irrational behaviors could be
harder to predict and therefore be harder to control. We will now
proceed to formalize our understanding of why expected utility theory
fails to capture human behavior.</p>
<h2 id="evidence-against-expected-utility-theory">A.5.2 Evidence Against
Expected Utility Theory</h2>
<p><strong>Overview.</strong> Humans violate the von Neumann-Morgenstern
axioms in many different ways. We consider three examples in this
section: the <em>Allais Paradox</em>, <em>fairness</em>, and the
<em>problem of framing</em>, in which humans make inconsistent choices
over lotteries that agents following von Neumann-Morgenstern would be
indifferent between. These problems show violations of von
Neumann-Morgenstern rationality: specifically, the independence and
decomposability axioms.</p>
<h3 id="allais-paradox">Allais Paradox</h3>
<p><strong>Humans are not perfect expected utility maximizers.</strong>
The Allais Paradox, described in 1953 to highlight an inconsistency
between utility theory and human behavior, presents two scenarios where
players must make a choice between two gambles <span class="citation"
data-cites="allais1953comportement">[2]</span>.<p>
</p>
<br>
<table class="tableLayout">
<caption>Table B.3: Allais Paradox: Scenario A</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Choice:</th>
<th style="text-align: left;">Gamble 1</th>
<th style="text-align: left;">Gamble 2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Scenario A</td>
<td style="text-align: left;">100% chance of $1 million</td>
<td style="text-align: left;">10% chance of $5 million % chance of $1
million % chance of $0</td>
</tr>
</tbody>
</table>
<br>
<br>
<table class="tableLayout">
<caption>Table B.4: Allais Paradox: Scenario B</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Choice:</th>
<th style="text-align: left;">Gamble 1</th>
<th style="text-align: left;">Gamble 2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Scenario B</td>
<td style="text-align: left;">11% chance of $1 million % chance of
$0</td>
<td style="text-align: left;">10% chance of $5 million % chance of
$0</td>
</tr>
</tbody>
</table>
<br> <br>
<p>In reality, most players favor Gamble 1 in Scenario A, due to the
certainty of a large payout of $1 million. Simultaneously, they favor
Gamble 2 in Scenario B, since the larger potential payout of $5 million
outweighs its slightly lower likelihood. Both these preferences are
individually sensible and unproblematic. However, holding both
preferences simultaneously is in violation of von Neumann-Morgenstern’s
independence axiom, and thus inconsistent with expected utility
theory.</p>
<p><strong>Humans violate the independence axiom.</strong> We explained
above that the independence axiom holds that preferences between two
lotteries are not impacted by the addition of equal probabilities of a
third, independent lottery to each lottery. It follows that we can
subtract the same lottery from two equivalent lotteries and preserve the
original preference.<p>
In Scenario A, an 89<span class="math inline">%</span> chance of winning
<span class="math inline">$</span>1 million is common to both choices.
Therefore, the decision between Gamble 1 and Gamble 2 in Scenario A is
equivalent to the reduced game described, in which the 100<span
class="math inline">%</span> chance of winning <span
class="math inline">$</span>1 million has simply been divided into an
89<span class="math inline">%</span> chance and an 11<span
class="math inline">%</span> chance of winning the same <span
class="math inline">$</span>1 million.<p>
</p>
<br>
<table class="tableLayout">
<caption>Table B.5: Allais Paradox: Scenario A reduced</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Gamble 1</th>
<th style="text-align: left;">Gamble 2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Scenario A</td>
<td style="text-align: left;">89% chance of $1 million % chance of $1
million</td>
<td style="text-align: left;">10% chance of $5 million % chance of $ 1
million % chance of $0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Scenario A reduced</td>
<td style="text-align: left;">11% chance of 1 million</td>
<td style="text-align: left;">10% chance of $ 5 million % chance of
$0</td>
</tr>
</tbody>
</table>
<br> <br>
<p>Similarly, there is a common lottery between Gamble 1 and Gamble 2 in
Scenario B. We can ignore an 89<span class="math inline">%</span> chance
of winning <span class="math inline">$</span>0 from both choices and we
will be left with the reduced game described, in which the 89<span
class="math inline">%</span> chance of winning <span
class="math inline">$</span>0 has simply been ignored.<p>
</p>
<table class="tableLayout">
<caption>Table B.6: Allais Paradox: Scenario B reduced</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Gamble 1</th>
<th style="text-align: left;">Gamble 2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Scenario B</td>
<td style="text-align: left;">11% chance of $1 million % chance of
$0</td>
<td style="text-align: left;">10% chance of $5 million % chance of
$0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Scenario B reduced</td>
<td style="text-align: left;">11% chance of $1 million</td>
<td style="text-align: left;">10% chance of $5 million % chance of
$0</td>
</tr>
</tbody>
</table>
<br><br>
<p>Scenario A Reduced and Scenario B Reduced are exactly the same!
Therefore, a rational agent should be consistent and select the same
gamble in either simplified game. Since the simplified scenarios are
equivalent to the original scenarios via the independence axiom—adding
third lotteries to both options should make no difference to the choice
in either scenario—we can conclude that a rational agent should also
select the same gamble in Scenario A and Scenario B. In reality, people
overwhelmingly favor Gamble 1 in Scenario A and Gamble 2 in Scenario B.
This behavior is inconsistent with rational behavior under the
independence axiom.<p>
We can interpret this discrepancy in two ways. We could assume that
humans are making an error of judgment. Or, we could conclude that human
choice isn’t unreasonable, but that the von Neumann-Morgenstern axioms
are a flawed characterization of rationality. Indeed, Kahneman and
Tversky concluded that the expected utility model fails to capture
important nuances in human decision making <span class="citation"
data-cites="kahneman2011thinking">[3]</span>.</p>
<h3 id="fairness">Fairness</h3>
<p><strong>It is sometimes thought that the independence axiom is at
odds with concepts of fairness.</strong> The independence axiom tells us
that adding equal probabilities of a third lottery makes no difference
to an agent’s preference between two other lotteries. However, in some
cases, we do care what else could happen: we make all-things-considered
judgments of how we value outcomes. In particular, we care about
fairness <span class="citation"
data-cites="mccarthy2016probability">[4]</span>.</p>
<p><strong>If an agent cares about fairness, they may be forced to
abandon independence.</strong> Suppose Rachel, on her deathbed, has to
leave everything she owns to one person. She is indifferent between
everything going to her son and everything going to her daughter, but
would like to treat them equally. There is a 50<span
class="math inline">%</span> chance that the law will change such that
everything goes to her son, no matter what her will says. She is now
considering two options, which we can write down as lotteries.</p>
<ol>
<li><p>She leaves everything to her daughter. Let this be the “fair
lottery” <span
class="math inline"><em>L</em><sub><em>F</em></sub></span>, wherein her
daughter receives everything with probability 0.5 (once her will is
executed), and her son receives everything with probability 0.5 (once
the laws change).</p></li>
<li><p>She leaves everything to her son. Let this be the “unfair
lottery” <span
class="math inline"><em>L</em><sub><em>U</em></sub></span>, wherein her
son receives everything with probability 1, since he receives it once
her will is executed or once the laws change.</p></li>
</ol>
<p>According to the independence axiom, if she is indifferent between
her son and daughter receiving her possessions for sure, then she should
also be indifferent between the above two lotteries because we can
obtain them by adding equal probabilities of a different outcome—a
50<span class="math inline">%</span> chance her son gets everything—to
the original choice. However, if she cares at all about fairness, then
she should prefer <span
class="math inline"><em>L</em><sub><em>F</em></sub></span> over <span
class="math inline"><em>L</em><sub><em>U</em></sub></span>—a preference
that is incompatible with the independence axiom!</p>
<p><strong>Again, we see that the von Neumann-Morgenstern axioms lack
descriptive power.</strong> While, in principle, adding alternatives
should be irrelevant, they are not always so. In the case of the Allais
paradox above, we see that it changes how we think about monetary
lotteries—this is sometimes attributed to factors like avoiding regret.
In this case, we are concerned with fairness. Next, we will consider how
humans systematically violate rationality based on the description of
options.</p>
<h3 id="framing">Framing</h3>
<p><strong>Human decision making is swayed by the presentation of
choices.</strong> Kahneman and Tversky noticed that humans will make
different decisions depending on how options are presented, even when
the underlying lotteries remain unchanged <span class="citation"
data-cites="tversky1981framing">[5]</span>. While a vNM-rational
decision maker would ignore the presentation of options and focus only
on its probabilities and outcomes, humans tend to be unaware of the
large degree of influence that <em>framing</em> has on their decision
making. Furthermore, human decision making usually uses vague feelings
of subjective probability rather than well-considered mathematically
defined lotteries. Consider the following example:<p>
An illness is expected to kill 600 people if left untreated.
Participants playing the role of health officials must choose between
two policy options in two different scenarios. These options are
presented in the table below.<p>
</p>
<br>
<table class="tableLayout">
<caption>Table B.7: Framing effects: Scenario A</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Choice:</th>
<th style="text-align: left;">Policy A1</th>
<th style="text-align: left;">Policy A2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Scenario A</td>
<td style="text-align: left;">Save 200 people with certainty.</td>
<td style="text-align: left;">1/3 chance save 600 people. /3 chance
saves no one.</td>
</tr>
</tbody>
</table>
<br> <br>
<table class="tableLayout">
<caption>Table B.8: Framing effects: Scenario B</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Choice:</th>
<th style="text-align: left;">Policy B1</th>
<th style="text-align: left;">Policy B2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Scenario B</td>
<td style="text-align: left;">400 people die with certainty.</td>
<td style="text-align: left;">1/3 chance save of no deaths. /3 chance of
600 deaths.</td>
</tr>
</tbody>
</table>
<br>
<p><strong>Framing effects violate von Neumann-Morgenstern
rationality.</strong> Participants tend to choose Policy A1 in Scenario
A, because of the certainty of saving lives. Participants also tend to
choose Policy B2 in Scenario B, because of the possibility of averting
more death and not condemning people to certain death. However, the only
difference between Scenario A and B is the manner of presentation, or
framing. Scenario A is presented in terms of lives saved, and Scenario B
in terms of deaths caused. The equivalence is shown in this table.</p>
<br>
<table class="tableLayout">
<caption>Table B.9: Framing effects: Scenarios A and B compared</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Choice:</th>
<th style="text-align: left;">Policy A1/B1</th>
<th style="text-align: left;">Policy A2 B2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Scenario A</td>
<td style="text-align: left;">Save 200 people with certainty (and so
causes 400 deaths for sure).</td>
<td style="text-align: left;">1/3 chance save 600 people (or no deaths).
/3 chance saves no one (or 600 deaths).</td>
</tr>
<tr class="even">
<td style="text-align: left;">Scenario B</td>
<td style="text-align: left;">Causes 400 deaths with certainty (and so
saves 200 people for sure)</td>
<td style="text-align: left;">1/3 chance save of no deaths (or saving
600 people). /3 chance of 600 deaths (or saving no one at all).</td>
</tr>
</tbody>
</table>
<br> <br>
<p>Together, the choice of Policy A1 in Scenario A and Policy B2 in
Scenario B violate a requirement of von Neumann-Morgenstern rationality:
that an agent be indifferent between two lotteries with the same
probabilities and outcomes, even if they are presented differently. In
reality, an individual’s habits, environment, and cognitive biases can
cause different responses to framings of the same underlying lottery.
These are ways individuals can deviate from expected utility theory.</p>
<h2 id="prospect-theory">A.5.3 Prospect Theory</h2>
<p><strong>Prospect theory is a prominent non-expected utility model of
human behavior.</strong> Prospect theory seeks to accurately describe
how people make choices in risky situations by providing a psychological
model of decision making. The model’s features are designed to more
accurately describe human behavior in situations when facing risk,
rather than provide a description of how humans “should” behave, like in
the vNM utility model. Thus, prospect theory explains common behavioral
patterns that are considered irrational in expected utility theory <span
class="citation" data-cites="kahneman2013prospect">[6]</span>.</p>
<p><strong>Prospect theory is a multi-stage decision-making
model.</strong> Prospect theory views decision making over two stages:
editing and evaluation. In the editing stage, outcomes are re-framed as
either gains or losses instead of just final wealth. This is important
because people can perceive the same outcome differently based on how
the problem is presented and their own biases. This stage also accounts
for framing effects. In the evaluation stage, gains and losses are
multiplied by weighted probabilities to determine the preferred outcome.
This stage takes into account two factors: a <em>value function</em>,
how people value different outcomes, and a <em>weighting function</em>,
how people weigh the probabilities of each outcome. Next, we will
explore how these two functions help to explain how people make
decisions in uncertain situations.</p>

<figure id="fig:company-life">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/prospecttheory.png" class="tb-img-full" style="width: 70%"/>
<p class="tb-caption">Figure B.7: Prospect theory’s value function is steeper in losses than in gains.</p>
</figure>

<p><strong>Prospect theory’s value function approximates how humans
think about wealth.</strong> A value function assigns value to an
outcome, much like a Bernoulli utility function assigns utility to an
outcome. In prospect theory, the value function has a few key
characteristics. The input is defined in gains and losses, modeling the
idea that people are sensitive to changes in wealth, not only to their
level of wealth. The curve is sigmoid to reflect that people are risk
seeking towards losses and risk averse towards gains. That is, people
like a chance at avoiding losses but dislike a chance of losing gains.
The curve is steeper in losses since people are modeled more sensitive
to a loss compared to a gain of equal amount.<p>
</p>
<p><strong>Prospect theory’s decision weights describe how humans think
about probabilities.</strong> A decision weight is the scaling factor
applied to an outcome that quantifies how much that outcome contributes
to the decision. In expected utility theory, decision weights are just
probabilities of outcomes. Instead of assuming people accurately assess
the likelihood of outcomes, however, prospect theory accounts for the
way humans actually process probabilities. People often overestimate the
risk of unlikely events with extreme consequences. Humans significantly
overestimate the risk of dying in a shark attack, possibly because of
the graphic nature of the attacks and their over-representation in pop
media, when in reality the true probability of death by shark attack is
quite low. Humans also place a greater weight on relative certainty:
people are usually willing to pay much more to improve their odds from
0<span class="math inline">%</span> to 1<span
class="math inline">%</span> than from 1<span
class="math inline">%</span> to 2<span class="math inline">%</span>,
since certainty of failure is removed. Prospect theory’s proposed
weighting for probabilities is shown in the next figure. This curve
illustrates how people tend to persistently overestimate relatively
small probabilities while persistently underestimating relatively large
probabilities.<p>
</p>

<figure id="fig:human-value">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/humanvalue.png" class="tb-img-full" style="width: 70%"/>
<p class="tb-caption">Figure B.8:  Humans often perceive probabilities incorrectly, overestimating the likelihood of low-probability events and underestimating the likelihood of high-probability events.</p>
</figure>


<h2 id="models-of-decision-making">A.5.4 Models of Decision Making</h2>
<p><strong>Generalized decision-making models.</strong> We can put the
above together to describe many different types of decision-making
models more than expected utility theory. Recall that expected utility
is of the form: <span
class="math display"><em>U</em>(<em>L</em>) = <em>p</em><sub>1</sub> ⋅ <em>u</em>(<em>o</em><sub>1</sub>) + <em>p</em><sub>2</sub> ⋅ <em>u</em>(<em>o</em><sub>2</sub>) + ⋯ + <em>p</em><sub><em>n</em></sub> ⋅ <em>u</em>(<em>o</em><sub><em>n</em></sub>).</span>
for a lottery <span class="math inline"><em>L</em></span> with outcomes
<span class="math inline"><em>o</em><sub><em>i</em></sub></span> and
associated probabilities <span class="math inline"><em>p</em></span>,
over a utility function <span class="math inline"><em>u</em></span>. We
can incorporate value functions and weighting functions to construct a
theory of decision making that is more general than expected utility
theory while following the same structure of adding together the
products of decision weights and values: <span
class="math display"><em>V</em>(<em>L</em>) = <em>w</em>(<em>p</em><sub>1</sub>) ⋅ <em>v</em>(<em>o</em><sub>1</sub>) + <em>w</em>(<em>p</em><sub>2</sub>) ⋅ <em>v</em>(<em>o</em><sub>2</sub>) + ⋯ + <em>w</em>(<em>p</em><sub><em>n</em></sub>) ⋅ <em>v</em>(<em>o</em><sub><em>n</em></sub>).</span>
Here, <span class="math inline"><em>V</em></span> is the value assigned
to the lottery. We have also added <span
class="math inline"><em>w</em></span><em><sub>i</sub></em>, a weighting
function that transforms each probability into a corresponding decision
weight, and <span
class="math inline"><em>v</em></span><em><sub>i</sub></em>, a value
function which—much like a utility function—values each outcome.</p>
<p><strong>Prospect theory, formalized.</strong> Prospect theory uses
this structure while using a slightly different value function which
evaluates gains and losses in wealth, rather than total wealth. The
S-shaped prospect theory value function is represented as <span
class="math inline"><em>v</em><sub><em>p</em></sub>(<em>o</em><sub><em>i</em></sub>−<em>o</em><sub>0</sub>)</span>,
with <span class="math inline"><em>o</em><sub><em>i</em></sub></span>
representing the outcome being evaluated, and <span
class="math inline"><em>o</em><sub>0</sub></span> representing the
agent’s initial state: <span
class="math display"><em>V</em><sub><em>p</em></sub> = <em>w</em><sub><em>p</em></sub>(<em>p</em><sub>1</sub>) ⋅ <em>v</em><sub><em>p</em></sub>(<em>o</em><sub>1</sub>−<em>o</em><sub>0</sub>) + ⋯ + <em>w</em><sub><em>p</em></sub>(<em>p</em><sub><em>n</em></sub>) ⋅ <em>v</em><sub><em>p</em></sub>(<em>o</em><sub><em>n</em></sub>−<em>o</em><sub>0</sub>).</span>
In monetary lotteries, this would contain <span
class="math inline"><em>v</em><sub><em>p</em></sub>(<em>w</em><sub><em>i</em></sub>−<em>w</em><sub>0</sub>)</span>,
which tells us that the value function considers the loss or gain in
wealth in each possible outcome. Prospect theory incorporates the
specific weighting and value functions determined by Kahneman and
Tversky’s research on human behavior. We can modify the model by
substituting each function with functions of our own choosing.</p>
<p><strong>Summary.</strong> In this section, we considered the Allais
Paradox, a thought experiment on fairness, and two instances where the
von Neumann-Morgenstern axioms fail to capture human preferences. We
also examined a study conducted by Kahneman and Tversky on framing
effects, where humans behave in a clearly irrational manner. We then
examined the value function and the weighting function, which are the
two main innovations that comprise prospect theory and other
non-expected utility theories that seek to capture insights into human
behavior where conventional theory fails.<p>
Understanding the limitations of expected utility theory, and having
more descriptive models of human behavior helps us understand how
humans, human-led organizations, and AIs imitating humans will behave.
In this chapter, and in the Single Agent Safety chapter, we present reasons that
expected-utility maximizers can be dangerous. Non-expected utility
theories provide some concepts to consider when designing agents that
are not expected-utility maximizers.</p>

<br>
<br>
<h3>References</h3>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-taylor2016quantilizers" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] J.
Taylor, <span>“Quantilizers: A safer alternative to maximizers for
limited optimization.”</span> in <em>AAAI workshop: AI, ethics, and
society</em>, 2016.</div>
</div>
<div id="ref-allais1953comportement" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] M.
Allais, <span>“Le comportement de l’homme rationnel devant le risque:
Critique des postulats et axiomes de l’ecole americaine,”</span>
<em>Econometrica</em>, vol. 21, no. 4, pp. 503–546, 1953, Accessed: Oct.
16, 2023. [Online]. Available: <a
href="http://www.jstor.org/stable/1907921">http://www.jstor.org/stable/1907921</a></div>
</div>
<div id="ref-kahneman2011thinking" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] D.
Kahneman, <em>Thinking, fast and slow</em>. Farrar, Straus; Giroux,
2011.</div>
</div>
<div id="ref-mccarthy2016probability" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] D.
McCarthy, <span>“Probability in ethics,”</span> in <em>The oxford
handbook of philosophy and probability</em>, A. Ha?jek and C. Hitchcock,
Eds., Oxford University Press, 2016, pp. 705–737.</div>
</div>
<div id="ref-tversky1981framing" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] A.
Tversky and D. Kahneman, <span>“The framing of decisions and the
psychology of choice,”</span> <em>science</em>, vol. 211, no. 4481, pp.
453–458, 1981.</div>
</div>
<div id="ref-kahneman2013prospect" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] D.
Kahneman and A. Tversky, <span>“Prospect theory: An analysis of decision
under risk,”</span> in <em>Handbook of the fundamentals of financial
decision making: Part i</em>, World Scientific, 2013, pp. 99–127.</div>
</div>
</div>
