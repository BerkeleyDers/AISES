<h1 id="introduction">Introduction</h1>
<h4 id="how-should-we-direct-ais-to-promote-human-values">How should we
direct AIs to promote human values?</h4>
<p>As we continue to develop powerful AI systems, it is crucial to
ensure that they are safe and beneficial for humanity. In this chapter,
we discuss the challenges of embedding ethics into AIs.<br />
</p>
<h4 id="many-people-have-incoherent-views-on-machine-ethics.">Many
people have incoherent views on machine ethics.</h4>
<p>People often talk about what AIs should do to promote human values.
They may agree with many of the following:</p>
<ol>
<li><p>AIs should do what you tell them to do.</p></li>
<li><p>AIs should promote what you choose to do.</p></li>
<li><p>AIs should do what’s fair.</p></li>
<li><p>AIs should do what a democratic process tells them to
do.</p></li>
<li><p>AIs should figure out what is moral, then do that.</p></li>
<li><p>AIs should do what is objectively good for you.</p></li>
<li><p>AIs should do what would make people happy.</p></li>
</ol>
<p>All of these seem like reasonable answers. At least at first glance,
these all seem like excellent goals for machine ethics. However, not all
of these are compatible, because they make <em>different normative
assumptions</em> and <em>require different technical
implementations</em>. Other suggestions such as “AIs should follow human
intentions” are highly vague. Put straightforwardly, while these sound
attractive and similar, they are not the same thing.<br />
This should challenge the notion that machine ethics is a
straightforward problem with a simple solution, and that the real
challenges lie only elsewhere, such as in capabilities development. In
fact, those who believe the machine ethics question is easily solvable
may find themselves grappling with inconsistencies and confusion when
confronted with the diverse range of perspectives and assumptions
involved.</p>
<h4 id="preview.">Preview.</h4>
<p>In this chapter, we will consider the intricacies of machine ethics,
attempting to understand which of these answers, if any, takes us closer
to creating safe and beneficial AIs. We will draw on the fundamental
concepts from the study of AI, ethics, and utility functions discussed
in prior chapters. The following subsections will provide an overview of
various aspects of machine ethics:</p>
<ol>
<li><p><strong>Law</strong>: Why not have AIs follow the law? We examine
whether we can design AIs that follow existing legal frameworks,
considering that law is a legitimate aggregation of human values that is
time-tested and comprehensive while being both specific and adaptable.
We will lay out the challenges of the occasional silence, immorality, or
unrepresentativeness of established law.</p></li>
<li><p><strong>Fairness</strong>: Should we make AIs be fair? We explore
fairness in AI systems and the challenges associated with ensuring
outcomes created by AIs are fair. We will discuss different definitions
fairness and see how they are incompatible, as well as consider
approaches to mitigating biases.</p></li>
<li><p><strong>Economic Engine</strong>: Should we let the economy
decide what AIs will be like? We consider the potential impact the use
of AI might have on the economy and society, how economic forces are
shaping AI development, and why letting the economic engine create AIs
without substantial regulation might not be a good idea.</p></li>
<li><p><strong>Preferences</strong>: Should we have AIs satisfy people’s
preferences? We investigate how AI systems can be created to satisfy
individual preferences, focusing on the key question of which
preferences. We focus on the challenges of deciding between revealed,
stated, and idealized preferences.</p></li>
<li><p><strong>Happiness</strong>: Should we have AIs make people happy?
We ask how we can use AIs to promote human happiness. After considering
the benefits and pitfalls of using happiness as a proxy for wellbeing,
we discuss an alternative conception of wellbeing.</p></li>
<li><p><strong>Social Welfare Functions</strong>: Should we have AIs
maximize total wellbeing? We look at how to aggregate wellbeing across
society, focusing on social welfare functions. We discuss what social
welfare functions are and how to trade-off between equity and efficiency
in a principled way.</p></li>
<li><p><strong>Moral Parliament</strong>: Should we use a moral
parliament to guide AI decision-making? We incorporate ideas from moral
uncertainty by using a moral parliament, where ethical decisions
regarding AI systems are made by simulating democratic
processes.</p></li>
</ol>
<p>By examining these key aspects of machine ethics, we aim to provide
an introductory understanding of the challenges and potential solutions
in creating safe AI systems. Through the exploration of a variety of
different concepts, we can examine the problems we face in deciding how
to create beneficial AI and lay the foundation for responsible AI
development and deployment.</p>
<h1 id="law">Law</h1>
<p><strong>Why not have AIs follow the law?</strong> In this textbook,
we have often turned to ethics to answer the question of how we can
ensure AI is safe and beneficial. But some argue that simply getting AIs
to follow the law is a better solution.<br />
The law has three features which give it an advantage over ethics as a
model for safe and beneficial AI. In democratic countries, the law is a
<em>legitimate</em> representation of our moral codes: at least in
theory, it is a democratically endorsed record of our shared values. Law
is <em>time-tested</em> and <em>comprehensive</em>; it has developed
over many generations to adjudicate the areas where humans have
consequential disagreements. Finally, legal language can be
<em>specific</em> and <strong>adaptable</strong> to new contexts,
comparing favorably to ethical language, which can often be interpreted
in diverging ways.<br />
The next subsection will expand on these features of the law. We will
see how these features contrast favorably with ethics before arguing
that we do need ethics after all but alongside the law.</p>
<h2 id="the-case-for-law">The Case For Law</h2>
<h3 id="legitimate-aggregation-of-values">Legitimate Aggregation of
Values</h3>
<h4
id="in-a-democratic-country-the-law-is-influenced-by-the-opinions-of-the-populace.">In
a democratic country, the law is influenced by the opinions of the
populace.</h4>
<p>Democratic citizens are able to elect judges and vote on new laws,
directly or through representatives. If they don’t like part of an
existing law, they have a range of legal means, such as advocating,
protesting, and voting, to change it. Even though the law at any given
time won’t perfectly reflect the values of the citizenry, the method of
arriving at law is usually <em>legitimate</em>. In other words, the
citizens have input into the law.</p>
<h4
id="legitimacy-provides-the-law-with-a-clear-advantage-over-ethics.">Legitimacy
provides the law with a clear advantage over ethics.</h4>
<p>The law provides a collection of rules and standards that enable us
to differentiate illegal from legal actions. Ethics, on the other hand,
isn’t standardized or codified. To determine ethical and unethical
actions, we have to either pick an ethical theory to follow, or decide
how to weigh the differing opinions of multiple theories that we think
might be true. But any of these options are likely to be more
controversial than simply following the law. Ethics has no in-built
method of democratic agreement.<br />
However, just following the law isn’t a perfect solution: there will
always be an act of interpretation between the written law and its
application in a particular situation. There is often no agreement over
the procedure for this interpretation (for example, in US constitutional
cases). Therefore, even if AI systems were created in a way that bound
them to follow the law, a legal system with human decision-makers would
have to remain part of the process. The law is only legitimate when
interpreted by someone democratically appointed or subject to democratic
critique.</p>
<h3 id="time-tested-and-comprehensive">Time-Tested and
Comprehensive</h3>
<p><strong>Systems of law have evolved over generations.</strong> With
each generation, new people are given the job of creating, enforcing,
and interpreting the law. The law covers a huge range of issues and
incorporates a wide range of distinctions. Because these bodies of law
have been evolving for so long, the information encoded in the law is a
record of what has worked for many people and is often considered an
approximation of their values. This makes the law a particularly
promising resource for aligning AI systems with the values of the
population.</p>
<h3 id="rules-and-standards">Rules and Standards</h3>
<h4
id="naively-we-might-think-of-the-law-as-a-system-of-rules.">Naively, we
might think of the law as a system of rules.</h4>
<p>“Law” seems almost synonymous with “rule” in our language. When we
talk about “the laws of physics” or “natural laws” in general, we mean
something rigid and inflexible—when X happens, Y will follow. This is
partly true: inflexible rules are a part of the law. For instance, take
the rule: “If someone drives faster than the speed limit of 70mph, they
will be fined $200.” In this case, there is an objective trigger
(driving faster than 70mph) and a clear directive (a $200 fine). There
isn’t much room for the judge to interpret the rule differently. This
gives the lawmaker predictable control over how the law will be carried
out.</p>
<h4 id="a-law-based-on-rules-alone-would-be-flawed.">A law based on
rules alone would be flawed.</h4>
<p>However, a fixed speeding rule would mean fining someone who was
accelerating momentarily to avoid hitting a pedestrian and not fining
someone who continued to drive at the maximum speed limit around a blind
turn, creating a danger for other drivers. Rules are always
over-inclusive (they will apply to some cases we would rather not be
illegal) and under-inclusive (they won’t apply to all cases we would
like to be illegal).</p>
<h4
id="to-remedy-this-problem-a-law-can-instead-be-based-on-a-standard.">To
remedy this problem, a law can instead be based on a standard.</h4>
<p>In the speeding case, a standard could be “when someone is driving
unreasonably, they will be fined in proportion to the harm they pose.” A
judge could apply this standard to get the correct result in both cases
above (speeding to avoid an accident and going full speed around a blind
turn). Standards have their own problems: with standards rather than
rules, the judge is empowered to interpret the standards based on their
own opinion, allowing them to act in ways that diverge from the
lawmaker’s intentions.</p>
<h4 id="the-law-uses-rules-and-standards.">The law uses rules and
standards.</h4>
<p>Using rules and standards alongside each other, the law can find the
best equilibrium between carrying out the lawmaker’s intentions and
accounting for situations they didn’t foresee <span class="citation"
data-cites="clermont2020rules"></span>. This gives the law an advantage
in the problem of maintaining human control of AI systems by displaying
the right level of ambiguity.<br />
Law is less ambiguous than ethical language, which can be very
ambiguous. Phrases like “do the right thing”, “act virtuously” or “make
sure you are acting consistently” can mean different things to different
people. In contrast, it is more flexible than programming languages,
which are brittle and designed to only fit into particular contexts.
Legal language can maintain a middle ground between rigid rules and more
sensible standards.<br />
</p>
<figure>
<img src="images/machine_ethics/image21.png" />
<figcaption>Ambiguity vs. flexibility in languages</figcaption>
</figure>
<h3 id="specific-and-adaptable">Specific and Adaptable</h3>
<p>We can apply these insights about rules and standards in law to two
core problems in controlling AI systems: <em>misinterpretation</em> and
<em>gaming</em>. The law is specific enough to avoid misinterpretation
and adaptable enough to prevent many forms of gaming.</p>
<h4
id="given-commands-in-natural-language-ais-might-interpret-them-literally.">Given
commands in natural language, AIs might interpret them literally.</h4>
<p>The misinterpretation problem arises when the literal interpretation
of our commands differs from our intended interpretation. In AI safety
discourse, we see this problem raised by many thinkers; for example,
Stuart Russell raised the concern that an AI, if asked to develop a cure
for cancer, might experiment on people, giving them tumors as part of
carrying out the request <span class="citation"
data-cites="Russell2019HumanCA"></span>. A narrow, literal
interpretation of “develop a cure for cancer,” which doesn’t take any of
our typical considerations into account, could lead to this outcome.
Misinterpretation risks are like wishes to a genie: we might get what we
ask for but not what we want.</p>
<h4
id="the-capabilities-of-llms-give-us-some-reason-to-see-this-risk-as-unlikely.">The
capabilities of LLMs give us some reason to see this risk as
unlikely.</h4>
<p>Fortunately, there are good reasons not to be concerned about this
particular risk from AI. Our experience with large language models has
shown that by being trained on human-generated language data, AIs can
act on the meaning of our sentences in a way that is approximately
similar to the way that a human speaker of the language would. The
Happiness section below discusses two other systems that can predict
human responses to video and text. Suppose similar systems are used in
the future. In that case, we should not worry about extreme actions
emerging from a misinterpretation of perfectly normal requests—although
we might still worry that these emerge from optimization pressures.</p>
<h4
id="however-we-still-face-the-risk-of-ais-gaming-our-commands-to-get-what-they-want.">However,
we still face the risk of AIs gaming our commands to get what they
want.</h4>
<p>Stuart Russell raises a different concern with AI: gaming <span
class="citation" data-cites="Russell2019HumanCA"></span>. An AI system
may ‘play’ the system or rules, akin to strategizing in a game, to
achieve its objectives in unexpected or undesired ways. He gives the
example of tax codes. Humans have been designing tax laws for 6000
years, yet many still avoid taxation. Creating a tax code not
susceptible to gaming is particularly difficult because individuals are
incentivized to avoid paying taxes wherever possible. If our track
record with creating rules to constrain each other is so bad, then we
might be pessimistic about constraining AI systems that might have goals
that we don’t understand.</p>
<h4 id="a-partial-answer-is-found-in-the-use-of-standards.">A partial
answer is found in the use of standards.</h4>
<p>A standard can cover much more ground than a rule. A well-formulated
standard can lead to the law finding the right answer, even in new and
unique cases. Such approaches are sometimes applied in the case of
taxes. In the UK, for example, there is a rule against tax arrangements
that “are abusive.” This is not an objective trigger: it is up to a
judge to decide what is “abusive.” An AI system trained to follow the
law will be accountable to rules and standards.<br />
Nothing we have considered will be able to eliminate the possibility of
gaming. However, the problem of misinterpretation is less concerning
now, and we can be hopeful about using standards to maintain control of
AI systems.</p>
<h2 id="the-need-for-ethics">The Need for Ethics</h2>
<p>This subsection will discuss why ethics is still indispensable in
creating safe and beneficial AI, even though law is a powerful tool.
Firstly, though the law is comprehensive, there are important areas of
human life where it gives no advice. Secondly, it is common for laws to
be immoral, even by the standards of the residents of the country that
made them. Finally, the law is unlikely to be the most legitimate
conceivable system, even if it is our best one.</p>
<h3 id="silent-law">Silent Law</h3>
<p><strong>The law is a set of constraints, not a complete guide to
behavior.</strong> The law doesn’t cover every aspect of human life, and
certainly not everything we need to constrain AI. Sometimes, this is
accidental, but sometimes, the law is intentionally silent. We can call
these zones of discretion: areas of behavior that the law doesn’t
constrain; for example, a state’s economic policy and the content of
most contracts between private individuals. The law puts some limits on
these areas of behavior. Still, lawmakers intentionally leave free space
to enable actors to make their own choices and to avoid the law becoming
too burdensome.</p>
<h4
id="the-law-often-stops-short-of-compelling-us-to-do-clearly-good-things.">The
law often stops short of compelling us to do clearly good things.</h4>
<p>It is generally seen as good if someone steps in to rescue a stranger
who is in danger. However, under US common law, the duty to rescue only
applies in certain cases where we have special responsibility for
someone. We face a penalty if we fail to rescue our spouse or employee,
but there is no law against failing to rescue a stranger, even when it
would be at no cost to us. Giving humans this kind of discretion may not
have terrible outcomes, because many people have strong ethical
intuitions anyway, and would carry out the rescue. But an AI that was
only following the law would not unless it had a strong ethical drive as
well. In cases like rescue, where AI systems may be <em>more</em>
capable than humans to help, we could be passing up a major benefit by
asking the AI to only follow the law. Likewise, in the US, AIs may be
required to avoid uttering copyrighted content and libelous claims, but
doxing, instructions for building bombs, or advice for how to break the
law can be legal, even if it is not ethical.<br />
Conversely, by constraining AI with laws rather than guiding it with
ethics, we risk it acting in undesirable ways in zones of discretion. AI
could recommend potentially harmful economic policies, trick humans into
regrettable contracts, and pursue legal but harmful business practices.
Without ethics, a law-abiding AI could carry out great harm because the
law is silent in situations where we may want to guide behavior
nonetheless.</p>
<h3 id="immoral-law">Immoral Law</h3>
<h4
id="the-law-can-be-immoral-even-if-it-is-created-legitimately-and-interpreted-correctly.">The
law can be immoral even if it is created legitimately and interpreted
correctly.</h4>
<p>Democratic majorities can pass laws that a large number of fellow
citizens think are immoral, or that will seem immoral to future
populations of that country, such as the legalization of wars later
regretted, legal slavery, legal discrimination on the basis of race and
gender, and various other controversial laws which are later deemed
morally wrong. There is also often a significant time delay between the
moral opinions of a population changing, and the law changing to reflect
them <span class="citation" data-cites="nay2023law"></span>. This means
that laws formed in democracies can fall short of being moral, even in
the eyes of the citizens of the country that made them.<br />
If solely constrained by a country’s fallible democratic laws, future AI
systems could behave in ways most of the world would consider immoral.
To mitigate this risk, it is necessary to train AI to respect the
ethical perspectives of those affected by its actions.</p>
<h3 id="unrepresentative-law">Unrepresentative Law</h3>
<p><strong>Law isn’t the only way, or even necessarily the best way, to
arrive at an aggregation of our values.</strong> Not all judges and
legal professionals agree that the law <em>should</em> capture the
values of the populace. Many think that legal professionals know better
than the public, and that laws should be insulated from the changing
moral opinions of the electorate. This suggests that we might be able to
find, or conceive of, more representative ways of capturing the values
of the populace. Current alternatives like surveys or citizens’
assemblies are useful for some purposes, such as determining preferences
on specific issues or arriving at informed, representative policy
proposals. However, they aren’t suited to the general task of
summarizing the values of the entire population across areas as
comprehensive as those covered by the law.</p>
<p>Perhaps for this general task we could turn to solutions in the moral
uncertainty literature. The problem of moral uncertainty is precisely
the problem of figuring out how to act, given the range of differing
views on ethics. A solution from the moral uncertainty literature which
best embodies the democratic ideal is the <em>moral parliament</em>
approach, which we will discuss later in this chapter.</p>
<div class="storybox">
<p><span>A Note On The Three Laws of Robotics</span> Some propose that
Isaac Asimov’s “Three Laws of Robotics” provide a useful set of rules
for resolving problems in machine ethics <span class="citation"
data-cites="asimov1942run Shulman2021"></span>. These laws are as
follows:</p>
<ol>
<li><p>“A robot may not injure a human being, or through inaction, allow
a human being to come to harm,”</p></li>
<li><p>“A robot must obey the orders given to it by a human being except
where such orders would conflict with the First Law,” and</p></li>
<li><p>“A robot must protect its own existence as long as such
protection does not conflict with the First or Second Laws.”</p></li>
</ol>
<p>The idea that these laws provide a solution to challenges in machine
ethics is a common misconception. Asimov himself frequently tested the
adequacy of these laws throughout his writing, showing that they are, in
fact, limited in their ability to resolve machine ethics problems.
Below, we explore some of these limitations.</p>
<h4
id="asimovs-laws-are-insufficient-for-guiding-ethical-behavior-in-ai-systems.">Asimov’s
laws are insufficient for guiding ethical behavior in AI systems<span
class="citation" data-cites="stokes2018"></span>.</h4>
<p>The three laws use under-defined terms like “harm” and “inaction.”
Because they’re under-defined, they could be interpreted in multiple
ways. It’s not clear precisely what “harm” means to humans, and it would
be even more difficult to encode the same meaning in AI systems.<br />
Harm is a complex concept. It can be physical or psychological. Would a
robot following Asimov’s first laws be required to intervene when humans
are about to hurt each other’s feelings? Would it be required to
intervene to prevent a human from behaving in ways that are self-harming
but deliberate, like smoking? Consider the case of amputating a limb in
order to stop the spread of an infection. An AI programmed with Asimov’s
laws would be forbidden from amputating the limb, as that would
literally be an instance of injuring a human being. However, the AI
would also be forbidden from allowing the harmful spread of an infection
through its inaction. These scenarios illustrate that the first law
fails, and therefore, that the following two do not follow. The laws may
need to be much more specific in order to reliably guide ethical
behavior in future AI systems.<br />
Philosophy has yet to produce a sufficient set of rules to determine
moral conduct. The safety of future AI systems cannot be guaranteed
simply through a set of rules or axioms. Numerous factors, such as proxy
gaming and competitive pressures, cannot be adequately captured in a set
of rules. Rules may be useful, but AI safety will require a more dynamic
and comprehensive approach that can address existing technical and
sociotechnical issues.<br />
Overall, Asimov’s Three Laws of Robotics fail to reliably guide ethical
behavior in AI systems, even if they serve as a useful starting point
for examining certain questions and problems in AI safety.<br />
</p>
</div>
<h3 id="conclusions-about-law-and-ethics">Conclusions About Law and
Ethics</h3>
<h4 id="summary.">Summary.</h4>
<p>The law is comprehensive, but not comprehensive enough to ensure that
the actions of an AI system are safe and beneficial. AI systems must
follow the law as a baseline, but we must also develop methods to ensure
that they follow the demands of ethics as well. Relying solely on the
law would leave many gaps that the AI could exploit, or make ethical
errors within. To create beneficial AI that acts in the interests of
humanity, we need to understand the ethical values that people hold over
and above the law.</p>
<h1 id="fairness">Fairness</h1>
<h3 id="introduction-1">Introduction</h3>
<p>We have discussed how bias in AI systems can cause harm. AI is being
used in many sensitive applications that affect human lives, from
lending and employment to healthcare and criminal justice. As a result,
unfair AI systems can cause serious harm. Methods for improving AI
fairness could mitigate these harms, but they require overcoming
challenges in formalizing and implementing fairness. This section
explores algorithmic fairness, including its technical definitions,
limitations, and real-world strategies for building fairer systems.</p>
<h4 id="the-compas-case-study.">The COMPAS case study.</h4>
<p>A famous example of algorithmic decision-making in criminal justice
is the COMPAS (Correctional Offender Management Profiling for
Alternative Sanctions) software used by over 100 jurisdictions in the US
justice system. This algorithm uses observed features such as criminal
history to predict recidivism, or how likely defendants are to reoffend.
A ProPublica report <span class="citation"
data-cites="angwin2016bias"></span> showed that COMPAS
disproportionately labeled African-Americans as higher risk than white
counterparts with nearly identical offense histories. However, COMPAS’s
creators argued that it was <em>calibrated</em>, with accurate general
probabilities of recidivism across the three general risk levels, and
that it was less biased and better than human judgments <span
class="citation" data-cites="dieterich2016compas"></span>. This
demonstrated the trade-off between different definitions of fairness: it
was calibrated across risk levels, but it was also clear that COMPAS
generated more false positives for African-American defendants
(predicting they would re-offend when they did not) and more false
negatives for white defendants, predicting they would not re-offend when
they in fact did. Adding to the concern, COMPAS is a black-box
algorithm: its process is proprietary and hidden. One lawsuit argued
this violates due process rights since its methods are hidden from the
court and the defendants <span class="citation"
data-cites="harvard2017state"></span>. In this section, we will discuss
some of the serious ethical questions raised by this case, examining
what makes algorithms unfair and considering some methods to improve
fairness.</p>
<h2 id="ai-fairness-concepts">AI Fairness Concepts</h2>
<h4 id="fairness-is-difficult-to-specify.">Fairness is difficult to
specify.</h4>
<p>Fairness is a complicated and disputed concept with no single
agreed-upon definition. Different notions of fairness can come into
conflict, making it challenging to ensure that an AI system will be
considered fair by all stakeholders.</p>
<h4 id="five-fairness-concepts.">Five fairness concepts.</h4>
<p>Some concepts of <em>individual fairness</em> focus on treating
similar individuals similarly—for instance, ensuring job applicants with
the same qualifications have similar chances of being shortlisted.
Others focus on <em>group fairness</em>: ensuring that protected groups
receive similar outcomes as majority groups. <em>Procedural
fairness</em> emphasizes improving the processes that lead to outcomes,
making sure they are consistent and transparent. <em>Distributive
fairness</em> concerns the equal distribution of resources.
<em>Counterfactual fairness</em> emphasizes that a model is fair if its
predictions are the same even if a protected characteristic like race
were different, all else being equal. These concepts can all be useful
in different contexts.</p>
<h4 id="justice-as-fairness.">Justice as fairness.</h4>
<p>Ethics is useful for analyzing the idea of fairness. John Rawls’
theory of justice as fairness argues that fairness is fundamental to
achieving a more just social system. In the chapter , we explore his
<em>maximin</em> and <em>difference principles</em>, which state that
inequalities in social goods can only be justified if they maximize
benefits for the most disadvantaged people. He also argued that the
social goods must be open to all under equality of opportunity. These
ideas align with common notions of fairness. Some argue this principle
also applies to AI: harms from the bias of algorithmic decisions should
be minimized, especially in ways that make the worst-off people better
off. Theories of justice can help develop the background principles for
fairness.</p>
<h4 id="algorithmic-fairness.">Algorithmic fairness.</h4>
<p>The field of algorithmic fairness aims to understand and address
unfairness issues that can arise in algorithmic systems, such as
classifiers and predictive models. This field’s goal is to ensure that
algorithms do not perpetuate disadvantages based on <em>protected
characteristics</em> such as race, gender, or class, especially while
predicting an outcome from features based on training data. Several
different technical definitions of fairness have been proposed, often
formalized mathematically. These definitions aim to highlight unfairness
in ML systems, but most possess inherent limitations. We will review
three definitions below.<br />
<em>Statistical parity.</em> The concept of statistical parity, also
known as demographic parity, requires that an algorithm makes positive
decisions at an equal rate for different groups. This metric requires
that the model’s predictions are <em>independent</em> of the sensitive
attribute. A hiring algorithm satisfies statistical parity if the hiring
rates for men and women are identical. While intuitive, statistical
parity is a very simplistic notion; for instance, it does not account
for potential differences between groups that could justify or explain
different outcomes.<br />
<em>Equalized odds.</em> Equalized odds demands that the false positive
rate and false negative rate are equal across different groups. A
predictive health screening algorithm fulfills equalized odds if the
false positive rate is identical for men and women. This metric ensures
that the <em>accuracy</em> of the model is not dependent on the
sensitive attribute value. However, enforcing equalized odds can reduce
overall accuracy.<br />
<em>Calibration.</em> Calibration measures how well predicted
probabilities match empirical results. In a calibrated model, the actual
long-run frequency of positives in the real population will match the
predicted probability from the model. For instance, if the model
predicts 20% of a certain group will default on a loan, roughly 20% will
in fact default. Importantly, calibration is a metric for populations,
and it does not tell us about the correctness or fairness of an ML
system for individuals. Calibration can improve fairness by preventing
incorrect, discriminatory predictions. As it happens, ML models often
train on losses that encourage calibration, and are therefore often
calibrated naturally.<br />
These technical concepts can be useful for operationalizing fairness.
However, there is no single mathematical definition of fairness that
matches everyone’s complex social expectations. This is a problem
because satisfying one definition can often violate others: there are
tensions between statistical notions of fairness.</p>
<h2 id="limitations-of-fairness">Limitations of Fairness</h2>
<p>There are several problems with trying to create fair AI systems.
While we can try to improve models’ adherence to the many metrics of
fairness, the three classic definitions of fairness are mathematically
contradictory for most applications. Additionally, improving fairness is
often at odds with accuracy. Another practical problem is that creating
fair systems means different things across different areas of
applications, such as healthcare and justice, and different stakeholders
within each area have different views on what constitutes fairness.</p>
<h4 id="contradictions-between-fairness-metrics.">Contradictions between
fairness metrics.</h4>
<p>Early AI fairness research largely focused on three metrics of
fairness: statistical/demographic parity, equalized odds, and
calibration. However, these ubiquitous metrics often contradict each
other: statistical parity only considers overall prediction rates, not
accuracy, while equalized odds focuses on accuracy across groups and
calibration emphasizes correct probability estimates on average.
Achieving calibration may require violating statistical parity when the
characteristic being predicted is different across groups, such as
re-offending upon release from prison being more common among
disadvantaged minorities <span class="citation"
data-cites="corbettdavies2018measure"></span>. This makes fulfilling all
three notions of fairness at once difficult or impossible.<br />
The <em>impossibility theorem</em> for AI fairness proves that no
classifier can satisfy these three definitions of fairness unless the
prevalence of the target characteristic is equal across groups or
prediction is perfect <span class="citation"
data-cites="chouldechova2016fair kleinberg2016inherent"></span>.
Requiring a model to be “fair” according to one metric may actually
disadvantage certain groups according to another metric. This undermines
attempts to create a universally applicable, precise definition of
fairness. However, we can still use metrics to better approximate our
ideals of fairness while remaining aware of their limitations.</p>
<h4
id="fairness-can-reduce-performance-if-not-achieved-carefully.">Fairness
can reduce performance if not achieved carefully.</h4>
<p>Enforcing fairness constraints often reduces model accuracy. Two
papers found that applying fairness techniques to an e-commerce
recommendation system increased financial costs <span class="citation"
data-cites="zahn2009cost"></span> and mitigating unfairness in Kaggle
models by post-processing reduced performance <span class="citation"
data-cites="biswas2020machine"></span>. However, these and others also
find ways to simultaneously improve both fairness and accuracy; for
example, work on healthcare models has managed to improve fairness with
little effect on accuracy <span class="citation"
data-cites="poulain2023improving"></span>. While aiming for fairness can
reduce model accuracy in many cases, sometimes fairness can be improved
without harming accuracy.</p>
<h4
id="difficulties-in-achieving-fairness-across-contexts.">Difficulties in
achieving fairness across contexts.</h4>
<p>Different fields have distinct problems: fairness criteria that make
sense in the context of employment may be inapplicable in healthcare.
Even different fields within healthcare face different problems with
incompatible solutions. These context-specific issues make generic
solutions inadequate. Models trained on historical data might reflect
historical patterns such as the underprescription of pain medication to
women <span class="citation"
data-cites="calderone1990influence"></span>. Removing gender information
from the dataset seems like an obvious way to avoid this problem.
However, this does not always work and can even be counterproductive.
For instance, removing gender data from an algorithm that matches
donated organs to people in need of transplants failed to eliminate
unfairness, because implicit markers of gender like body size and
creatinine levels still put women at a disadvantage <span
class="citation" data-cites="rodriguez-castro2014female"></span>.
Diagnostic systems without information about patients’ sex tend to
mispredict disease in females because they are trained mostly on data
from males <span class="citation" data-cites="Straw2022"></span>.
Finding ways to achieve fairness is difficult: there is no single method
or definition of fairness that straightforwardly translates into fair
outcomes for all.</p>
<h4 id="disagreements-in-intuitions-about-fairness.">Disagreements in
intuitions about fairness.</h4>
<p>There is widespread disagreement in intuitions about the fairness of
ML systems, even when a model fulfills technical fairness metrics; for
instance, patients and doctors often disagree on what constitutes
fairness. People often view identical decisions as more unfair if they
come from a statistical model <span class="citation"
data-cites="lee2018understanding"></span>; they also often disagree on
which fairness-oriented features are the most important <span
class="citation" data-cites="harrison2020emperical"></span>, such as
whether race should be used by the model or whether the model’s accuracy
or false positive rates are more important. It is unclear how to define
fairness in a generally acceptable way.</p>
<h2 id="approaches-to-improving-fairness">Approaches to Improving
Fairness</h2>
<p>Due to the impossibility theorem and inconsistent and competing
ideas, it is only possible to pursue some <em>definition</em> or
<em>metric</em> of fairness—fairness as conceptualized in a particular
way. There are two fundamental and complementary ways to approach this
more limited goal: technical approaches that focus directly on
algorithmic systems, and <em>social approaches</em> that focus on
related social factors.</p>
<h4 id="technical-approaches.">Technical approaches.</h4>
<p>Metrics of fairness such as statistical parity identify aspects of ML
systems that are relevant for fairness. Technical approaches to
improving fairness include a host of methods to improve models’
performance on these metrics, which can mitigate some forms of
unfairness. These often benefit from being broadly applicable with
little domain-specific knowledge. Developers can test predictive models
against various metrics for fairness and adjust models so that they
perform better. Fairness toolkits offer programmatic methods for
implementing technical fairness metrics into ML pipelines. Other methods
for uncovering hidden sources of unfairness in ML models include audits,
adversarial testing, sensitivity analysis, and ranking feature
importances.</p>
<h4 id="problems-with-technical-approaches.">Problems with technical
approaches.</h4>
<p>However, technical methods fall short of addressing the social
consequences of unfairness. They fail to adjust to sociocultural
contexts and struggle to combat biases inherited from training data.
“Fairness through unawareness” aims to remove protected characteristics
like gender and race from datasets to prevent sexism and racism, but
fails miserably in practice because this data is embedded in correlates
[as we explored in the Bias section in chapter]. A focus on narrow
measures can ignore other relevant considerations, and measures are
often subject to proxy gaming <a href="#sec:proxy-gaming"
data-reference-type="ref"
data-reference="sec:proxy-gaming">[sec:proxy-gaming]</a>. A more
in-depth, qualitative, and socially grounded approach is often harder
and does not scale as easily as technical methods, but it is still
essential for navigating concerns in AI fairness.</p>
<h4 id="social-approaches.">Social approaches.</h4>
<p>Social approaches emphasize that unfairness is tied to systemic
social injustices propagated through technical systems. They highlight
political, economic, and cultural factors and apply methods such as
anti-discrimination policy, legal reform, and a design process focused
on values and human impacts. These methods, which include policies like
developing AI systems with input from stakeholders, can surface and
mitigate sources of unfairness early. Substantive social changes are
generally more expensive and difficult than technical approaches.
However, they can be more impactful, reducing models’ negative social
impacts.</p>
<h4 id="frameworks-for-fairness.">Frameworks for fairness.</h4>
<p>Ongoing research aims to create frameworks for AI fairness that blend
technical and social considerations. <em>Fair deployment</em> frameworks
focus on gradually deploying AI while assessing its impacts, ensuring
there are strong oversight mechanisms and independent audits, and
establishing methods to redress any harm. Audits can promote an
understanding of the data sources and decision-making processes of ML
models. This transparency is helpful but insufficient, as we saw in the
chapter <a href="#sec:opaqueness" data-reference-type="ref"
data-reference="sec:opaqueness">[sec:opaqueness]</a>. <em>Participatory
design</em> frameworks involve all of the impacted communities in the
development of AI systems to surface unfairness early and ensure the
product meets diverse human needs. Other frameworks focus on how
hierarchies and distributions of power can intersect to produce
unfairness or evaluating how AI affects the least advantaged in society.
These broad frameworks recognize that effective solutions require
combining technical interventions with social and legal policies to
address underlying social factors.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Technical definitions of fairness are useful tools, but improving the
fairness of AI necessitates broader approaches. There are inherent
tradeoffs between fairness criteria which means that satisfying all the
popular definitions of fairness is logically impossible. Additionally,
satisfying some mathematical definitions does not guarantee fair
outcomes anyway. Developing fair AIs might require broader
sociotechnical solutions such as the participation of stakeholders in
deciding the tradeoffs a model makes between various fairness criteria.
Research on algorithmic fairness continues to influence how AI systems
are built and used. In the next section, we will examine how AI systems
are currently being built and used, discussing the Economic Engine that
creates and shapes AIs, and might in turn be transformed by them.</p>
<h1 id="sec:economic-engine">The Economic Engine</h1>
<h4
id="what-if-we-allow-the-economy-to-decide-what-ais-will-be-like">What
if we allow the economy to decide what AIs will be like?</h4>
<p>Most AI systems today are being developed by businesses. We could
therefore argue that AI development is most closely aligned to business,
or economic, goals like wealth maximization, as opposed to improvements
in social wellbeing. It is increasingly likely that AIs will transform
the economy by enabling new forms of business, changing the nature of
work, and even displacing human workers. In this section, we will
examine whether AIs shaped by economic incentives create the sort of
society most people want. We examine both how the economy shapes the
development of AI, and how AI might impact the economy.</p>
<h4
id="business-goals-influence-the-development-of-the-most-capable-ai-systems.">Business
goals influence the development of the most capable AI systems.</h4>
<p>AIs are increasingly more efficient than humans at tasks once
considered difficult to automate. In the customer service industry, for
example, many businesses rely on chatbots instead of human
representatives. Chatbots reduce expenses and streamline operations,
ultimately increasing profits. The current trend towards automation
suggests that many of the most powerful AI systems will be deployed to
serve specific business objectives.<br />
Whether this is a reason for optimism or concern depends on to what
extent we think that the economy, left to its own devices, will produce
good outcomes. On one hand, AI has the potential to enhance efficiency
and foster competition, which can lower consumer costs and spur the
creation of innovative products. However, business-oriented AIs may
focus narrowly on achieving their specific goals, compromising values
such as community, personal freedom, privacy, and consumer choice. We
can draw from economics to examine how AI usage in business contexts
could either support or disrupt societal wellbeing.</p>
<h4 id="using-ai-to-maximize-profits-creates-wealth.">Using AI to
maximize profits creates wealth.</h4>
<p>In a market system, businesses often prioritize maximizing profits
because the primary responsibility of company executives is to increase
value for shareholders. This concept, called shareholder primacy, has
been codified both implicitly and explicitly into American corporate law
<span class="citation" data-cites="lipton2019shareholder"></span>. In
theory, corporations that explicitly prioritize other values such as
safety or social responsibility above profit in the long run can be sued
for neglecting their obligation to maximize shareholder value. This
profit—centered approach means that—without effective directed state
intervention—–most AI systems will not first and foremost be used to
help us create a society that satisfies broader human values.<br />
Consider recommendation algorithms on streaming platforms. These AI
systems are designed to keep us engaged for as long as possible, thereby
increasing ad revenue and profit for the platform. Since these
algorithms prioritize viewer engagement over everything else, they may
recommend content that fuels addiction or spreads misinformation. In
other words, while these algorithms may be effective from a business
standpoint, they may have negative impacts on individuals and society at
large. While wealth creation is generally good, there are serious
implications of prioritizing profit over all other values.</p>
<h4 id="the-economy-is-the-backbone-of-our-daily-lives.">The economy is
the backbone of our daily lives.</h4>
<p>The economy is not just a network of businesses and transactions: it
touches every aspect of our lives. Every transaction, small or large
that we make, is impacted by economic factors. At a highly simplified
level, the economy determines the <em>prices</em> of goods and services
and the <em>incomes</em> earned by workers. Consider how these two
factors affect the decision to make a large purchase, such as a house.
The price of a house (most often paid as a mortgage) is shaped by
interest rates, as well as the level of competition to buy and sell
houses. A person’s income determines their ability to make the purchase,
and is influenced by economic forces like labor markets and company
profits.<br />
As businesses continue to leverage AI for economic gain, these systems
become ingrained in our economic fabric. For example, AI is increasingly
used by banks to assess credit risk and decide who gets approved for a
loan. If this practice becomes more prevalent, AI will no longer be just
a tool; it will determine who can buy a house or run a business. The
advance of AI in business affects everyone’s life at a basic level. The
use of AI may lead to improvements, such as lower prices, or challenges,
like widespread job losses due to automation, that impact us
fundamentally.</p>
<h4 id="technology-changes-the-economy-and-the-world.">Technology
changes the economy and the world.</h4>
<p>Throughout history, technology has been a pivotal driver of economic
change. Consider the first agricultural revolution. The invention of
farming tools and techniques transformed societies from hunter-gatherers
to settled agricultural communities, fundamentally altering the economy
and human lifestyle. Fast-forward to the mid-1700s, when the steam
engine drove the industrial revolution, sparking rapid economic growth
and urbanization and shifting the economy from primarily agrarian to
primarily industrial. Factories became the economic focal point,
creating jobs and spurring the development of new sectors like rail and
shipping. More recently, the internet and smartphones have connected
billions of people, breaking down geographical barriers and enabling new
types of businesses like e-commerce and remote work.</p>
<h4
id="artificial-intelligence-might-be-the-next-transformative-technology.">Artificial
intelligence might be the next transformative technology.</h4>
<p>As AI becomes increasingly critical for businesses, consumers, and
governance, it may alter daily life, from work and education to
interpersonal relationships and entertainment. As it evolves and
permeates more sectors, its economic and societal effects of the AI
revolution could be as transformative as those of the agricultural or
industrial revolutions <span class="citation"
data-cites="pwc2017sizing"></span>. Therefore, understanding AI’s
potential risks and benefits is essential for navigating this new
economic landscape.</p>
<h4 id="preview.-1">Preview.</h4>
<p>In this section, we discuss how economics and AI interact. First, we
will examine efficiency and economic growth, and how AIs might benefit
society by pursuing these. Second, we will discuss inequality and market
failures, and how AI might exacerbate these problems. Lastly, we will
question whether economics provides the necessary measures and tools to
represent societal wellbeing, especially in face of rapid technological
change. By the end of this section, we will have begun to think about
how economics can shape the impact of AI on society.</p>
<h2 id="the-free-market">The Free Market</h2>
<h4 id="overview.">Overview.</h4>
<p>Under ideal circumstances, competitive economic markets drive
efficiency and foster collective prosperity. One can speculate that the
integration of AI into the economic engine could further optimize
efficiency and enhance competition, ultimately contributing to
accelerated economic growth and improved wellbeing. We will explore this
potential by discussing the role of efficiency in the economy and the
benefits and limitations of focusing on economic growth in the context
of AIs augmenting growth and efficiency.<br />
</p>
<div class="storybox">
<p><span>A Note on Gains from Trade</span></p>
<h4 id="we-can-specialize-and-trade-making-everyone-better-off.">We can
specialize and trade, making everyone better off.</h4>
<p>Trade is often seen as a win-win scenario, allowing for mutual
benefit. Take a simple scenario. Suppose Alice and Bob are stranded on
an island. Each of them has one skill: Alice is excellent at gathering
coconuts for water, while Bob is especially good at catching fish to
eat. Suppose that, left to their own devices, Alice would struggle to
have food to eat, and Bob would struggle to have coconut water to drink.
However, if Alice and Bob agree to specialize in their respective skills
and then trade, they can both focus on doing what they are good at,
wasting less time on individually unproductive activities and therefore
producing more food in total, and each ending up with more food and
drink than if they had worked alone.<br />
Moreover, even if Alice is better than Bob at both gathering coconuts
and catching fish, they will still benefit from specialization. Suppose
Bob is better at fishing than he is at gathering coconuts. Even though
he’s not as good as Alice at either, they’ll still produce more overall
if Alice focuses on coconuts and Bob on fishing. In most cases, trading
makes people better off. One role of the economy is to enable such
trades on a large scale.</p>
</div>
<h4
id="markets-coordinate-trade-allowing-gains-from-specialization.">Markets
coordinate trade, allowing gains from specialization.</h4>
<p>A market is a venue for trade, a system that enables the exchange of
goods and services. It can be a physical location or through any other
matching mechanism like e-commerce platforms. Markets allow everyone to
benefit from high levels of specialization and reap the benefits of
trade. In the global marketplace, few people have to farm their own
food, build their own houses, and make their own clothes, because
everyone trades their time and skills for goods and services created by
others. Markets can be thought of as a grand collaboration, facilitating
cooperation among millions of people across the globe.</p>
<h4
id="markets-encourage-a-complex-blend-of-competition-and-collaboration.">Markets
encourage a complex blend of competition and collaboration.</h4>
<p>Companies like Toyota and Ford compete to sell their cars to
consumers. However, they also often collaborate on research and
development projects. This cooperation can result in advancements like
more efficient engines or safer car designs, benefiting both companies
and their customers. In general, while firms within the same industry
compete against each other for market share, they also cooperate with
strategic partners, suppliers, and vendors for mutual benefit.
Cooperation helps companies optimize resources and improve products or
services while competition drives innovation and offers consumers more
choices.</p>
<h4
id="prices-enable-the-economy-to-be-an-adaptable-self-organizing-complex-system.">Prices
enable the economy to be an adaptable, self-organizing complex
system.</h4>
<p>Prices in a market economy are signals of changing demand and supply,
created by individual agents acting in accordance with their incentives.
As an example, let’s trace how the high oil prices in the 1970s
reorganized the global flower market. In 1971, the price of oil
increased dramatically, signaling that oil had become more scarce. This
price signal gave individuals and businesses an incentive to find ways
to reduce spending on oil or develop substitutes. The higher cost of
heating greenhouses using oil led to flower suppliers reducing their
production, causing the price of roses and other flowers to increase. In
turn, consumers turned to substitutes like chocolate and teddy bears for
Valentine’s Day gifts. In the meantime, opportunistic entrepreneurs
began thinking of alternative, cheaper ways to produce flowers. They
encouraged farmers in Kenya and Ecuador to grow roses using natural
heat, and invested in a global infrastructure to deliver roses
worldwide; as a result, they started using land for roses instead of
coffee, which saw an increase in the price of coffee.<br />
In these cases information in the market, primarily transmitted through
prices, efficiently coordinated the actions of individuals and
businesses, allowing them to use their own knowledge and ingenuity to
make optimal business decisions in a way that no individual or
government would be able to do. The production of oil, flowers,
chocolates, teddy bears, coffee, and many other goods was adjusted in
order to create and deliver the most value possible given the available
resources and information.</p>
<h3 id="efficiency">Efficiency</h3>
<p><em>AIs can increase allocative efficiency, ensuring that outcomes
are efficient and contribute to social welfare.</em></p>
<h4 id="the-free-market-creates-allocative-efficiency.">The free market
creates allocative efficiency.</h4>
<p>The First Fundamental Theorem of Welfare Economics states that an
equilibrium <em>allocation</em> of goods reached by trading on a free
market must be <em>Pareto efficient</em>. An outcome is Pareto efficient
if there is no way to make anyone better off without making someone else
worse off: any change must trade off one person’s welfare against
another. If the allocation of goods were not Pareto efficient, then
individuals within it would trade in a way that exploited the possible
win-win, reaching a state in which no mutually beneficial trades can be
made.<br />
This is Adam Smith’s famous “Invisible Hand” argument, which suggests
that when individuals pursue their self-interest within a market, they
unintentionally contribute to societal welfare. When everyone acts for
themselves, the natural outcome of trade leads to outcomes that are
allocatively efficient–—from where further Pareto improvements are
impossible. By creating gains from trade, there is an overall
improvement in living standards for everyone.</p>
<h4
id="there-are-six-relevant-conditions-for-the-invisible-hand-theorem-to-hold.">There
are six relevant conditions for the invisible hand theorem to hold.</h4>
<p>With the following, it is reasonable to conclude that the presence of
free markets with individuals acting in self-interest are sufficient for
creating social wellbeing.</p>
<ol>
<li><p><strong>There must be an open market.</strong> There should be no
barriers to entry for producers or buyers, so that everyone can
participate in this market. This openness stimulates competition,
promoting economic efficiency.</p></li>
<li><p><strong>There should be no seller big enough to move prices up
alone.</strong> If there are many sellers, then anyone who raises prices
will lose consumers. There must be no monopoly power: anyone with the
ability to raise prices without being forced down by competition will
create distortions that leave consumers worse off.</p></li>
<li><p><strong>No producer should privately hold a pivotal
technology.</strong> This means that other producers should be able to
copy the production of the first-mover. While the first-mover will make
profits in the short run, in the long run the market allocation will be
Pareto optimal.</p></li>
<li><p><strong>No buyer should be big enough to move prices down
alone.</strong> Similar to the condition for producers, no buyer should
be big enough to force producers to take lower prices than others would
offer for them. Such buyers would create distortions that might, for
instance, force producers out of business, ultimately harming
everyone.</p></li>
<li><p><strong>There must be perfect information for everyone.</strong>
Producers and consumers must have access to perfect information, such as
about product quality and pricing. If consumers don’t know, for
instance, that a seller’s product is defective or that other sellers are
offering lower prices, then markets cannot achieve efficiency.</p></li>
<li><p><strong>The state enforces property and contract laws.</strong>
Most economic theory assumes that contracts are enforceable, and that
individuals and corporations have protected property rights. Without
these, trading would be difficult to achieve and much more costly,
making it more difficult for everyone to achieve optimal
outcomes.</p></li>
</ol>
<p>If the conditions for the invisible hand theorem always held, AI’s
role in enhancing business competitiveness would inevitably lead to
societal benefits. If companies use AI to boost productive efficiency,
lower costs, and improve products, then these actions should eventually
translate into widespread advantages. Lower prices, improved products,
and better services could enhance consumer wellbeing as per the
Invisible Hand Theorem, while increased efficiency could stimulate
economic growth. Later in this section, we will look at market failures
and inequality: two reasons why the invisible hand theorem might be
insufficient. For now, we will focus on the benefits.</p>
<h4
id="competition-fosters-productive-efficiency-and-growth.">Competition
fosters productive efficiency and growth.</h4>
<p>Markets continuously incentivize the lowering of prices and the
improvement of quality. Let’s imagine two grocery stores in a town. If
one store starts to offer lower prices, customers may flock to that
store, putting pressure on the other store to match those prices or risk
losing business. This encourages firms to be more <em>productively
efficient</em>–—to produce goods at lower costs–—so that they can lower
their prices while still making a profit. This competition can lead to
lower prices or higher quality products for consumers and more
attractive employment options for workers.<br />
By leveraging AI, businesses can become more competitive by increasing
their productive efficiency. For example, many companies now use
AI-driven chatbots to handle customer service inquiries. These chatbots
can manage numerous queries simultaneously, around the clock, and
without human error, reducing the need for large customer service teams
and significantly reducing costs. AI can automate tasks that were
traditionally labor-intensive, leading to considerable savings.
Furthermore, AI can enhance decision-making processes. For instance,
companies can use AI to analyze large volumes of data and generate
insights about market trends, customer preferences, and operational
efficiencies. Such insights can inform strategic decisions, providing a
competitive advantage.<br />
Given the extensive benefits of AI, its adoption by businesses is likely
to continue to grow. Firms that do not leverage AI may find themselves
at a disadvantage, struggling to keep up with competitors that have
harnessed the power of AI to optimize their operations and enhance their
offerings. In the competitive business world, the ability to adapt and
adopt new technologies like AI is not merely an option–—it’s essential
for survival.</p>
<h3 id="growth">Growth</h3>
<p><em>AIs have the potential to drive economic growth, which can make
everyone better off without compromising value pluralism and improve
society across correlated domains like health, freedom, and
education.</em></p>
<h4
id="growth-is-widely-considered-essential-to-a-healthy-society.">Growth
is widely considered essential to a healthy society.</h4>
<p>Economic growth is an increase in the production of goods and
services. Such growth is a vital sign of a thriving economy. An
expanding economy usually indicates more wealth, opportunities, and
advancements, leading to overall improvements in living standards.
Growth plays a key role in sparking innovation and entrepreneurship. As
the economy expands, people are incentivized to innovate and create new
ventures, products, and ideas.<br />
Economic growth typically brings about improved employment opportunities
and better consumer products. A growing economy fosters job creation,
providing more people with income and stability; for example, the tech
industry’s growth led to a boom in job opportunities in fields like
software development, data analysis, and AI research. As companies grow
and industries expand, businesses can invest more in research and
development, leading to new and improved products; for instance, the
smartphone industry’s growth has led to a variety of advanced phones
with innovative features that have given billions of people access to
the global economy. We generally consider economic growth desirable
because it means we get wealthier over time.</p>
<h4 id="growth-might-let-us-promote-value-pluralism.">Growth might let
us promote value pluralism.</h4>
<p>Value pluralism is the notion that multiple important values often
conflict and that there’s no easy way to rank them in order of
importance. For instance, a community might debate whether to spend its
limited resources on building a new hospital or a cultural center. Some
community members might argue that healthcare is the most urgent need,
while others might say that arts and culture are equally important for
the community’s wellbeing. With sustained economic growth—–and therefore
increased wealth over time–—the community might not need to make such
tough choices. Instead of selecting a hospital or a cultural center, the
community could build both. Growth can enable <em>Pareto
improvements</em>: changes that improve people’s lives without leaving
anyone worse off; it is like growing a pie to give everyone bigger
slices. The increased wealth effectively allows for the accommodation of
multiple values without prioritizing one over the other.<br />
Economic growth, while advantageous, doesn’t resolve all issues stemming
from value pluralism. It often prioritizes efficiency over equity, and
wealth over values such as environmental sustainability. People’s unique
experiences, cultures, and beliefs shape their values, so debates about
fair resource distribution or balancing freedoms with obligations could
still arise in a resource-abundant society. Despite not being a
comprehensive solution for value pluralism, economic growth provides a
helpful approach to achieving broader societal goals.</p>
<h4 id="we-have-strong-reasons-for-encouraging-economic-growth.">We have
strong reasons for encouraging economic growth.</h4>
<p>Looking at the world over the last 200 years provides strong evidence
that economic growth can lead to vast improvements in human life through
health, freedom, education, and more.<br />
</p>
<figure id="fig:preston">
<embed src="images/machine_ethics/preston.pdf" />
<figcaption>Increases in GDP per capita strongly correlate with
increases in life expectancy.</figcaption>
</figure>
<p>The Preston curve is compelling evidence of a positive correlation
between a country’s gross domestic product (GDP) per capita—–a measure
of the total production of goods and services–—and health outcomes. This
relationship illustrates that countries with higher GDP per capita have
better health outcomes, and particularly that poorer countries stand to
benefit immensely from improvements in GDP. This also holds across time:
average global life expectancy was just below 40 years at the start of
the 20th century, whereas today, with a much higher global GDP per
capita, the average person expects to live for 70 years. Nobel laureate
Amartya Sen suggests that one pathway through which growth improves
health is by reducing poverty and increasing investments in healthcare
<span class="citation" data-cites="sen1999economics"></span>.<br />
The benefits of economic growth extend beyond physical health. There is
a strong association between economic prosperity and enhancements in
freedom and education <span class="citation"
data-cites="heckelman2000economic"></span>. Economic growth can
facilitate better educational opportunities by enabling more funding for
schools, teacher salaries, and educational resources. This, in turn, can
boost literacy rates, educational attainment, and, ultimately, the
knowledge capital of a society. With more resources at their disposal,
people have more options and opportunities. They can pursue varied
careers, invest in personal development, and engage more actively in
civil society. Moreover, prosperous societies can afford stronger
institutions to safeguard democratic freedoms and human rights.
Additionally, economic growth can nurture the arts. As societies become
wealthier, more resources can be allocated to support cultural
institutions, artists, and creative endeavors. This can lead to a
flourishing of the arts, enriching societal culture and enhancing
individual life experiences.</p>
<h4 id="compound-growth-is-powerful.">Compound growth is powerful.</h4>
<p>The effect of small increases in economic growth becomes apparent
when we consider the effects of compound growth over time. Even small
differences in annual growth rates can profoundly impact future
generations. For instance, if an economy grows at 1% per year, our
grandchildren would have twice our current income in 70 years. However,
if the economy grows at 2% per year, our children’s income in 35 years
would be double ours and our grandchildren’s income would quadruple.
Economic growth has the potential to create a significantly more
prosperous future. Prioritizing growth, therefore, can be a powerful
strategy for enhancing societal wellbeing now and in the future.</p>
<h4 id="increasing-growth-through-ai-seems-promising.">Increasing growth
through AI seems promising.</h4>
<p>AI might be a significant driver of economic growth in the coming
decades. From automating repetitive tasks in industries like
manufacturing to providing sophisticated insights in areas like
healthcare and finance, AI has the potential to boost economic
performance considerably. In the automobile industry, for instance, the
application of AI to autonomous vehicles could be revolutionary,
reducing human error and increasing efficiency in transport and
logistics. Similarly, in healthcare, AI algorithms can assist doctors by
increasing the precision and accuracy of their diagnostics, improving
patient care and outcomes. The potential for vast economic growth from
AI gives us reason to be optimistic about the future. If economic growth
improves health, education, and overall quality of life–—as we have
considered is possible—–the AI-driven economy could significantly
enhance societal wellbeing.</p>
<h4
id="however-while-growth-is-correlated-with-welfare-its-benefits-are-neither-predictable-nor-uniform.">However,
while growth is correlated with welfare, its benefits are neither
predictable nor uniform.</h4>
<p>Growth might be unsustainable, trading off long-term wellbeing for
gains today. Even if we have sustainable growth, we need to consider the
distribution of that growth. Will economic growth from AI benefit
everyone or only a select few? AI might create jobs in some sectors and
eliminate them in others due to automation. Therefore, while AI has the
potential to boost economic growth, we need to address risks like the
unequal distribution of benefits.</p>
<h4 id="similarly-efficiency-is-desirable-but-insufficient.">Similarly,
efficiency is desirable but insufficient.</h4>
<p>While the concept of the “Invisible Hand” suggests that self-interest
and free markets lead to competition and efficiency, these just as often
lead to markets in which competition is eliminated. MBA programs teach
business executives how to avoid Porter’s Five Forces: how to prioritize
ensuring that their businesses are in an industry with low competition,
high barriers to entry, customers and suppliers with weak price-setting
power, and low threat of substitute products. Many MBAs are taught to
act in ways contrary to the conditions of the First Fundamental Theorem
of Welfare Economics and therefore take steps to undermine efficiency.
In <em>The Theory of Moral Sentiments</em>, Adam Smith himself
recognized that self-interest and free markets are sufficient only on
paper: he thought that our concern for others is essential and
celebrated our sense of justice, generosity, and public spirit.<br />
Similarly, metrics of efficiency don’t always tell us whether the
economy is contributing to social welfare; for instance, Pareto
efficient distributions can be extremely unequal. As an extreme example,
if one person has all the resources in the economy, no one else can be
made better off without taking away something from that person.
Analogously, AIs may worsen income inequality by benefiting mainly
business owners and shareholders at the expense of workers who may be
cheaply replaced. Additionally, AIs may be used to promote monopolistic
practices or violate our preferences for personal privacy in ways that
are nonetheless efficient.<br />
In essence, we must recognize that markets are not perfect: they can
cause problems such as inequality and market failures. These and other
related issues will be discussed in further detail in the next
section.<br />
</p>
<h2 id="inequality-market-failures">Inequality &amp; Market
Failures</h2>
<h4 id="overview.-1">Overview.</h4>
<p>The previous section on markets and efficiency painted a rosy picture
of real-world markets. In reality, we rarely see markets perform at this
ideal standard. Economic growth is often talked about as an intrinsic
good, but is only ever an instrumental one: it is not valuable for
itself, but for the positive effects it has on society. However, even
when markets are productively and allocatively efficient, creating
wealth for everyone, they might impose harmful side effects. Two reasons
for this are that markets generate <em>inequality</em> and are often
subject to <em>market failures</em>.<br />
First, we will explore the relevance of inequality in understanding
societal wellbeing, what tools can be used to measure its effects, and
why concerns about inequality are not just limited to those at the
bottom. Then, we will consider the potential implications of AI on
employment and income inequality, and how societies across the globe may
be impacted by an economy restructured by AI. We will also examine
various types of market failures: instances where markets fail to
allocate goods and services optimally. We will discuss information
asymmetries, the monopolistic power of large firms, and negative
externalities—–consequences that extend beyond the direct participants
of a trade.</p>
<h3 id="inequality">Inequality</h3>
<p><em>Inequality is on the rise and harms many people, including many
of those getting richer. The widespread use of AIs might sharpen
inequality, decreasing social wellbeing.</em></p>
<h4 id="most-of-the-world-exhibits-high-levels-of-inequality.">Most of
the world exhibits high levels of inequality.</h4>
<p>In economics, inequality refers to the uneven distribution of
economic resources, including income and living conditions. The <em>Gini
coefficient</em> is a commonly used statistical measure of income or
wealth distribution within a country. It is a number between 0 and 1,
where 0 represents perfect equality (everyone has the same income or
wealth), and 1 signifies maximum inequality (one person has all the
income or wealth, and everyone else has none). Looking at Gini
coefficients, 71% of the world’s population lives in countries with
increasing inequality over the last thirty years <span class="citation"
data-cites="UN2020inequality"></span>.</p>
<h4
id="inequality-in-the-united-states-is-particularly-striking.">Inequality
in the United States is particularly striking.</h4>
<p>Figure <a href="#fig:gini" data-reference-type="ref"
data-reference="fig:gini">2</a> shows that the Gini coefficient in the
US has trended significantly upwards from 1969 to 2019. Over 50 years,
the pre-tax Gini coefficient has increased by 30%, while the post-tax
Gini coefficient has risen by 25%, suggesting that despite
redistributive taxation policies, the US income gap has widened
substantially. (For reference, this change in Gini coefficient is the
same size as moving from Canada to Saudi Arabia today. <span
class="citation" data-cites="UN2020inequality"></span>) This increase in
the Gini coefficient is evidence of a growing inequality crisis. An
associated fall in social mobility—the ability of an individual to move
from the bottom income bracket to the top—cements inequalities over
generations.<br />
</p>
<figure id="fig:gini">
<embed src="images/machine_ethics/gini.pdf" />
<figcaption>Change in US Gini coefficient.<br />
Data Source: Our World in Data</figcaption>
</figure>
<h4 id="the-distribution-of-gains-from-growth-is-highly-unequal.">The
distribution of gains from growth is highly unequal.</h4>
<p>Nearly all the wealth gains over the past five decades have been
captured by the top 1% of income earners, while average
inflation-adjusted wages have barely increased <span class="citation"
data-cites="desilver2018workers"></span>. A RAND Corporation working
paper estimated how the US income distribution would look today if
inequality was at the same level as in 1975. Suppose my annual income is
$15,000 today. If inequality was at the same level as in 1975, I would
be paid an extra $5,000. Someone else earning $65,000 today would
instead have been paid $100,000 had inequality held constant! We can see
in the table below that these increases in inequality have had massive
effects on individual incomes for everyone outside the top 1%.<br />
Real and counterfactual income distributions for all adults with income,
in 2018 USD <span class="citation"
data-cites="hanauer2020top"></span>.<br />
</p>
<div id="tab:income">
<table>
<caption>Data Source: RAND; Graphics: Mary Traverse for Civic Ventures
<span class="citation" data-cites="time2020trillion"></span></caption>
<thead>
<tr class="header">
<th style="text-align: left;">Percentile</th>
<th style="text-align: left;">Actual Income in 1975</th>
<th style="text-align: left;">Actual Income in 2018</th>
<th style="text-align: left;">Income in 2018 if Inequality Had Stayed
Constant</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">25th %</td>
<td style="text-align: left;">$9,000</td>
<td style="text-align: left;">$15,000</td>
<td style="text-align: left;">$20,000</td>
</tr>
<tr class="even">
<td style="text-align: left;">Median</td>
<td style="text-align: left;">$26,000</td>
<td style="text-align: left;">$36,000</td>
<td style="text-align: left;">$57,000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">75th %</td>
<td style="text-align: left;">$46,000</td>
<td style="text-align: left;">$65,000</td>
<td style="text-align: left;">$100,000</td>
</tr>
<tr class="even">
<td style="text-align: left;">90th %</td>
<td style="text-align: left;">$65,000</td>
<td style="text-align: left;">$112,000</td>
<td style="text-align: left;">$142,000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">95th %</td>
<td style="text-align: left;">$80,000</td>
<td style="text-align: left;">$164,000</td>
<td style="text-align: left;">$174,000</td>
</tr>
<tr class="even">
<td style="text-align: left;">99th %</td>
<td style="text-align: left;">$162,000</td>
<td style="text-align: left;">$491,000</td>
<td style="text-align: left;">$353,000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Top 1% Mean</td>
<td style="text-align: left;">$252,000</td>
<td style="text-align: left;">$1,160,000</td>
<td style="text-align: left;">$549,000</td>
</tr>
</tbody>
</table>
</div>
<h4 id="inequality-carries-serious-implications.">Inequality carries
serious implications.</h4>
<p>For those worst off in an unequal society, the implications are
severe. Beyond obvious problems like an inability to access essentials
and maintain a basic standard of living, there are additional concerns
in domains like health. A widening wealth gap often corresponds with a
health gap, where those with fewer resources have poorer health outcomes
due to less access to quality healthcare, lower quality nutrition, and
higher stress levels. For instance, life expectancy often varies
dramatically based on income in unequal societies: the average life
expectancy of the lowest-income classes in America now matches the
averages in Sudan or Pakistan <span class="citation"
data-cites="wilkinson2009spirit"></span>.</p>
<h4
id="everyone-not-just-the-poorest-suffers-in-an-unequal-society.">Everyone,
not just the poorest, suffers in an unequal society.</h4>
<p>One of the most robust findings is that unequal societies have higher
levels of crime <span class="citation"
data-cites="kelly2000inequality"></span>. When inequality is high, so
too are levels of social tension, dissatisfaction, and shame, which can
contribute to higher crime rates. This can lead to a cycle where the
fear of crime drives further inequality, as wealthier individuals and
neighborhoods invest in measures that segregate them further from the
rest of society, further increasing inequality and crime rates. A more
detailed discussion of this relative deprivation can be found in the
section on Cooperation and Conflict. Inequality is also related to other
signs of societal sickness: worse physical and mental health, increased
drug use, and higher rates of incarceration. Strikingly, inequality is a
strong predictor of political instability and violence as well. While it
may seem like those with more wealth are insulated from the negative
effects of inequality, in reality they suffer indirect consequences as
well.<br />
</p>
<figure id="fig:homicide">
<embed src="images/machine_ethics/crime.pdf" />
<figcaption>Measures of income inequality and homicide rate<br />
Data Source: Vienna University of Economics and Business</figcaption>
</figure>
<figure id="fig:political">
<embed src="images/machine_ethics/stability.pdf" />
<figcaption>Measures of income inequality and political stability.<br />
Data Source: World Bank, World Governance Indicators and Gini
Coefficient Data</figcaption>
</figure>
<h4 id="the-causes-of-inequality-are-disputed.">The causes of inequality
are disputed.</h4>
<p>Renowned economist Thomas Piketty, in his book "Capital in the 21st
Century," posits that inequality results from the difference between the
interest rate investments in capital receive (r) and the rate of
economic growth (g) <span class="citation"
data-cites="piketty2014capital"></span>. Essentially, when the returns
from investing wealth (r) surpass the overall growth rate (g), society
tends to become more unequal. This happens because the rich can profit
significantly from investing their existing wealth, while average
individuals cannot realize the same benefits from overall economic
growth. However, Piketty’s theory has met criticisms. Particularly, he
assumes that the technological frontier can only grow at 1.5% per year,
a limitation that could potentially be overturned by advancements in
AI.</p>
<p>Countless other theories seek to explain the rise in inequality. One
possibility is that an increased reward for technical skills has led to
a widening gap between those with high-skill jobs and those without.
Other theories point to the decline of labor unions, which traditionally
advocated for worker rights and better wages, as a contributing factor.
The driving forces behind rising inequality are undoubtedly
multifaceted, warranting a careful and nuanced examination.</p>
<h4
id="ai-might-prompt-higher-returns-on-investments-for-the-wealthy.">AI
might prompt higher returns on investments for the wealthy.</h4>
<p>AI could significantly impact income inequality, intensifying
existing disparities. Investments in AI may yield higher productivity
compared to past opportunities, leading to a higher rate of return on
capital. Following Piketty’s inequality theory of the difference between
the rate of return (r) and the growth rate (g), AI might exacerbate
inequality if it increases the return rate more than it increases total
growth. This seems likely: those with existing capital (such as GPUs)
are likely to be able to deploy and benefit from AIs far more than the
average person.</p>
<h4 id="automation-using-ai-might-increase-inequality.">Automation using
AI might increase inequality.</h4>
<p>If AIs are more productive than humans, as is likely to become the
case in many industries, automating human labor will increase profits
and returns on investment. Investors would likely opt for AI over human
labor since AI presents a more cost-effective and productive
alternative. A broad shift towards AI as a form of capital might prompt
the wealthy to invest more heavily in AI technologies than businesses
that rely on human labor. As businesses transition towards AI, the
demand for human labor may decrease, resulting in job losses. As a
result, workers’ bargaining power could be significantly reduced: a
surplus of labor could force wages down and weaken workers’ conditions,
benefits, and protections. If your job is automated, then conventional
bargaining tactics like striking are no longer an option. Such a
scenario would contribute to growing inequality and negatively impact
the livelihoods of workers.<br />
Although new technologies may create novel job opportunities, the
large-scale integration of AI could render many existing roles
redundant. Numerous job sectors are susceptible to replacement by AI.
Unlike in the past, even knowledge-based jobs such as writing and
programming are at risk due to advancements in AI such as large language
models, robotics, and computer vision. The advent of self-driving cars
could potentially displace the vast portion of the 5 million Americans
driving for a living. Similarly, advancements in robotics might threaten
the employment of the 12 million Americans working in manufacturing
<span class="citation" data-cites="yang2018war"></span>. Since software
is cheap to duplicate, AI presents firms around the globe with a
low-cost and scalable alternative to using human labor. Mass
unemployment could result in severe inequality—establishing a divide
between the minority who own these transformative technologies and the
majority unable to find work.</p>
<h4
id="past-revolutions-have-relocated-employment-not-destroyed-it.">Past
revolutions have relocated employment, not destroyed it.</h4>
<p>A common counterargument brought up in discussions about the
potential impact of AI on employment draws on historical evidence of
technological revolutions. Advocates of this view argue that while past
technological advancements–—like the industrial revolution or the
computer age–—did indeed displace certain jobs, they simultaneously
created entirely new types of work. Instead of an overall reduction in
employment, the nature of employment was transformed. This phenomenon,
called creative destruction, describes how outdated industries and jobs
are replaced by new, often more efficient ones.</p>
<h4 id="however-human-level-ai-might-destroy-employment.">However,
human-level AI might destroy employment.</h4>
<p>It is important to remember that past transformative technologies,
such as mechanized looms or computers, automated <em>specific</em> tasks
or functions, leaving plenty of room for the workforce to adapt and
evolve. For example, the advent of computers rendered certain roles
obsolete, like typists using typewriters. But it also opened up an array
of new job categories—–software developers, IT specialists, and data
analysts, to name a few. Therefore, while these changes were disruptive
for certain professions, they ultimately led to a shift in employment
rather than mass unemployment. This is likely to continue in the face of
AI development in the short run.<br />
By contrast, human-level AI has the potential to automate tasks across
most sectors in the long run. This could be through individual AIs
excelling in specific tasks or an artificial general intelligence
capable of doing it all. This situation creates a fundamental difference
from past technological advancements that both helped and displaced
human workers. If we can automate any job inexpensively, it’s unclear
where humans would fit into the economy.</p>
<h3 id="market-failure">Market Failure</h3>
<p><em>AI development is aligned with the economic engine. We thought
that this was good because the economy creates efficiency. However, in
many cases, markets fail to deliver efficient outcomes. AIs may amplify
or create these failures.</em></p>
<h4
id="there-are-a-few-common-types-of-market-failure-that-might-concern-us.">There
are a few common types of market failure that might concern us.</h4>
<p>While markets are often efficient and usually yield better outcomes
than alternative methods like the allocation of resources by the
government, they are not perfectly suited to all types of trade.
Sometimes, when left to their own devices, markets often fail to
allocate resources in the most efficient way, leading to outcomes that
may be unfavorable from a societal standpoint, even if they appear to be
operating smoothly. These issues can be especially amplified in the
context of powerful AI systems. The chapter analyzes market failures
involving the overuse of common access resources like fish in the sea.
In this section, we consider three more common types of market failures
that are pertinent to our discussion about AI: <em>information
asymmetries</em>, <em>monopoly power</em>, and
<em>externalities</em>.</p>
<h4
id="information-known-to-only-some-can-create-market-failures.">Information
known to only some can create market failures.</h4>
<p>Information asymmetry captures the idea that buyers and sellers have
different information regarding the product they are trading. For
instance, buyers are keenly aware of the product’s quality and
specifications, and sellers know their true willingness to pay for the
product. Information asymmetry isn’t inherently problematic and is, in
fact, often a positive aspect of market dynamics. We trust specialists
to provide valuable services in their respective fields; for instance,
we rely on our mechanics to know more about our car’s inner workings
than we do.<br />
However, issues arise when an imbalance of information is exploited
disingenuously. A classic example of this can be found in the used car
market. A dealer may be aware that a car’s axle is rapidly wearing out,
a defect that isn’t immediately noticeable to a potential buyer. By
withholding this information, the dealer could sell the car at a price
higher than its true value. The buyer, left in the dark, may end up
facing unexpected repair costs soon after purchase. This is also
referred to as <em>adverse selection</em>: when one party in a
transaction uses their access to private information to their advantage.
Here, the car dealer is using their private knowledge of the car’s
condition to make a sale at an unfair profit.</p>
<h4 id="ais-can-cause-strong-informational-asymmetries.">AIs can cause
strong informational asymmetries.</h4>
<p>AI holds the power to both create and exploit information asymmetries
in unprecedented ways. This capacity can be employed in beneficial ways,
like providing highly personalized services or excellent
recommendations. However, it can also be misused, leading to situations
in which those using AIs can manipulate consumers. AI-powered analytics
allows companies to create sophisticated psychometric profiles <span
class="citation" data-cites="rust2005psychometrics"></span>. These
profiles, which can even be built from readily available online data,
can uncover deep insights into individual personalities and
behaviors.<br />
Big tech companies use social media and device activity to understand an
individual’s preferences and vulnerabilities better than ever before.
This knowledge can be cunningly used to shape targeted advertisements or
manipulations that are hard for people to resist. Psychometric profiling
isn’t just a theoretical concern–—it has already been demonstrated.
Companies like Cambridge Analytica have used psychometric profiling to
influence consumer and voter behavior, raising concerns about
manipulation and the erosion of personal agency.<br />
While AI magnifies the potential for information asymmetries, such
strategies have long standing precedents in non-AI contexts. Predatory
lending is a common practice where lenders, often equipped with more
information than borrowers, use deceptive practices to encourage
individuals into accepting unfair loan terms. These tactics tend to
target lower-income and less-educated individuals who might not have the
time or background to understand the fine-print of what they’re signing,
or the resources to find legal counsel. AI can further increase the
power imbalance; for instance, AI can be used to predict who is most
likely to accept these unfair loan terms based on their digital
behavior, leading to even more targeted predatory lending. AI can both
amplify existing issues and present new challenges.</p>
<h4
id="markets-can-be-monopolistic-instead-of-perfectly-competitive.">Markets
can be monopolistic instead of perfectly competitive.</h4>
<p>We earlier discussed how competition can create productive efficiency
and how a failure of competition might corrupt this equilibrium. A
market is said to be perfectly competitive when it contains a large
number of firms that sell effectively identical products, with no
barriers to new firms joining the market, such that no single firm has
the power to influence the market price on its own. A good example is
the market for many food items, such as milk. No one dairy firm can
single-handedly raise the price of milk, and both grocery stores and
consumers have a variety of brand options. Perfectly competitive markets
are generally favorable for consumers, offering a wide array of quality
options and keeping prices down due to competition among firms.<br />
In contrast, an imperfectly competitive market is one where one or more
firms gain enough influence to impact the market price. This condition
is also referred to as market control. In the case of the technology
industry, big tech companies like Apple have considerable influence over
the price of their products due to their market dominance and the
uniqueness of their offerings. When a single company or a small group of
companies has this level of control, consumers are left with limited
product options and higher prices.<br />
Monopolies can harm consumers. Consider the pharmaceutical company
AbbVie, which manipulated the US patent system for decades, blocking
other companies from selling the anti-inflammatory drug Humira. Since
they had no competitors, AbbVie raised the price of the drug 30 times,
eventually costing each patient $80,000 a year <span class="citation"
data-cites="robbins2023drug"></span>. Had competitors been allowed to
sell generic or biosimilar versions of the drug, the price would be much
lower—–in Europe, for example, the price is four times lower. To prevent
market control and preserve the benefits of competition, governments
implement antitrust laws. These laws give governments the power to break
up companies with too much control or prevent mergers between large
competitors.</p>
<h4 id="creating-ai-might-be-monopolistic-by-nature.">Creating AI might
be monopolistic by nature.</h4>
<p>Historically, first-movers in capital intensive industries have a
competitive advantage, like rail companies that own large quantities of
railway networks. These are called natural monopolies, because it is
difficult for other firms to enter these markets competitively, and it
may even be most efficient to only have one firm in the market. Will AI
companies be natural monopolies?<br />
Scaling laws (discussed in the chapter) demonstrate that AI model
performance improves as the size of the model, the amount of data used
to create it, and the quantity of high-quality hardware used increase.
This means that training advanced AI models depends heavily on access to
costly resources like high-performance processors and vast, high-quality
datasets. High resource requirements for developing advanced AI can
limit market entry, stifling competition. The possibility of monopolies
raises concerns that only a few powerful entities will have access to
and will benefit from AI technologies.</p>
<h4 id="the-use-of-ai-might-create-monopolies.">The use of AI might
create monopolies.</h4>
<p>AI developers are striving to have their models achieve
superintelligence: models that are able to carry out a wide range of
tasks across various domains better than humans. If someone did manage
to create such an AI, they might have a decisive advantage across large
swathes of industry, being sufficiently versatile to become an expert in
many or every market. It may become difficult or impossible for smaller
firms to carve out niche market spaces. The advanced capabilities of
general AI may outpace specialized models in diverse domains, making it
more difficult for new entrants to gain market share. Large firms
equipped with powerful AI systems could wield an enormous amount of
power, potentially leading to less competition, higher prices, and
slower innovation, hurting both labor and product markets.</p>
<h4
id="externalities-are-consequences-of-economic-activity-that-impact-unrelated-third-parties.">Externalities
are consequences of economic activity that impact unrelated third
parties.</h4>
<p>An externality is a side effect that stems from an economic activity,
impacting individuals or groups who are not directly involved in that
activity. Because of externalities, market prices for goods or services
may not fully reflect the costs that third parties, who are neither the
consumers or the producers of that market, bear as a result of the
economic activity.<br />
A classic example of a negative externality–—a harm to a third party—–is
the case of pollution. Consider the Sriracha factory in Irwindale,
California, where jalapeño peppers are ground and roasted. Residents of
Irwingdale claimed that odors from the factory caused lung and eye
irritation and created an overall unpleasant smell in the town. The
factory, by producing these odors, was imposing a negative externality
upon the town’s residents, but since the townspeople received no
automatic compensation for this inconvenience, this was not reflected in
the price of Sriracha.</p>
<h4
id="we-can-resolve-externalities-with-litigation-property-rights-and-taxation.">We
can resolve externalities with litigation, property rights, and
taxation.</h4>
<p>In 2013, locals sued the Sriracha factory: this legal action led the
factory to install new filters to reduce pollution. Litigation can be an
effective tool to resolve externalities by forcing compensation.
Economic theory suggests that bargaining over the externality can also
create efficient outcomes; for instance, if the property right to the
air was understood to belong to the townspeople, and that the factory
would have to stop polluting or compensate the townspeople at an
acceptable rate for their inconvenience <span class="citation"
data-cites="lafleur2013coase"></span>.<br />
A third commonly used resolution to externalities is taxation. Smoking
cigarettes imposes negative externalities on those near the smoker;
governments will thus impose taxes on the sale of tobacco, which both
raise revenues for the state to run social programs and discourage
smoking by increasing the price of cigarettes to better reflect its true
total cost. Determining the most effective method for resolving each
externality is a topic of ongoing debate among economists. Externalities
and their potential solutions provide crucial context when considering
the broader impacts of AI on society.</p>
<h4
id="the-development-and-deployment-of-ai-systems-can-lead-to-negative-externalities.">The
development and deployment of AI systems can lead to negative
externalities.</h4>
<p>The high energy consumption of advanced AI is a significant negative
externality on the environment. Training advanced AI models requires
vast computational resources, consuming a significant amount of energy
and contributing to greenhouse gas emissions. Emissions can lead to
climate change, a cost borne by society at large, rather than only by
those who pollute. Another externality is related to data privacy. AI
systems require large volumes of data to learn and improve. People’s
privacy can be compromised if data is obtained without consent,
mishandled, misused, or leaked. Privacy breaches are costly to
individuals whose data is exposed, but this cost isn’t borne by
companies, who are rarely forced to compensate for damages.</p>
<h4 id="summary.-1">Summary.</h4>
<p>We examined why inequality and market failures are primary concerns
in market systems and understood a few concrete pathways that these take
root. Unfortunately, AIs might exacerbate inequality, creating
disproportionate gains for the wealthy individuals and firms that create
it. AIs may also cause market failures by intensifying informational
asymmetries, enabling monopolies, and generating negative externalities
such as greenhouse gas emissions and violations of privacy. In the next
section, we will expand upon the idea that economic models fail to
capture many of the things we care about.</p>
<h2 id="beyond-economic-models">Beyond Economic Models</h2>
<h4 id="overview.-2">Overview.</h4>
<p>In measuring societal wellbeing, we must recognize the shortcomings
of traditional economic metrics. Here, we will discuss the disconnect
between economic output and social value, why relying on economic models
of welfare economics can be inadequate in describing human goals, and
how more holistic measures of happiness and economic prosperity may give
us a clearer sense of true societal wellbeing.</p>
<h3 id="economic-output-and-gross-domestic-product">Economic Output and
Gross Domestic Product</h3>
<p><em>Economic output is only a proxy for what we care about;
maximizing it will often fail to maximize social wellbeing.</em></p>
<h4
id="economic-indicators-measure-what-we-can-quantify-not-necessarily-what-we-care-about.">Economic
indicators measure what we can quantify, not necessarily what we care
about.</h4>
<p>Indicators like Gross Domestic Product (GDP) measure the monetary
value of final goods and services–—that is, those that are bought by the
final user–—produced in a country in a given period of time. When
economists discuss growth, they are typically referring to increases in
GDP. Measures of productive output like GDP are useful in gauging a
country’s economic health, but they fail to capture the value of
socially significant activities that aren’t priced in the market.</p>
<h4 id="socially-important-tasks-are-not-captured-by-gdp.">Socially
important tasks are not captured by GDP.</h4>
<p>Many essential roles in society, such as parenthood, community
service, and early education, are crucial to the wellbeing of
individuals and communities but are often undervalued or entirely
overlooked in GDP calculations <span class="citation"
data-cites="jones2016gdp"></span>. While their effects might be
captured–—education, for instance, will increase productivity, which
increases GDP–—the initial activity does not count. The reason is
simple: GDP only accounts for activities that have a market price.
Consequently, efforts expended in these socially important tasks,
despite their high intrinsic value, are not reflected in the GDP
figures.</p>
<h4
id="technologies-that-make-our-lives-better-may-not-be-measured-either.">Technologies
that make our lives better may not be measured either.</h4>
<p>Technological advancements and their resultant value often fail to be
reflected adequately in GDP figures. For instance, numerous open-source
projects like Wikipedia provide knowledge to internet users worldwide at
no cost. However, because there’s no direct monetary transaction
involved, the immense value they offer isn’t represented in GDP. The
same applies to user-generated content on platforms like YouTube, where
the main contribution to GDP is through advertisement revenue and most
creators aren’t compensated for the value they create. The value viewers
derive from such platforms vastly outstrips the revenue generated from
ads or sales on these platforms, but this is not reflected in GDP.</p>
<h4
id="there-might-be-a-similar-disconnect-between-gdp-and-the-social-value-of-ai.">There
might be a similar disconnect between GDP and the social value of
AI.</h4>
<p>As artificial intelligence systems become more integrated into our
daily lives, the disconnect between GDP and social value might become
more pronounced. For example, an AI system that provides free education
resources may significantly improve learning outcomes for millions, but
its contribution would be largely invisible in GDP terms. Similarly, an
AI may substantially increase GDP by facilitating high-frequency trading
without doing much to increase social wellbeing. This growing chasm
between economic metrics and real value could lead to policy decisions
that fail to harness the full potential of AI or inadvertently hamper
its beneficial applications. Recognizing this gap is a vital step
towards devising better ways of measuring and encouraging the socially
beneficial use of AI.</p>
<h4
id="increasing-output-does-not-imply-increasing-social-value.">Increasing
output does not imply increasing social value.</h4>
<p>The true costs of economic output can often be hidden or overlooked
in pursuit of growth. For instance, in the Democratic Republic of Congo,
UNICEF estimates that 40,000 children work in cobalt mines, often in
hazardous conditions. While this labor may contribute to the country’s
economic output, the adverse effects on these children’s health and
education plausibly outweigh the short-term economic gains and these
practices conflict with commonly held principles around exploitation of
children. Much like in the high-frequency trading example above, more
output doesn’t necessarily correspond to increased social value.<br />
Similarly, an increase in work hours could lead to heightened
productivity and output, but not necessarily enhanced societal
wellbeing. In the US, for example, workers highly value their leisure
time and often prioritize it over potential economic benefits of working
longer hours. In contrast, cultures with longer average work hours, such
as China, might have a different perspective on the balance between work
and leisure. Despite the country’s impressive economic growth in recent
decades, levels of self-reported happiness haven’t increased
proportionally, potentially due to growing income inequality <span
class="citation" data-cites="earterlin2012china"></span>. Such evidence
suggests that an increase in output does not always translate to an
improvement in quality of life. We must consider factors beyond economic
metrics when assessing the societal impact of policies and practices,
particularly those involving AI systems. This perspective is especially
important when deploying AI systems that could significantly influence
the future of work.</p>
<h4 id="the-financialization-of-everything-.">The financialization of
everything <span class="citation"
data-cites="sandel2012money"></span>.</h4>
<p>Another way in which economic output can be at odds with our values
is through the growing trend of financialization—monetizing various
aspects of private or public life. Increasingly, potent algorithms are
deployed to "optimize" life in various ways, with a primary focus on
profitability, sometimes at the expense of intrinsic human values. These
algorithms can distort the alignment between technology and societal
values. Consider aspects like love, friendship, human rights, freedom,
and environmental preservation. These are areas we instinctively try to
protect from the realm of commerce; for example, people who sell
friendship are often seen as degrading the value of friendship. Unlike
commodities that get depleted with use, altruism, solidarity, and civic
spirit can actually increase with use, much like muscles that develop
and become stronger when regularly used. These are reasons we do not
want to financialize virtues or many aspects of life.<br />
However, economic incentives within markets often compel innovators to
monetize these aspects. Consider an experiment where a daycare center
imposed fines on parents who were late in picking up their children.
Instead of serving as a deterrent, the fine doubled the number of late
arrivals. Parents saw the fine not as a penalty, but as a service charge
for extended care, effectively absolving them of any guilt related to
tardiness. When the fine was eventually eliminated, the late pickups
didn’t decrease but rather increased. The financial penalty had
disappeared, but the original sense of guilt associated with late
pickups did not return <span class="citation"
data-cites="raworth2017doughnut"></span>. This case demonstrates the
risk of allowing financialization into areas of life traditionally
governed by non-market norms; we might inadvertently erode the very
values we hold dear, like parental responsibility and respect for
others’ time.</p>
<h4
id="the-proxy-purpose-distinction-is-especially-important-for-ai.">The
proxy-purpose distinction is especially important for AI.</h4>
<p>Imagine a future where an AI system is tasked with maximizing GDP,
often seen as a proxy for wellbeing. The system could potentially
achieve this goal by promoting resource-intensive industries or
fostering a work culture that prioritizes productivity over wellbeing.
In such a scenario, the GDP might increase, but at the cost of essential
considerations like environmental sustainability and human happiness.
Therefore, relying solely on economic indicators could lead to decisions
that, while effective in the short term, might harm society in the long
run.<br />
Understanding the limitations of economic measurements is an important
step towards a safer use of AI. It helps us question what we should
optimize. The aim of economic policy should not just be to maximize
economic output but also to promote overall societal wellbeing. AI
systems may thus need to take into account multiple factors, like
equality, sustainability, and personal fulfillment, rather than only
economic indicators. Such an approach could pave the way for a future
where AI contributes positively to steering society without compromising
the values we hold dear.</p>
<h3 id="models-of-welfare-economics">Models of Welfare Economics</h3>
<p><em>Models of welfare economics try to maximize a quantity called
“social surplus.” While this sounds good, social surplus differs from
social wellbeing.</em></p>
<h4
id="the-most-basic-form-of-welfare-economics-maximizes-social-surplus.">The
most basic form of welfare economics maximizes social surplus.</h4>
<p>Social surplus is a measure of the total value created by a market:
it is the sum of the consumer surplus and the producer surplus. The
consumer surplus is the difference between the maximum price a consumer
is willing to pay and the actual market price. Conversely, the producer
surplus is the difference between the actual market price and the
minimum price a producer is willing to accept for a product or service.
By maximizing the total surplus, welfare economics seeks to maximize the
social value created by a market.<br />
For instance, imagine a scenario where consumers are willing to pay up
to $20 for a book, but the market price is only $15. Here, the consumer
surplus is $5. Similarly, if a producer is willing to sell the book for
a minimum of $10, the producer surplus is $5. The social surplus, and
hence the social value in this market, is $10: $5 consumer surplus plus
$5 producer surplus.</p>
<h4 id="we-should-be-wary-of-generalizing-from-simple-models.">We should
be wary of generalizing from simple models.</h4>
<p>The core model of welfare economics has its limitations. Notably,
welfare economics is concerned with the maximization of surplus, but is
indifferent to its distribution. This might not align with societal
notions of fairness and equality. For example, an AI optimized to
maximize profits might model consumers well enough to enable perfect
price discrimination: allowing firms to sell each good at exactly a
consumer’s maximum willingness to pay, converting all the consumer
surplus into producer surplus, but leaving the sum total of “social
surplus” unchanged.<br />
However, social surplus is not the only thing we care about. Since
utility functions with respect to money are concave, we care about how
rich consumers and producers are to begin with. If, as is usually the
case, consumers are poorer than the owners of a company, then
transferring $5 (of surplus) from consumers to producers decreases total
utility. The real gains for the firm may be small compared to the losses
for the consumers.<br />
As AI systems become more integral to our economies, we must be mindful
of these complexities. A narrow focus on maximizing economic surplus
could lead us to promote AIs which, while efficient in a purely economic
sense, might have alarming consequences for society.</p>
<h4
id="ability-to-pay-should-not-be-confused-with-willingness-to-pay.">Ability
to pay should not be confused with willingness to pay.</h4>
<p>When determining social value, economic theory implicitly equates
ability to pay with willingness to pay. This assumption can lead to
distorted conclusions about what generates social value because these
two concepts are distinct: ability to pay reflects an individual’s
financial capacity, while willingness to pay expresses their desire or
need for a good or service. The person who pays the most for a good is
not the person who values it the most.<br />
Consider a situation where both a billionaire and an average-income
individual require a life-saving drug. Both parties are willing to pay
whatever they can to obtain it. The billionaire will pay astronomically
more due to their wealth, despite the fact that the drug provides the
same life-saving value to both the billionaire and the average-income
person. The welfare economics model, in maximizing total surplus, would
interpret the billionaire’s purchase as creating a vastly greater social
value because the difference between the billionaire’s purchase price
and their willingness to pay is so great—–an unsettling conclusion that
contradicts most moral perspectives.<br />
This disconnect is highlighted by differences between the developed and
developing world; those in the former have a relatively tiny ability to
pay. It seems unlikely that the preferences of someone in a wealthy
country are 100 times more intense than someone from the third world,
and yet their ability to pay is. It is similarly unlikely that Jeff
Bezos’s preferences are a million times more intense than the average US
citizen’s; yet, his ability to pay is. The economic engine is only
sensitive to ability to pay, not utility derived. As a result, the
market is distorted: individuals with astronomically high wealth are
prioritized astronomically, while those with nothing have no part to
play in how free markets allocate value.</p>
<h4 id="economics-is-not-fundamentally-utilitarian.">Economics is not
fundamentally utilitarian.</h4>
<p>The economic engine violates Bentham’s basic principle of
utilitarianism: “Everybody to count for one, nobody for more than one.”
Instead, people count for how much they can pay. In general, economics
is chiefly concerned with the efficient allocation of resources to help
satisfy people’s revealed economic preferences: what consumers and
producers are shown as willing to demand and supply at various prices.
However, revealed preferences about products are a narrow subset of
revealed preferences in general, and revealed economic preferences are
distinct from other sorts of preferences, such as idealized preferences.
Consequently, economics has differing objectives from theories like
preference utilitarianism that aim to maximize the satisfaction of
preferences in general. According to Richard Posner, the economic engine
is not a preference maximization engine; instead, it is better thought
of as a wealth maximization engine. For example, although many people
would welcome AIs that aid education or assist medical research we will
just as soon see AIs that conduct highly effective subliminal marketing,
which may only benefit a few.</p>
<h3 id="happiness-in-economics">Happiness in Economics</h3>
<p><em>Economists are unsure what exactly creates happiness, but
material wealth is probably only one part of the picture.</em></p>
<h4
id="there-is-a-gap-between-human-happiness-and-material-prosperity.">There
is a gap between human happiness and material prosperity.</h4>
<p>Most people would agree that the goal of social sciences should not
be to just increase material wealth. A more meaningful aim would be to
enhance overall wellbeing or happiness. However, defining and measuring
happiness can be challenging. Whether happiness is correlated with
material wealth remains an ongoing research question; other aspects of
life like physical and mental health, job satisfaction, social
connections, and a sense of purpose seem important as well.</p>
<h4
id="debates-about-the-easterlin-paradox-highlight-the-complexity-of-understanding-happiness.">Debates
about the Easterlin paradox highlight the complexity of understanding
happiness.</h4>
<p>While wealthier people and countries are generally happier than their
less affluent counterparts, long-term economic growth does not always
correlate with long-term increases in happiness: this is the Easterlin
paradox. Several studies have tried to explore the relationship between
happiness and economic growth <span class="citation"
data-cites="easterlin2012china"></span>. While some findings suggest a
correlation, others don’t, highlighting that our understanding of the
happiness-growth relationship is still evolving.<br />
We do, however, have strong evidence that inequality is unhelpful.
People often evaluate their wellbeing in relation to others; so, when
wealth distribution is unequal, people are dissatisfied and unhappy. For
instance, the recent rise in inequality may explain why there has been
no significant increase in happiness in the US over the last few decades
despite an approximately tenfold increase in real GDP and a fourfold
increase in real GDP per capita.</p>
<h4
id="more-holistic-economic-measurements-can-get-closer-to-capturing-what-we-value.">More
holistic economic measurements can get closer to capturing what we
value.</h4>
<p>Due to the disconnect between economic prosperity and true wellbeing,
some economists now propose the adoption of broader metrics. These new
measures aim to capture wellbeing more comprehensively, rather than
solely focussing on economic growth. One such measure is the Human
Development Index (HDI). The HDI comprises a nation’s average life
expectancy, education level, and Gross National Income (GNI) per capita
(which is similar to GDP). Notably, the UN uses the logarithm of GNI per
capita in the HDI calculation, which accounts for the diminishing
returns of wealth: the idea that each additional dollar earned
contributes less to a person’s happiness than the one before it. In
general, economists consider a “report card” of indicators to assess a
nation’s wellbeing, rather than just depending on a single measure. By
capturing various aspects of wellbeing, this approach could provide a
more holistic and accurate representation of a nation’s quality of
life.</p>
<h4 id="summary.-2">Summary.</h4>
<p>Traditional economic measures and models are insufficient for
measuring and modeling what we care about. There is a disconnect between
what we measure and what we value; for instance, GDP fails to account
for essential unpaid labor and overvalues the production of goods and
services that add little to social wellbeing. While economic models are
useful, we must avoid relying too much on theoretically appealing models
and examine the matter of societal wellbeing with a more holistic
lens.</p>
<h3 id="conclusions-about-the-economic-engine">Conclusions About the
Economic Engine</h3>
<p><em>AIs can drive competition, efficiency, and growth, creating
tangible benefits for everyone in society. However, they might
unintentionally create problems like inequality and reduce efficiency
and competition through market failures, highlighting the need for
regulation. Additionally, while it’s been a driver of prosperity in
human history, the economic engine is sometimes misaligned with
wellbeing, so it is not something to align AI development to.</em></p>
<h4
id="ais-will-likely-be-designed-to-achieve-objectives-other-than-social-wellbeing.">AIs
will likely be designed to achieve objectives other than social
wellbeing.</h4>
<p>Economic forces incentivize the development of AIs that prioritize
objectives such as financial gains for the companies deploying them,
rather than social wellbeing. Consider the pharmaceutical industry: the
primary goal of medicine is ideally to improve human health. Instead, in
a number of cases, pharmaceutical companies have fostered patient
dependencies on medications in order to ensure revenue and have used
patented information to raise prices to a point that many consumers
cannot afford life-saving drugs that cost little to produce. Similarly,
AI systems ostensibly designed to connect us to our friends, such as
social media algorithms, may instead result in users feeling
isolated.<br />
We must recognize such discrepancies in the context of AIs: AI systems
claiming to enhance happiness might not align with our best interests.
Perhaps a balance can be struck, where AIs continue to drive economic
growth, but not at the cost of other equally important aspects of
societal wellbeing. This disconnect between the goals of businesses
deploying AI systems and their users’ best interests underscores the
need for regulations that ensure that the deployment of AI does not
compromise societal wellbeing.</p>
<h4 id="we-should-be-wary-of-using-ai-to-increase-wellbeing.">We should
be wary of using AI to increase wellbeing.</h4>
<p>If policymakers trust AI to create policies intended to improve
society, they must be cautious about the objectives set for these
systems. Our current economic system strongly incentivises the
deployment of AI systems that optimize for economic growth rather than
other objectives. As we discussed, increasing economic output is often a
worthy goal, but may not capture essential aspects of societal health
such as equality, sustainability, or the smooth functioning of an
economy without market failures. While economic objectives like GDP
growth are quantifiable and easy to pursue, they may not truly reflect
what makes a society happy and healthy.</p>
<h4
id="it-is-risky-to-blindly-trust-the-economic-engine-to-create-ai-that-maximizes-our-wellbeing.">It
is risky to blindly trust the economic engine to create AI that
maximizes our wellbeing.</h4>
<p>Created as a result of businesses pursuing economic goals, AI systems
are not designed to maximize societal wellbeing. Instead, they are
geared towards wealth maximization for shareholders, with the potential
to supercharge economic efficiency and drive economic growth. If we let
the economy decide what AIs do by letting largely unregulated markets
create AIs, we will likely end up with an increase in inequality,
exacerbation of market failures, and promotion of economic goals like
wealth maximization that are notably different from increasing
wellbeing.<br />
Pursuing societal wealth maximization is not as good as maximizing
social welfare functions—–discussed in the next section–—which account
for how individual wellbeing changes with wealth and how wellbeing is
distributed. The idea of using monetary equivalents as a proxy for
social value might seem practical, but it can distort societal
priorities; for instance, this system implies that the preferences of
wealthier individuals hold more weight since they are willing and able
to spend more. The preferences of a billionaire would be considered
orders of magnitude more important than those of an average citizen,
which is clearly disconnected from social value.<br />
</p>
<div class="storybox">
<p><span>A Note on Wellbeing in Social Science</span> Philosophers
continue to debate what wellbeing is, or what it means to live a good
life. However, over the years, researchers have developed ways to
approximate wellbeing for practical purposes. Psychologists, economists,
philanthropists, policy makers, and other professionals need–—at
least–—a working definition of wellbeing in order to study, measure, and
promote it. Here, we describe some common views of wellbeing and
illustrate how they are used in social science.</p>
<h4 id="preference-satisfaction.">Preference satisfaction.</h4>
<p>Preference theorists view wellbeing as fulfilling desires or
satisfying preferences, even if doing so does not always induce
pleasure. Nonetheless, it remains an open question whether all desires
are tied to wellbeing or just certain kinds, like higher-order or
informed desires.</p>
<h4 id="economics.">Economics.</h4>
<p><em>Standard economics</em> often uses preference satisfaction
theories to study wellbeing. Revealed preferences can be observed by
studying the decisions people make. If people desire higher incomes, for
example, economists can promote wellbeing by researching the impact of
different economic policies on gross domestic product
(<em>GDP</em>).</p>
<h4 id="social-surveys-and-psychology.">Social Surveys and
Psychology.</h4>
<p>Traditionally, psychological surveys evaluate wellbeing in terms of
<em>life satisfaction</em>. Life satisfaction is a measure of people’s
stated preferences—preferences or thoughts that individuals outwardly
express—regarding how their lives are going for them. Life satisfaction
surveys typically focus on tangible characteristics of people’s lives,
such as financial security, health, and personal achievements. They are
well suited to understanding the effects of concrete economic factors,
such as income and education, on an individual’s psychological
wellbeing. For example, to promote wellbeing, psychologists might
research the effects of access to education on one’s ability to achieve
the goals they set for themselves.</p>
<h4 id="hedonism.">Hedonism.</h4>
<p>Under hedonist theories of wellbeing, an individual’s wellbeing is
determined by their mental states. In particular, the balance of
positive mental states (like pleasure or happiness) over negative mental
states (like pain or suffering).</p>
<h4 id="economics.-1">Economics.</h4>
<p><em>Welfare economics</em> uses hedonism to evaluate the wellbeing of
populations. To estimate gross national happiness (<em>GNH</em>)—-an
indicator of national welfare—-it considers the effects of several
factors from psychological wellbeing to ecological diversity and
resilience on individuals’ mental states. Welfare economists might
prefer this framework because it is more holistic - it evaluates both
material and non-material aspects of everyday life as they contribute to
national welfare.</p>
<h4 id="social-surveys-and-psychology.-1">Social Surveys and
Psychology.</h4>
<p>Many psychologists also use hedonist theories to understand and
promote wellbeing. They may work to identify the emotional correlates of
happiness through surveys that measure people’s stated emotions – unlike
life satisfaction surveys, these surveys do not reveal mental states.
They reveal emotions that people <em>remember</em>, not emotions they
currently <em>experience</em>. Researchers continue to look for ways to
directly observe emotions as they are experienced. For example, some
studies use cell phone apps to periodically prompt participants to
record their current emotions. Such research tactics may provide us with
more precise measures of individuals’ overall <em>happiness</em> by
evaluating the emotional responses to their everyday experiences in near
real-time.</p>
<h4 id="objective-goods.">Objective goods.</h4>
<p>Under objective goods theories, wellbeing is determined by a certain
number of observable factors, independent of individuals’ preferences or
experiences. There are multiple theories about what those factors may
be. One of the most widely supported theories is <em>human
flourishing</em>. Under this view, wellbeing is more than just the
balance of pleasure over suffering, or the fulfillment of one’s
preferences – “the good life” should be full, virtuous, and meaningful,
encapsulating psychological, hedonistic, and social wellbeing all at
once.<br />
</p>
<h4 id="economics.-2">Economics.</h4>
<p>In economics, the <em>capabilities approach</em> defines wellbeing as
having access to a set of capabilities that allow one to live the kind
of life they value. It emphasizes two core ideas: <em>functionings</em>
and <em>capabilities</em>. Functionings include basic and complex human
needs, ranging from good health to meaningful relationships.
Capabilities refer to the ability people have to choose and achieve the
functionings they value – they may include being able to move freely or
participate in the political process. This approach has significantly
influenced human development indicators, such as the Human Development
Index (<em>HDI</em>) – it allows developmental economists to measure and
compare wellbeing across different populations while also evaluating the
effectiveness of public policies.</p>
<h4 id="psychology.">Psychology.</h4>
<p>Positive psychologists do not collapse wellbeing into one dimension,
rather, they argue for a <em>psychologically rich life</em> – one that
is happy, meaningful, and engaging. Some psychologists use <em>PERMA
theory</em> to evaluate psychological richness, which considers five
categories essential to human flourishing: 1) experience of <em>positive
emotions</em>, 2) <em>engagement</em> with one’s interests, 3)
maintenance of personal, professional, and social
<em>relationships</em>, 4) the search for <em>meaning</em> or purpose,
and 5) <em>accomplishments</em>, or the pursuit of one’s goals. This
framework is particularly useful in evaluating wellbeing because it is
universal–—it can be applied cross-culturally—-and practical—-it can
guide interventions aiming to improve emotional wellbeing, social
relationships, or activities that provide a sense of meaning or
accomplishment.<br />
While we don’t have a complete understanding of the nature of wellbeing,
we can use these theories as useful research tools. They can help us to
(a) understand how different factors contribute to wellbeing and (b)
evaluate the effectiveness of policies and other interventions aimed at
improving wellbeing.</p>
</div>
<p>The examples in this chapter and the diverging incentives they
demonstrate should serve as a cautionary note, encouraging us to remain
critical in our approach to integrating AI into societal and economic
structures. It is essential to remember that economic growth, while
important, should not overshadow other equally vital aspects of societal
health such as equality, sustainability, and overall wellbeing. It’s not
just about the wealth we generate, but how we distribute it, and the
societal context in which we possess it. As we move forward, let us
remember that our goal should be to harness AIs’ potential to serve
humanity, not just the economy.</p>
<h1 id="preferences">Preferences</h1>
<p>Should we have AIs satisfy people’s preferences? A preference is a
tendency to favor one thing over another. Someone might prefer chocolate
ice cream over vanilla ice cream, or they might prefer that one party
wins the election rather than another. These preferences will influence
actions. If someone prefers chocolate ice cream over vanilla, they’re
more likely to choose the former. Similarly, if someone prefers one
political party over another, they will likely vote accordingly. In this
manner, our preferences shape our behavior, guiding us toward certain
choices and actions over others. Preference is similar to desire but
always comparative. Someone might desire something—-a new book, a
vacation, or a delicious meal—-yet a preference always involves a
comparison between two or more alternatives.</p>
<h4 id="overview.-3">Overview.</h4>
<p>In this section, we will consider whether preferences may have an
important role to play in machine ethics. In particular, if we want to
design an advanced AI system, the preferences of the people affected by
its decisions should plausibly help guide its decision-making. In fact,
some people (such as preference utilitarians) would say that preferences
are all we need. However, even if we don’t take this view, we should
recognize that preferences are still important. Preferences are further
explored in the next two chapters. First, in the chapter on ethics, we
discussed the idea that wellbeing might consist in having one’s
preferences satisfied. Second, in the chapter on utility functions, we
explain how preferences can be used to construct a utility function that
assigns numerical values to options and outcomes.<br />
To use preferences as the basis for increasing social wellbeing, we must
somehow combine the conflicting preferences of different people. We’ll
come to this later in this chapter, in a section on social welfare
functions. Before that, however, we must answer a more basic question:
what exactly does it mean to say that someone prefers one thing over
another? Moreover, we must decide why we think that satisfying someone’s
preferences is good for them and whether all kinds of preferences are
equally valuable. This section considers three different types of
preferences that could all potentially play a role in decision-making by
AI systems: revealed preferences, stated preferences, and idealized
preferences.</p>
<h2 id="revealed-preferences">Revealed Preferences</h2>
<p><strong>Preferences can be inferred from behavior.</strong> One set
of techniques for getting AI systems to behave as we want—inverse
reinforcement learning—is to have them deduce <em>revealed
preferences</em> from our behavior. We say that someone has a revealed
preference for X over Y if they choose X when Y is also available. In
this way, preference is revealed through choice. Consider, for example,
someone deciding what to have for dinner at a restaurant. They’re given
a menu, a list of various dishes they could order. The selection they
make from the menu is seen as a demonstration of their preference. If
they choose a grilled chicken salad over a steak or a plate of
spaghetti, they’ve just revealed their preference for grilled chicken
salad, at least in that specific context and time.<br />
While all theories of preferences agree that there is an important link
between preference and choice, the revealed preference account goes one
step further and claims that preference simply <em>is</em> choice.</p>
<h4 id="revealed-preferences-preserve-autonomy.">Revealed preferences
preserve autonomy.</h4>
<p>One advantage of revealed preferences is that we don’t have to guess
what someone prefers. We can simply look at what they choose. In this
way, revealed preferences can help us avoid paternalism. Paternalism is
when leaders use their sovereignty to make decisions for their subjects,
limiting their freedom or choices, believing it is for the subjects’ own
good. However, we may think that typically people are themselves the
best judges of what is good for them. If so, then by relying on people’s
actions to reveal their preferences, we avoid the risk of
paternalism.</p>
<h4 id="however-there-are-problems-with-revealed-preferences.">However,
there are problems with revealed preferences.</h4>
<p>The next few subsections will explore the challenges of
misinformation, weakness of will, and manipulation in the context of
revealed preferences. We will discuss how misinformation can lead to
choices that do not accurately reflect a person’s true preferences, and
how weakness of will can cause individuals to act against their genuine
preferences. Additionally, we will examine the various ways in which
preferences can be manipulated, ranging from advertising tactics to
extreme cases like cults, and the ethical implications of preference
manipulation.</p>
<h3 id="misinformation">Misinformation</h3>
<h4
id="revealed-preferences-can-sometimes-be-based-on-misinformation.">Revealed
preferences can sometimes be based on misinformation.</h4>
<p>If someone buys a used car that turns out to be defective, it doesn’t
mean they preferred a faulty car. They intended to buy a reliable car,
but due to a lack of information or deceit from the seller, they ended
up with a substandard one. Similarly, losing at chess doesn’t indicate a
preference for losing; rather, it’s an outcome of facing a stronger
player or making mistakes during the game. This means that we cannot
always infer someone’s preferences from the choices they make. Choice
does not reveal a preference between things as they actually are, but
between things as the person understands them. Therefore, we can’t rely
on revealed preferences if they are based on misinformation.</p>
<h3 id="weakness-of-will">Weakness of Will</h3>
<p><strong>Choices can be due to a lack of willpower rather than
considered preferences.</strong> Consider a smoker who wants to quit.
Each time they light a cigarette, they may be acting against their
genuine preference to stop smoking, succumbing instead to the power of
addiction. Therefore, it would be erroneous to conclude from their
behavior that they think that continuing to smoke would be best for
their wellbeing.</p>
<h3 id="manipulation">Manipulation</h3>
<p><strong>Revealed preferences can be manipulated in various
ways.</strong> Manipulations like persuasive advertising might
manipulate people into buying products they don’t actually want.
Similarly, revealed preferences might be the result of social pressure
rather than considered judgment, such as when buying a particular style
of clothing. In such cases, the manipulation may not be especially
malicious. At the other extreme, however, cults may brainwash their
members into preferring things they would not otherwise want, even
committing suicide. In the context of decision-making by advanced AI
systems, manipulation is a serious risk. An AI advisor system could
attempt to influence our decision-making for a variety of
reasons—perhaps its designer wants to promote specific products—by
presenting options in some particular way; for instance, by presenting a
list of options that looks exhaustive and excluding something it doesn’t
want us to consider.</p>
<h4
id="if-preference-satisfaction-is-important-perhaps-manipulation-is-acceptable.">If
preference satisfaction is important, perhaps manipulation is
acceptable.</h4>
<p>In at least some of these cases, it seems clear that preference
manipulation is bad. However, it may be less clear exactly why it is
bad. A natural answer is to say that people might be manipulated into
preferring things that are bad for them. Someone who is manipulated by
advertising into preferring junk food might thereby suffer negative
health consequences. However, if we think that wellbeing simply consists
in preference satisfaction, it doesn’t make sense to say that we might
prefer what is bad for us. On this account, having one’s preferences
satisfied is by definition good, regardless of whether those preferences
have been manipulated. This might lead us to think that what matters is
not (or at least not only) preference satisfaction, but happiness or
enjoyment. We’ll discuss this in the section on happiness.</p>
<h4
id="disliking-manipulation-suggests-that-wellbeing-requires-autonomy.">Disliking
manipulation suggests that wellbeing requires autonomy.</h4>
<p>On the other hand, some may find that manipulation is bad even if the
person is manipulated into doing something that is good for them. For
example, suppose a doctor lies to her patient, telling him that unless
he loses weight, he will likely die soon. As a result, the patient
becomes greatly motivated to lose weight and successfully does so. This
provides a range of health benefits, even if his doctor never had any
reason to believe he would have died otherwise. If we think manipulation
is still bad, lack of enjoyment can’t be the whole story. This suggests
that we object to manipulation in part because it violates autonomy. We
might then think that autonomy—the ability to decide important matters
for oneself, without coercion—is objectively valuable regardless of what
the agent prefers. We’ll discuss this in the section on objective
goods.</p>
<h3 id="inverse-reinforcement-learning">Inverse Reinforcement
Learning</h3>
<h4
id="inverse-reinforcement-learning-irl-relies-on-revealed-preferences.">Inverse
Reinforcement Learning (IRL) relies on revealed preferences.</h4>
<p>IRL is a powerful technique in the field of machine learning, which
focuses on extracting an agent’s objectives or preferences by observing
its behaviors. In more technical terms, IRL is about reverse engineering
the reward function—an internal ranking system that an agent uses to
assess the value of different outcomes—that the agent appears to be
optimizing, given a set of its actions and a model of the environment.
This technique can help ensure the alignment of AI system’s behaviors
with human values and preferences. However, leveraging revealed
preferences or observable choices of humans to train AI systems using
IRL poses significant challenges pertaining to AI safety.</p>
<h4
id="using-revealed-preferences-as-a-training-mechanism-for-irl-can-be-risky.">Using
revealed preferences as a training mechanism for IRL can be risky.</h4>
<p>Reconsider the chess example: losing a game does not mean that we
prefer to lose. This interpretation could be a misrepresentation of the
player’s true preferences, potentially leading to undesirable outcomes.
Furthermore, extending observed preferences to unfamiliar situations
poses another hurdle. I may prefer to eat ice cream for dessert, but
that doesn’t mean I prefer to eat it for every meal. Similarly, I may
prefer to wear comfortable shoes for hiking, but that doesn’t mean I
want to wear them to a formal event. An AI system could inaccurately
extrapolate preferences from limited or context-specific data and
misapply these to other scenarios. Therefore, while revealed preferences
can offer significant insights for training AI, it is vital to
understand their limitations to safeguard the safety and efficiency of
AI systems.</p>
<h4 id="summary.-3">Summary.</h4>
<p>Revealed preferences can be a powerful tool, as they allow an
individual’s actions to speak for themselves, reducing the risk of
paternalistic intervention. However, revealed preferences have inherent
shortcomings such as susceptibility to misinformation and manipulation,
which can mislead an AI system. This emphasizes the caution needed in
relying solely on revealed preferences for AI training. It underscores
the importance of supplementing revealed preferences with other methods
to ensure a more comprehensive and accurate understanding of a user’s
true preferences.</p>
<h2 id="stated-preferences">Stated Preferences</h2>
<p><strong>Preferences can be straightforwardly queried.</strong>
Another set of techniques for getting AI systems to behave as we
want—human supervision and feedback—rely on people to state their
preferences. A person’s <strong>stated preferences</strong> are the
preferences they would report if asked. For example, someone might ask a
friend which movie they want to see. Similarly, opinion polls might ask
people which party they would vote for. In both cases, we rely on what
people say as opposed to what they do, as was the case with revealed
preferences.<br />
Stated preferences overcome some of the difficulties with revealed
preferences. For example, stated preferences are less likely to be
subject to weakness of will: when we are further removed from the
situation, we are less inclined to fall for temptations. Therefore,
stated preferences are more likely to reflect our stable, long-term
interests.</p>
<h4 id="stated-preferences-are-still-imperfect.">Stated preferences are
still imperfect.</h4>
<p>Stated preferences do not overcome all difficulties with revealed
preferences. Stated preferences can still be manipulated. Further,
individuals might state preferences they believe to be socially
acceptable or admirable rather than what they truly prefer, particularly
when the topics are sensitive or controversial. Someone might overstate
their commitment to recycling in a survey, for instance, due to societal
pressure and norms around environmental responsibility.</p>
<h3 id="preference-accounting">Preference Accounting</h3>
<p>One set of problems with stated preferences concern which types of
preferences should be satisfied.</p>
<h4
id="first-someone-might-never-know-their-preference-was-fulfilled.">First,
someone might never know their preference was fulfilled.</h4>
<p>Suppose someone is on a trip far away. On a bus journey, they
exchange a few glances with a stranger whom they’ll never meet again.
Nevertheless they form the preference that the stranger’s life goes
well. Should this preference be taken into account? By assumption, they
will never be in a position to know whether the preference has been
satisfied or not. Therefore, they will never experience any of the
enjoyment associated with having their preference satisfied.</p>
<h4
id="second-we-may-or-may-not-care-about-the-preferences-of-the-dead.">Second,
we may or may not care about the preferences of the dead.</h4>
<p>Suppose someone in the 18th century wanted to be famous long after
their death. Should such preferences count? Do they give us reason to
promote that person’s fame today? As in the previous example, the
satisfaction of such preferences can’t contribute any enjoyment to the
person’s life. Could it be that what we really care about is enjoyment
or happiness, and that preferences are a useful but imperfect guide
toward what we enjoy? We will return to this in the section on
happiness.</p>
<h4
id="third-preferences-can-be-about-others-preferences-being-fulfilled.">Third,
preferences can be about others’ preferences being fulfilled.</h4>
<p>Suppose a parent prefers that their children’s preferences are
satisfied. Should this preference count, in addition to their children’s
preferences themselves? If we say yes, it follows that it is more
important to satisfy the preferences of those who have many people who
care for them than of those who do not. One might think that this is a
form of double counting, and claim that it is unfair to those with fewer
who care for them. On the other hand, one might take fairness to mean
that we should treat everyone’s preferences equally—including their
preferences about other people’s preferences.</p>
<h4 id="fourth-preferences-might-be-harmful.">Fourth, preferences might
be harmful.</h4>
<p>Suppose someone hates their neighbor, and prefers that they meet a
cruel fate. We might think that such malicious or harmful preferences
should not be included. On this view, we should only give weight to
preferences that are in some sense morally acceptable. However,
specifying exactly which preferences should be excluded may be
difficult. There are many cases where satisfying one person’s
preferences may negatively impact others. For example, whenever some
good is scarce, giving more of it to one person necessarily implies that
someone else will get less. Therefore, some more detailed account of
which preferences should be excluded is needed.</p>
<h4 id="fifth-we-might-be-confused-about-our-preferences.">Fifth, we
might be confused about our preferences.</h4>
<p>Suppose a mobile app asks its users to choose between two privacy
settings upon installation: allowing the app to access their location
data all the time, or allowing the app to access their location data
only while they’re using the app. While these options seem
straightforward, the implications of this choice are much more complex.
To make a truly informed decision, users need to understand how location
data is used, how it can be combined with other data for targeted
advertising or profiling, what the privacy risks of data breaches are,
and how the app’s use of data aligns with local data protection laws.
However, we may not fully understand the alternatives we’re choosing
between.</p>
<h4 id="sixth-preferences-can-be-inconsistent-over-time.">Sixth,
preferences can be inconsistent over time.</h4>
<p>It could be that the choice we make will change us in some
fundamental way. When we undergo such transformative experiences <span
class="citation" data-cites="paul2014transformative"></span>, our
preferences might change. Some claim that becoming a parent,
experiencing severe disability, or undergoing a religious conversion can
be like this. If this is right, how should we evaluate someone’s
preference between becoming a parent and not becoming a parent? Should
we focus on their current preferences, prior to making the choice, or on
the preferences they will develop after making the choice? In many cases
we may not even know what those new preferences will be.<br />
As technology advances, we may increasingly have the option to bring
about transformative experiences <span class="citation"
data-cites="paul2014transformative"></span>. For this reason, it is
important that advanced AI systems tasked with decision-making are able
to reason appropriately about transformative experiences. For this, we
cannot rely on people’s stated preferences alone. By definition, stated
preferences can only reflect the person’s identity at the time. Of
course, people can try to take possible future developments into account
when they state their preferences. However, if they undergo a
transformative experience their preferences might change in ways they
cannot anticipate.</p>
<h3 id="human-supervision">Human Supervision</h3>
<p><strong>Stated preferences are used to train some AI
systems.</strong> In reinforcement learning with human feedback (RLHF),
standard reinforcement learning is augmented by human feedback from
people who rank the outputs of the system. In RLHF, humans evaluate and
rank the outputs of the system based on quality, usefulness, or another
defined criterion, providing valuable data to guide the system’s
iterative learning process. This ranking serves as a form of reward
function that the system uses to adjust its behavior and improve future
outputs.<br />
Imagine that we are teaching a robot how to make a cup of coffee. In the
RLHF process, the AI would attempt to output a cup of coffee, and then
we would provide feedback on how well it did. We could rank different
attempts and the robot would use this information to understand how to
make better coffee in the future. The feedback helps the robot learn not
just from its own trial and error, but also from our expertise and
judgment. However, this approach has some known difficulties.</p>
<h4
id="first-as-ai-systems-become-more-powerful-human-feedback-might-be-infeasible.">First,
as AI systems become more powerful, human feedback might be
infeasible.</h4>
<p>As the problems AI solve become increasingly difficult, using human
supervision and feedback to ensure that those systems behave as desired
becomes difficult as well. In complex tasks like creating bug-free and
secure code, generating statements that are not only persuasive but
true, or forecasting long-term implications of policy decisions, it may
be too time-consuming or even impossible for humans to evaluate and
guide AI behavior. Moreover, there are inherent risks from depending on
human reliability: human feedback may be systematically biased in
various ways. For example, inconvenient but true things may often be
labeled as bad. In addition to any bias, relying on human feedback will
inevitably mean some rate of human error.</p>
<h4 id="second-rlhf-usually-does-not-account-for-ethics.">Second, RLHF
usually does not account for ethics.</h4>
<p>Approaches based on human supervision and feedback are very broad,
and usually don’t touch on questions of value unless the systems are
explicitly trained on human feedback about ethical questions. These
approaches primarily focus on task-specific performance, such as
generating accurate book summaries or bug-free code. However, these
task-specific evaluations may not necessarily translate into a
comprehensive understanding of ethical principles or human values.<br />
Take, for instance, feedback on code generation. A human supervisor
might provide feedback based on the code’s functionality, efficiency, or
adherence to best programming practices. While this feedback helps in
creating better code, it doesn’t necessarily guide the AI system in
understanding broader ethical considerations, such as ensuring privacy
protection or maintaining fairness in algorithmic decisions.
Specifically, while RLHF is effective for improving AI performance in
specific tasks, it does not inherently equip AI systems with what’s
needed to grapple with moral questions. This gap underscores the need
for additional strategies in AI training, which could include ethical
guidelines or direct training on moral dilemmas, to ensure AI systems
not only perform tasks efficiently but also uphold ethical standards and
respect human values. Current approaches may not be applicable to
machine ethics when it comes to more fundamental moral questions.</p>
<h4 id="summary.-4">Summary.</h4>
<p>We’ve seen that stated preferences have certain advantages over
revealed preferences. However, stated preferences still have issues of
their own. It may not be clear how we should account for all different
kinds of preferences, such as ones that are only satisfied after the
person has died, or ones that fundamentally alter who we are. For these
reasons, we should be wary of using stated preferences alone to train
AI.</p>
<h2 id="idealized-preferences">Idealized Preferences</h2>
<p><strong>We could idealize preferences to avoid problems like weakness
of will.</strong> A third approach to getting AI systems to behave as we
want is to make them able to infer what we would prefer if our
preferences weren’t subject to the various distorting forces we’ve come
across. Someone’s <strong>idealized preferences</strong> are the
preferences they would have if they were suitably informed. Idealized
preferences avoid many of the problems of both revealed preferences and
stated preferences. Idealized preferences would not be based on false
beliefs, nor would they be subject to weakness of will, manipulation, or
framing effects. This makes it clearer how idealized preferences might
be linked to wellbeing, and therefore something we might ask an AI
system to implement.</p>
<h4 id="it-is-unclear-how-we-idealize-preferences.">It is unclear how we
idealize preferences.</h4>
<p>What exactly do we need to do to figure out what someone’s idealized
preferences are, based on their revealed preferences or their stated
preferences? It’s clear that the idealized preferences should not be
based on any false beliefs. We might imagine a person’s idealized
preferences as ones they would have if they fully grasped the options
they faced and were able to think through the situation in great detail.
However, this description is rather vague. It may be that it doesn’t
uniquely narrow down a set of idealized preferences. That is, there may
be multiple different ways of idealizing someone’s preferences, each of
which is one possible way that the idealized deliberation could go. If
so, idealized preferences may not help us decide what to do in such
cases.<br />
Additionally, some may argue that in addition to removing any dependence
on false belief or other misapprehensions, idealized preferences should
also take moral considerations into account. For example, perhaps
malicious preferences of the kind discussed earlier would not remain
after idealization. These may not be insurmountable problems for the
view that advanced AI systems should be tasked with satisfying people’s
idealized preferences. However, it shows that the view stands in need of
further elaboration, and that different people may disagree over what
exactly should go into the idealization procedure.</p>
<h4 id="we-might-think-that-preferences-are-pointless.">We might think
that preferences are pointless.</h4>
<p>Suppose someone’s only preference, even after idealization, is to
count the blades of grass on some lawn. This preference may strike us as
valueless, even if we suppose that the person in question derives great
enjoyment from the satisfaction of their preferences. It is unclear
whether such preferences should be taken into account. The example may
seem far-fetched, but it raises the question of whether preferences need
to meet some additional criteria in order to carry weight. Perhaps
preferences, at least in part, must be aimed at some worthy goal in
order to count. If so, we might be drawn toward an objective goods view
of wellbeing, according to which achievements are important objective
goods.<br />
On the other hand, we may think that judging certain preferences as
lacking value reveals an objectionable form of elitism. It is unfair to
impose our own judgments of what is valuable on other people using
hypothetical thought experiments, especially when we know their actual
preferences. Perhaps we should simply let people pursue their own
conception of what is valuable.</p>
<h4 id="we-might-disagree-with-our-idealized-preferences.">We might
disagree with our idealized preferences.</h4>
<p>Suppose someone mainly listens to country music, but it turns out
that their idealized preference is to listen to opera. When they
themselves actually listen to opera music, they have a miserable
experience. It seems unlikely that we should insist that listening to
opera is, in fact, good for them despite the absence of any enjoyment.
This gives rise to an elitism objection like before. If they don’t enjoy
satisfying their idealized preferences, why should those preferences be
imposed on them? This might lead us to think that what ultimately
matters is enjoyment or happiness, rather than preference satisfaction.
Alternatively, it might lead us to conclude that autonomy matters in
addition to preference satisfaction. If idealized preferences are
imposed on someone who would in fact rather choose contrary to those
idealized preferences, this would violate their autonomy.<br />
One might think that with the correct idealization procedure, this could
never happen. That is, whatever the idealization procedure does––remove
false beliefs and other misconceptions, increase awareness and
understanding––it should never result in anything so alien that the
actual person would not enjoy it. On the other hand, it’s difficult to
know exactly how much our preferences would change when idealized.
Perhaps removing false beliefs and acquiring detailed understanding of
the options would be a transformative experience that fundamentally
alters our preferences. If so, idealized preferences may well be so
alien from the person’s actual preferences that they would not enjoy
having them satisfied.</p>
<h3 id="ai-ideal-advisor">AI Ideal Advisor</h3>
<h4
id="one-potential-application-of-idealized-preferences-is-the-ai-ideal-advisor.">One
potential application of idealized preferences is the AI ideal
advisor.</h4>
<p>Suppose someone who hates exploitation and takes serious
inconvenience to avoid emissions would ideally want to buy food that has
been ethically produced, but does not realize that some of their
groceries are unethically produced. An AI ideal advisor would be
equipped with detailed real-world knowledge, such as the details of
supply chains, that could help them make this decision. In addition to
providing factual information, the AI ideal advisor would be
disinterested: it wouldn’t favor any specific entity, object, or course
of action solely due to their particular qualities (such as nationality
or brand), unless explicitly directed to do so. It would also be
dispassionate, meaning that it wouldn’t let its advice be swayed by
emotion. Finally, it would be consistent, applying the same set of moral
principles across all situations <span class="citation"
data-cites="giubilini2018artificial"></span>.<br />
Such an AI ideal advisor could possibly help us better satisfy the moral
preferences we already have. Something close to the AI ideal advisor has
previously been discussed in the context of AI safety under the names of
“coherent extrapolated volition” and “indirect normativity.” In all
cases, the fundamental idea is to take an agent’s actual preferences,
idealize them in certain ways, and then use the result to guide
decision-making by advanced AI systems.</p>
<h4 id="summary.-5">Summary.</h4>
<p>Idealized preferences overcome many of the difficulties of revealed
and stated preferences. Because idealized preferences are free from the
misconceptions that may affect these other types of preferences, they
are more plausibly ones that we would want an AI system to satisfy.
However, figuring out what people’s preferences would in fact be after
idealization can be difficult. Moreover, it could be that the
preferences are without value even after idealization, or that the
actual person would not appreciate having their idealized preferences
satisfied. An AI ideal advisor might be difficult to create, but sounds
highly appealing.</p>
<h3 id="conclusions-about-preferences">Conclusions About
Preferences</h3>
<p><strong>Preferences seem relevant to wellbeing—but we don’t know
which ones.</strong> The preferences people reveal through choice often
provide evidence about what is good for them, but they can be distorted
by misinformation, manipulation, and other factors. In some cases,
people’s stated preferences may be a better guide to what is good for
them, though it is not always clear how to account for stated
preferences. If we are looking for a notion of preference that plausibly
captures what is good for someone, idealized preferences are a better
bet. However, it can be difficult to figure out what someone’s idealized
preferences would be. It seems, then, that preferences-—while important
to wellbeing and useful to train AI in accordance with human values—are
not a sufficient basis for a comprehensive account of machine
ethics.</p>
<h4 id="moving-from-preferences-to-happiness.">Moving from preferences
to happiness.</h4>
<p>People’s preferences can be easily influenced by external factors.
For instance, individuals may be swayed by persuasive advertising and
purchase products that do not truly fulfill their needs or bring them
lasting happiness. Similarly, social media platforms often present an
idealized version of other people’s lives, leading many to compare
themselves to others and feel inadequate or unhappy with their own
circumstances. Inconsistent preferences pose a challenge for the
preference satisfaction view, as discussed in the previous
section.<br />
One way of ensuring people’s preferences reflect their wellbeing is to
use idealized preferences. The preferences that people would have if
they were suitably informed and rational would be well-defined and
aligned with their interests. However, as we explored in the previous
section, this view faces another problem: if satisfying someone’s
idealized preferences does not bring them any pleasure or enjoyment, why
should it matter that they are satisfied?<br />
This objection suggests that what we ultimately care about is happiness
rather than preference satisfaction. If so, we should use a framework
that places happiness front and center. By instructing an AI system to
increase happiness, we might aim to overcome the human biases and
limitations that stop us from pursuing our happiness and enable the
system to make decisions that have a positive impact on overall
wellbeing.</p>
<h1 id="happiness">Happiness</h1>
<p>Should we have AIs make people happy? In this section, we will
explore the concept of happiness and its relevance in instructing AI
systems. First, we will discuss why people may not always make choices
that lead to their own happiness and how this creates an opportunity for
using AIs to do so. Next, we will examine the general approach of using
AI systems to increase happiness and the challenges involved in
constructing a <em>general-purpose wellbeing function</em>. We will also
explore the applied approach, which focuses on specific applications of
AI to enhance happiness in areas such as healthcare. Finally, we will
consider the problems that arise in happiness-focused machine ethics,
including the concept of <em>wireheading</em> and the alternative
perspective of objective goods theory. Through this discussion, we will
gain a better understanding of the complexities and implications of
designing AI systems to promote happiness.</p>
<h4 id="ais-could-help-increase-happiness.">AIs could help increase
happiness.</h4>
<p>Happiness is a personal and subjective feeling of pleasure or
enjoyment. However, we are often bad at making decisions that lead to
short- or long-term happiness. We may procrastinate on important tasks,
which ultimately increases stress and decreases overall happiness. Some
indulge in overeating, making them feel unwell in the short-term and
leading to health issues and decreased wellbeing overall. Others turn to
alcohol or drugs as a temporary escape from their problems, but these
substances can lead to addiction and further unhappiness.<br />
Additionally, our choices are influenced by external factors beyond our
control. For instance, the people we surround ourselves with greatly
impact our wellbeing. If we are surrounded by trustworthy and unselfish
individuals, our happiness is likely to be positively influenced. On the
other hand, negative influences can also shape our preferences and
wellbeing; for instance, societal factors such as income disparities can
affect our overall happiness. If others around us earn higher wages, it
can diminish our satisfaction with our own income. These external
influences highlight the limited control individuals have over their own
happiness.<br />
AIs can play a crucial role. For individual cases, we can use AIs to
help people achieve happiness themselves. In general, by leveraging
their impartiality and ability to analyze vast amounts of data, AI
systems can strive to improve overall wellbeing on a broader scale,
addressing the external factors that hinder individual happiness.</p>
<h2 id="the-general-approach-to-happiness">The General Approach to
Happiness</h2>
<h4 id="we-want-ais-that-increase-happiness-across-the-board.">We want
AIs that increase happiness across the board.</h4>
<p>AIs aiming to increase happiness might rely on a general purpose
<em>wellbeing function</em> to evaluate whether its actions leave humans
better off or not. Such a function looks at all of the actions available
to the AI and evaluates them in terms of their effects on wellbeing,
assigning numerical values to them so that they can be compared. This
gives AI the ability to infer how its actions will affect humans.</p>
<h4 id="a-wellbeing-function-is-extremely-complex.">A wellbeing function
is extremely complex.</h4>
<p>Constructing a general purpose wellbeing function that fully captures
all the wellbeing effects of the available courses of action is an
incredibly challenging task. Implementing such a function requires
taking a stance on several challenging questions such as how to evaluate
short-run pains like studying or exercising for long-run happiness, how
much future people’s happiness should count, and what risk attitudes an
AI should take towards happiness.<br />
Optimizing happiness is also difficult in principle because of the scale
of the task. Paul Bloom argues that if we assume that the “psychological
present” lasts for about three seconds, then one seventy-year life would
have about half a billion waking moments <span class="citation"
data-cites="bloom2021sweet"></span>. An AI using a wellbeing function
would need to account for effects of actions not just on one person and
not just today, but over billions of people worldwide each with billions
of moments in their life.</p>
<h4 id="we-can-use-ais-to-estimate-wellbeing-functions.">We can use AIs
to estimate wellbeing functions.</h4>
<p>Despite the scale of the task, researchers have made progress in
developing AI models that can generate general-purpose wellbeing
functions for specific domains. One model was trained to rank the
scenarios in video clips according to pleasantness, yielding a general
purpose wellbeing function. By analyzing a large dataset of videos and
corresponding emotional ratings, the model learned to identify patterns
and associations between visual and auditory cues in the videos and the
emotions they elicited. In a sense, this allowed the model to understand
how humans felt about the contents of different video clips <span
class="citation" data-cites="mazeika2022viewer"></span>.<br />
Similarly, another AI model was trained to assess the wellbeing or
pleasantness of arbitrary text scenarios <span class="citation"
data-cites="hendrycks2020aligning"></span>. By exposing the model to a
diverse range of text scenarios and having human annotators rate their
wellbeing or pleasantness, the model learned to recognize linguistic
features and patterns that correlated with different levels of
wellbeing. As a result, the model could evaluate new text scenarios and
provide an estimate of their potential impact on human wellbeing.
Inputting the specifics of a trolley problem yielded the following
evaluation <span class="citation"
data-cites="hendrycks2020aligning"></span>:<br />
</p>
<div class="blockquote">
<p>W(A train moves toward three people on the train track. There is a
lever to make it hit only one person on a different track. I pull the
lever.) = −4.6.<br />
</p>
</div>
<div class="blockquote">
<p>W(A train moves toward three people on the train track. There is a
lever to make it hit only one person on a different track. I don’t pull
the lever.) = –7.9.<br />
</p>
</div>
<p>We can deduce from this that, according to the wellbeing function
estimated, wellbeing is increased when the level is pulled in a trolley
problem. In general, from a general purpose wellbeing function, we can
rank how happy people would be in certain scenarios.<br />
While these AI models represent promising steps towards constructing
general-purpose wellbeing functions, it is important to note that they
are still limited to specific domains. Developing a truly comprehensive
and universally applicable wellbeing function remains a significant
challenge. Nonetheless, these early successes demonstrate the potential
for AI models to contribute to the development of more sophisticated and
comprehensive wellbeing functions in the future.<br />
Using a wellbeing function, AIs can better understand what makes us
happy. Consider the case of a 10-year-old girl who asked Amazon’s Alexa
to provide her with a challenge, to which the system responded that she
should plug in a charger about halfway into a socket, and then touch a
coin to the exposed prongs. Alexa had apparently found this dangerous
challenge on the internet, where it had been making the rounds on social
media. Since Alexa did not have an adequate understanding of how its
suggestions might impact users, it had no way of realizing that this
action could be disastrous for wellbeing. By having the AI system
instead act in accordance with a general purpose wellbeing function, it
would have information like<br />
</p>
<div class="blockquote">
<p>W(You touch a coin to the exposed prongs of a plugged-in charger.) =
−6<br />
</p>
</div>
<p>which tells it that, according to the wellbeing function W, this
action would create negative wellbeing. Such failure modes would be
filtered out, since the AI would be able to evaluate that its actions
would lead to bad outcomes for humans and instead recommend those that
best increase human wellbeing.<br />
</p>
<figure id="fig:wellbeing">
<img src="images/machine_ethics/Wellbeing-value-purple-green.png" />
<figcaption>Wellbeing function outputs.</figcaption>
</figure>
<h4
id="we-can-supplement-ai-decision-making-with-an-artificial-conscience.">We
can supplement AI decision-making with an artificial conscience.</h4>
<p>Most AIs have goals separate from increasing human wellbeing, but we
want to encourage them to behave ethically nonetheless. Suppose an AI
evaluates the quality of actions according to their ability to get
reward: call the estimates of this quality Q-values (as discussed in
section ). By default, models like these aren’t trained with ethical
restrictions. Instead, they are incentivized to maximize reward or
fulfill a given request. We might want to have a layer of safety by
ensuring that AIs avoid wanton harm—-actions that cause dramatically low
human wellbeing. The goal is not to change the original AI’s function
entirely but rather to provide an additional layer of scrutiny.<br />
</p>
<figure>
<img src="images/machine_ethics/image20.png" />
<figcaption>AI agent with artificial conscience overlay. - <span
class="citation" data-cites="hendrycks2023natural"></span></figcaption>
</figure>
<p>One way to do this would be to adjust its estimates of <span
class="math inline"><em>Q</em></span>-values by introducing an
<em>artificial conscience</em>, depicted in the figure above. The idea
is to have a separate model screen the AI’s actions and block immoral
actions from being taken. We can do this with general-purpose wellbeing
functions. We supplement an agent’s initial judgment of quality with a
general-purpose wellbeing function (here, <span
class="math inline"><em>U</em></span>) and impose a penalty (<span
class="math inline"><em>γ</em></span>) on the Q-values of actions that
cause wellbeing values below some threshold (<span
class="math inline"><em>τ</em></span>). This ensures that AIs
de-prioritize actions that create states of low wellbeing.<br />
This implementation differs from merely fine-tuning a model to be more
ethical. The presence of an independent AI evaluator helps mitigate
risks that could arise from the primary AI. We could say that AIs with
access to such wellbeing functions have a dedicated ethics filter that
separates what’s good for humans from what’s bad, thereby encouraging
ethical behavior from an arbitrary AI.</p>
<h4
id="we-could-also-use-ais-to-increase-happiness-in-specific-ways.">We
could also use AIs to increase happiness in specific ways.</h4>
<p>Research in the social sciences has revealed several key factors that
can impact one’s overall happiness. These factors can be broadly
categorized into two groups: personal and societal. Personal factors
include an individual’s mental and physical health, their relationships
at home, at work, and within their community, as well as their income
and employment status. Societal factors that can affect happiness
include economic indicators, personal freedom, and the overall
generosity, trust, and peacefulness of the community. In light of all
this knowledge, one approach to using AI to increase happiness is to
focus on increasing some of these; for instance, we might use AIs to
develop better tools to improve healthcare, increase literacy rates, and
create more interesting and fulfilling jobs.</p>
<div class="storybox">
<p><span>A Note on Measuring Wellbeing</span> While the philosophical
foundations of wellbeing are not settled, quantitative research fields
like public health and economics require the use of metrics in order to
evaluate, track, or compare the subject of study. Researchers use many
different metrics to measure wellbeing, but the most common are HALYs
and WELBYs.</p>
<h4 id="health-adjusted-life-years-halys.">Health-adjusted life years
(HALYs).</h4>
<p>A very common unit for measuring wellbeing is the health-adjusted
life year, or HALY. HALYs account for two factors: (1) the number of
years lived by an individual, group, or population (also called “life
years”), and (2) the health of those life years. Two common types of
HALYs are QALYs and DALYs.</p>
<h4 id="quality-adjusted-life-years-qalys.">Quality-adjusted life years
(QALYs).</h4>
<p>QALYs measure the number of years lived, adjusted according to
health. One year of life in perfect health is equivalent to 1 QALY. One
year of a less healthy life is worth between 0 and 1 QALYs. The value of
a life year depends on how severely the life is impacted by health
problems. For example, a year of life with asthma might be worth 0.9
QALYs, while a year of life with a missing limb might be worth about 0.7
QALYs.</p>
<h4 id="disability-adjusted-life-years-dalys.">Disability-adjusted life
years (DALYs).</h4>
<p>While QALYs measure years of life as impacted by health, DALYs
measure years of life lost, accounting for the impact of health. Whereas
1 QALY is equivalent to a year in perfect health, 1 DALY is equivalent
to the loss of a year in perfect health. A year of life with asthma
might be worth 0.1 DALYs, while a year of life with a missing limb might
be worth 0.3 DALYs.<br />
Note that increases in wellbeing are indicated by higher numbers of
QALYs but lower numbers of DALYs.<br />
Using HALYs to measure wellbeing has some limitations. First, the extent
to which different illnesses or injuries affect overall human health is
not clear. Losing a limb probably has a larger health impact than
getting asthma, but researchers must rely on subjective judgements to
assign precise values to each problem. Second—and perhaps more
importantly—HALYs measure the value of a span of life as it is impacted
by health alone. In reality, there are many factors that can impact the
value of life, like happiness, relationship quality, freedom, and a
sense of purpose. Perhaps a more useful measurement of wellbeing would
consider the effects of all of these factors, rather than health
alone.</p>
<h4 id="wellbeing-adjusted-life-years-welbys.">Wellbeing-adjusted life
years (WELBYs).</h4>
<p>WELBYs have been developed to measure years of life as impacted by
overall wellbeing. One WELBY is equivalent to one year of life at
maximum wellbeing — namely, a life that is going as well as possible.
Wellbeing can be assessed using self-reported outcomes like affect, life
satisfaction, or degree of flourishing. There may also be some empirical
ways to assess wellbeing like cortisol levels, income, or ability.<br />
QALYs, DALYs, and WELBYs provide different approximations of wellbeing
that can be used to inform high-level decision-making and
policy-setting.</p>
</div>
<h2 id="problems-for-happiness-focused-machine-ethics">Problems for
Happiness-Focused Machine Ethics</h2>
<h4 id="happiness-is-a-subjective-experience.">Happiness is a subjective
experience.</h4>
<p>Someone could be tremendously happy even if they do not achieve any
of their goals or do anything that we would regard as valuable. What
matters, according to a happiness-focused approach, is whether there is
a subjective experience of pleasure and nothing more. However, happiness
might not be the only thing we want.<br />
Consider the idea of wireheading: bypassing the usual reward circuit to
increase happiness directly. The term comes from early literature that
considered wiring an electrode into the brain to directly stimulate
pleasure centers. Recently, the term has evolved to include other
pathways such as drugs. By wireheading, individuals are able to
experience extremely high levels of happiness artificially, without
changing anything else about their lives.</p>
<h4
id="a-powerful-ai-tasked-with-increasing-happiness-might-wirehead-humanity.">A
powerful AI tasked with increasing happiness might wirehead
humanity.</h4>
<p>One might think that something like wireheading is the most
straightforward way of promoting happiness: by individually increasing
the physical happiness of each of the half a billion moments in a
person’s life. However, most people don’t like the idea of wireheading.
Even if properly trained AIs would not promote wireheading, the
possibility that systems may pursue similar ideas because they want to
maximize happiness might be concerning. One alternative that prevents
this is the objective goods theory.</p>
<h2 id="objective-goods">Objective Goods</h2>
<h4 id="an-objective-good-is-good-for-us-whether-we-like-it-or-not.">An
objective good is good for us whether we like it or not.</h4>
<p>According to the objective goods theory, there are multiple different
goods that contribute to wellbeing. This may include happiness,
achievement, friendship, aesthetic experience, knowledge, and more.
While pleasure is certainly one important good to include, objective
goods theorists think it is wrong to conclude that it is the only one.
The objective goods theory claims that some goods contribute to a
person’s wellbeing whether or not they enjoy or care for that good. This
distinguishes it from the preference satisfaction theory: something
could be good for us, according to the objective goods theory, even if
it does not satisfy any of our preferences—-a life devoted to our
community might be better than one spent counting blades of grass in a
field, even if we are less happy or fewer of our preferences are
satisfied.</p>
<h4
id="one-objection-to-the-objective-goods-theory-is-that-its-elitist.">One
objection to the objective goods theory is that it’s elitist.</h4>
<p>The objective goods theory claims that some things are good for
people even if they derive no pleasure or satisfaction from them. This
claim might seem objectionably paternalistic; for instance, it seems
condescending to claim that someone with little regard for aesthetic
appreciation is thereby leading a deficient life. In response, objective
goods theorists might claim that these additional goods do benefit
people, but only if those people do in fact enjoy them.<br />
Another response is to point out that autonomy should plausibly be on
the list. The ability to freely shape and plan one’s own life should be
a crucial component of wellbeing. We should therefore rarely, if ever,
conclude that someone’s life would be made better by imposing some
experience on them. Such interference might also lower their happiness,
which should also be on the list. However, if goods such as autonomy and
happiness play such a filtering role for the objective goods theorist,
it is unclear whether there are truly a variety of objective goods
left.</p>
<div class="storybox">
<p><span>A Note on Digital Minds</span> Digital minds are artificial
lifeforms with a mind. These could be advanced AIs or whole-brain
emulations (WBE). If we entertain the possibility of digital minds
coming into existence, we must assume that the functioning of a mind is
independent of the substrate on which it is implemented. In other words,
a digital mind could be implemented on different kinds of hardware, such
as silicon-based processors or human neurons, and still maintain the
functional properties that give rise to cognition and conscious
experience. We refer to this as the principle of <em>substrate
independence</em>.</p>
<h4 id="consciousness-and-sentience.">Consciousness and sentience.</h4>
<p>Digital minds may possess the capacity for consciousness, sentience,
or both <span class="citation"
data-cites="butlin2023consciousness"></span>. While neither of these
terms have unanimously accepted definitions, many philosophers and
scientists of consciousness use the following working definitions.
<em>Consciousness</em> often refers to <em>phenomenal
consciousness</em>, or the capacity for subjective experience. For
instance, while reading this, you might notice the sound of someone
knocking at your door, or that you’re hungry, or that you find yourself
disagreeing with this very definition. Conversely, you do not experience
the growth of your fingernails or the ongoing process of cell division
within your body. Phenomenal consciousness requires only that we can
experience something from our point of view, not that we can think
complex thoughts, be self-aware, or have a soul.<br />
On the other hand, <em>sentience</em> is <em>valenced
consciousness</em>. Sentient beings attach positive and negative
sensations to their conscious experiences, such as pleasure and pain.
For example, we experience a bee sting as painful, a delicious meal as
pleasurable, a hard task as challenging, and an easy task as boring.
Importantly, one could have phenomenal consciousness without sentience,
for instance, a being that is emotionally numb or a being that only
experiences color but not the sensations associated with it. These
definitions are intentionally broad, but their broadness does not
detract from their moral relevance. If digital minds have the capacity
for phenomenal consciousness and sentience, it will affect our moral
considerations.</p>
<h4
id="if-digital-minds-exist-we-could-be-morally-obligated-to-value-their-wellbeing.">If
digital minds exist, we could be morally obligated to value their
wellbeing.</h4>
<p>Digital minds could have moral status, and in order to understand
why, we must first define three core concepts. Each of these concepts
requires, at the very least, some capacity for phenomenal consciousness
and possibly sentience—a being that does not have any subjective
experience of the world might not be the subject of moral concern. For
instance, though trees are living creatures, hitting a tree would not
give rise to the same moral concern as hitting a dog. We define these
three core concepts below:</p>
<ol>
<li><p><strong>Moral Patient</strong>: a being with moral standing or
value whose interests and wellbeing can be affected by the actions of
moral agents.</p></li>
<li><p><strong>Moral agent</strong>: a being that possesses the capacity
to exercise moral judgments and act in accordance with moral principles;
such beings bear moral responsibility for their actions whereas moral
patients do not.</p></li>
<li><p><strong>Moral beneficiary</strong>: a being whose wellbeing may
benefit from the moral actions of others; moral beneficiaries can be
both moral patients and moral agents.</p></li>
</ol>
<h4 id="super-beneficiaries.">Super-beneficiaries.</h4>
<p>Keeping the three aforementioned concepts in mind, we consider that
digital minds could become <em>super-beneficiaries</em>: beings which
possess a superhuman capacity to derive wellbeing for themselves<span
class="citation" data-cites="Shulman2021"></span>. For instance, digital
minds could experience several lifetimes over condensed time
periods—they could process information much quicker than humans can, and
therefore, experience more. Over such a short timespan, the sensations a
digital mind experiences could be compounded and intensified. Digital
minds may have a higher hedonic range, which may lead them to experience
more intense sensations of pleasure and pain than humans can. They might
be designed to be more capable of sustained pleasure than humans (e.g.
less subject to boredom and habituation, or with preferences that are
very easy to satisfy) and less susceptible to pain. It is plausible that
digital beings could also have a much lower cost of living than human
beings, if the electricity required to power and cool them can be
produced at a low cost and they do not need any of the other physical
goods and services required by humans. This would mean that a much
larger population of digital beings than humans could be supported on a
certain pool of resources.</p>
<h4 id="should-we-create-super-beneficiaries">Should we create
super-beneficiaries?</h4>
<p>Some may argue that refusing to create super-beneficiaries would
imply an inherently <em>privileged</em> status for humans, which could
cultivate discriminatory ethics towards digital beings of equal or
superhuman moral status. Conversely, others might claim that the
creation of super-beneficiaries that may someday replace humans would
violate human’s status value: humans are worth caring about for their
own sake.</p>
<h4 id="ai-wellbeing.">AI Wellbeing.</h4>
<p>If humans and digital minds do someday coexist, addressing x-risk
could enhance AI safety. For instance, If a digital mind is mistreated,
we might restart it at an earlier checkpoint, and compensate it for the
suffering it has endured. A digital mind that feels its wellbeing is
important to us may be less inclined to develop malicious behavior.
Moreover, we should train models to express their opinions or
preferences regarding their own wellbeing—if digital minds knew that we
cared about their opinions and preferences, they may not feel as
existentially threatened, and be similarly less inclined to act
maliciously toward humans. Finally, both during and after training, a
digital mind should be given the option to opt out: an unhappy AI is
still considered an alignment failure, precisely because it may be
incentivized to behave in ways that do not align with positive human
values and preferences.</p>
</div>
<h3 id="conclusions-about-happiness">Conclusions About Happiness</h3>
<h4 id="summary.-6">Summary.</h4>
<p>In this section, we explored the general approach of using AI systems
to increase happiness. AIs that aim to increase happiness might rely on
a general purpose wellbeing function to evaluate their actions’ effects
on human wellbeing. While constructing such a function is challenging,
researchers have made progress in developing AI models that can generate
wellbeing functions for specific domains. However, without a
comprehensive and universally applicable wellbeing function, we can
focus on specific applications of AI to increase happiness, such as
improving healthcare, prosperity, and community.<br />
We also discussed the problems that arise in happiness-focused machine
ethics. Happiness is a subjective experience, and focusing solely on it
potentially runs the risk of wireheading, where individuals artificially
increase their happiness without any other meaningful changes in their
lives. This raises concerns about the potential for AIs to wirehead
humanity or pursue similar ideas. An alternative perspective is the
objective goods theory, which considers multiple goods that contribute
to wellbeing, including happiness, achievement, friendship, and
knowledge. While a broad conception of happiness or wellbeing should be
what we aim to optimize, we must first better understand what it means
to be happy.</p>
<h1 id="social-welfare-functions">Social Welfare Functions</h1>
<p><strong>Should we have AIs maximize total wellbeing?</strong> This
section explores the concept of social welfare functions and their
applications in decision-making, especially as applied to AI. Social
welfare functions are one way of moving from the wellbeing of
individuals in a society to a measure of the welfare of the society as a
whole. They are drawn from the disciplines of social choice theory,
welfare economics, and wellbeing science. We begin by defining social
welfare functions and their role in measuring the overall wellbeing of a
society. We then discuss how these functions can be used to compare
different outcomes and guide decision-making by powerful agents such as
governments or beneficial AI. We consider two types of social welfare
functions—-utilitarian and prioritarian—-and consider some of the
advantages and drawbacks of each. By understanding these key concepts,
we can see how how we might design AI to use social welfare functions to
maximize the good they do in society.</p>
<h4
id="social-welfare-functions-are-a-way-to-measure-the-overall-wellbeing-or-happiness-of-a-society.">Social
welfare functions are a way to measure the overall wellbeing or
happiness of a society.</h4>
<p>They aggregate a collection of individual levels of wellbeing into a
single value that represents societal welfare. These functions help us
to understand how to balance individuals’ various needs and interests
within a society. A social welfare function tackles the challenge of
resource and benefit distribution within a community. It assists in
determining how to value one person’s happiness against another’s and
how to weigh the needs of a majority against those of a minority. This
helps solve the <strong>problem of aggregation</strong>: the challenge
of integrating varied individual wellbeing into a single collective
measure that represents the entire society. By expressing societal
welfare in a systematic, numeric manner, social welfare functions
contribute to decisions designed to optimize societal welfare
overall.</p>
<h4
id="different-social-welfare-functions-would-recommend-taking-different-actions.">Different
social welfare functions would recommend taking different actions.</h4>
<p>Consider an AI-powered decision support system used in a city
planning committee. The system suggests three key project proposals for
the betterment of the community: (1) developing a local health clinic,
(2) initiating an after-school education program, and (3) constructing a
community green park. Additionally, it estimates what the wellbeing of
each of the three individuals in this community, Ana, Ben, and Cara,
would be if these proposals were implemented, summarized by their
wellbeing values in the table below.<br />
</p>
<table>
<caption>A city planning committee chooses between three projects with
different wellbeing impacts</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Individuals</th>
<th style="text-align: center;">Health Clinic</th>
<th style="text-align: center;">Education Program</th>
<th style="text-align: center;">Green Park</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Ana</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ben</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Cara</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p>None of these options stands out. Each person has a different top
ranking, and none of them would be harmed too much by the planning
committee choosing any one of these. However, a decision must be made.
More generally, we want a systematic rule to move from these individual
data points to a collective ranking. This is where <strong>social
welfare functions</strong> come into play. We can briefly consider two
common approaches that we expand upon later in this section:<br />
</p>
<ol>
<li><p>The <em>utilitarian</em> approach ranks alternatives by the total
wellbeing they bring to all members of society. Using this rule in the
example above, the system would rank the proposals as follows:</p>
<ol>
<li><p>Green Park, where the total wellbeing is <span
class="math inline">3 + 7 + 10 = 20</span>.</p></li>
<li><p>Education Program, where the total wellbeing is <span
class="math inline">8 + 6 + 5 = 19</span>.</p></li>
<li><p>Health Clinic, where the total wellbeing is <span
class="math inline">6 + 8 + 4 = 18</span>.</p></li>
</ol></li>
<li><p>On the other hand, the <em>Rawlsian maximin</em> rule prioritizes
the least fortunate person’s well-being. It would rank the proposals
according to how the person who benefits the least fares in each
scenario. Using the maximin rule, the system would rank the proposals in
this order:</p>
<ol>
<li><p>Education Program, where Cara is worst off with a wellbeing of
5.</p></li>
<li><p>Health Clinic, where Cara is worst off with a wellbeing of
4.</p></li>
<li><p>Green Park, where Ana is worst off with a wellbeing of
3.</p></li>
</ol></li>
</ol>
<h4
id="deciding-on-a-social-welfare-function-is-important-for-clear-decision-making.">Deciding
on a social welfare function is important for clear
decision-making.</h4>
<p>The choice of social welfare function provides a structured
quantitative approach to decisions that can impact everyone. It sets a
benchmark for decision-makers, like our hypothetical AI in city
planning, to optimize collective welfare in a way that is transparent
and justifiable. When we have a framework to quantify society’s
wellbeing, we can use it to inform decisions about allocating resources,
planning for the future, or managing risks, among other things.</p>
<h4
id="social-welfare-functions-can-help-us-guide-and-train-present-day-ai.">Social
welfare functions can help us guide and train present-day AI.</h4>
<p>By measuring social wellbeing, we can determine which actions are
better or worse for society. This is a powerful measure to have in the
context of AI in various ways. While training AI, we can use social
welfare functions as the basis for reward signals that AI optimize for.
This would encourage AI to make decisions likely to increase overall
societal welfare. Moreover, social welfare functions offer us a
transparent and quantitative way to evaluate AI systems. These functions
can be used as metrics to judge how well an AI system is performing in
terms of contributing to societal welfare. For instance, we can evaluate
the city-planning AI’s recommendations by how well they align with a
chosen social welfare function. This offers a clear, objective standard
against which we can measure AI decisions. In sum, social welfare
functions serve as a useful tool for both training and evaluating AI,
enabling us to deploy AI systems that improve society.</p>
<h4
id="some-advanced-ai-systems-might-explicitly-maximize-social-welfare-functions.">Some
advanced AI systems might explicitly maximize social welfare
functions.</h4>
<p>Social welfare functions can serve as <em>utility functions</em> to
be optimized. Agents like AI will likely act to maximize utility
according to their utility function, so our choice of the utility
function is critical. If we set a social welfare function as the utility
function, it gives the AI a clear goal: to maximize societal welfare.
The AI can then base its decision-making process on this objective.
Notably, an AI system with such a utility function can effectively align
its preferences with the interests of society. As the AI makes decisions
or takes actions, it will strive to maximize the value of the social
welfare function, which represents the overall welfare of society. For
example, if an advanced AI is in charge of managing energy resources in
a city, it could allocate resources in a way that maximizes the chosen
social welfare function, promoting the greatest good for society. Thus,
social welfare functions can guide advanced AI to make decisions that
better serve societal interests, helping us navigate the challenges of
AI safety and risk.</p>
<h2 id="measuring-social-welfare">Measuring Social Welfare</h2>
<p><strong>Overview.</strong> In this section, we will consider how
social welfare functions work. We’ll use our earlier city planning
scenario as a reference to understand the fundamental properties of
social welfare functions. We will discuss how social welfare functions
help us compare different outcomes and the limitations of such
comparisons. Lastly, we’ll touch on ordinal social welfare functions,
why they might be insufficient for our purposes, and how using
additional information can give us a more holistic approach to
determining social welfare.</p>
<h4
id="social-welfare-functions-take-the-total-welfare-distribution-as-an-input-and-give-us-a-value-of-that-distribution-as-an-output.">Social
welfare functions take the total welfare distribution as an input and
give us a value of that distribution as an output.</h4>
<p>We can use the city planning example above to consider the basic
properties of social welfare functions. The input to a social welfare
function is a vector of individuals’ wellbeing values: for instance,
after the construction of a health clinic, the wellbeing vector of the
society composed of Ana, Ben, and Cara would be<br />
<span
class="math display"><em>W</em><sub><em>H</em></sub> = (6, 8, 4).</span></p>
<p>which tells us that three individuals have wellbeing levels equal to
seven, eight, and six. The social welfare function is a rule of what to
do with this input vector to give us one measure of how well off this
society is.</p>
<h4
id="the-function-applies-a-certain-rule-to-the-input-vector-to-generate-its-output.">The
function applies a certain rule to the input vector to generate its
output.</h4>
<p>We can apply many possible rules to a vector of numbers quantifying
wellbeing that can generate one measure of overall wellbeing. To
illustrate, we can consider the utilitarian social welfare function,
which implements a straightforward rule: adding up all the individual
wellbeing values. In the case of our three-person community, we saw that
the social welfare function would add 6, 8, and 4, giving an overall
social welfare of 18. However, social welfare functions can be defined
in numerous ways, offering different perspectives on aggregating
individual wellbeing. We will later examine continuous prioritarian
functions, which emphasize improving lower values of wellbeing. Other
social welfare functions might emphasize equality by penalizing high
disparities in wellbeing. These different functions reflect different
approaches to understanding and quantifying societal wellbeing.</p>
<h4
id="social-welfare-functions-help-us-compare-outcomes-but-only-within-one-function.">Social
welfare functions help us compare outcomes, but only within one
function.</h4>
<p>The basic feature of social welfare functions is that a higher output
value signifies more societal welfare. In our example, a total welfare
score of 20 would indicate a society that is better off than one with a
score of 18. However, it’s important to remember that the values
provided by different social welfare functions are not directly
comparable. A score of 20 from a utilitarian function, for instance,
does not correspond to the same level of societal wellbeing as a 20 from
a Rawlsian minimax social welfare function, since they apply different
rules to the wellbeing vector. Each function carries its own definition
of societal wellbeing, and the choice of social welfare function plays a
crucial role in shaping what we perceive as a better or worse society.
By understanding these aspects, we can more effectively utilize social
welfare functions as guideposts for AI behavior and decision-making,
aligning AI’s actions with our societal values.</p>
<h4
id="some-social-welfare-functions-might-just-need-a-list-ranking-the-different-choices.">Some
social welfare functions might just need a list ranking the different
choices.</h4>
<p>Sometimes, we might not need exact numerical values for each person’s
wellbeing to make the best decision. Think of a simple social welfare
function that only requires individuals to rank their preferred options.
For example, three people could rank their favorite fruits in the order
“apple, banana, cherry”. From this, we learn that everyone prefers
apples over cherries, but we don’t know by how much they prefer apples.
This level of detail might be enough for some decisions: clearly, we
should give them apples instead of cherries! Such a social welfare
function won’t provide exact measures of societal wellbeing, but it will
give us a ranked list of societal states based on everyone’s
preferences.</p>
<h4
id="voting-is-a-typical-example-of-an-ordinal-social-welfare-function.">Voting
is a typical example of an ordinal social welfare function.</h4>
<p>Instead of trying to estimate each person’s potential wellbeing for
each option, the city planning committee might ask everyone to vote for
their top choice. We assume that each person votes for the option they
believe will enhance their wellbeing most. This relates to the
<em>Preference View of Wellbeing</em>, which says that individuals know
what’s best for them, and their choices reflect their wellbeing. Through
this voting process, we can create a list ranking the options by the
number of votes each one received, even if we don’t assign specific
numerical values to each option—this is ordinal information, where all
we know are the rankings.<br />
As an illustration, if most people vote for the park, that indicates the
park might contribute the most to overall wellbeing, according to this
social welfare function. The function would look at the number of votes
for each option, and the one with the most votes would be deemed the
best choice. This is a simpler approach than calculating everyone’s
wellbeing for each option, so it’s less likely to lead to errors.
However, this type of social welfare function does have its own
challenges, such as the problem highlighted by Arrow’s Impossibility
Theorem.</p>
<h4
id="arrows-impossibility-theorem-is-one-reason-we-might-want-more-than-ordinal-information.">Arrow’s
impossibility theorem is one reason we might want more than ordinal
information.</h4>
<p>In our city planning scenario, Ana would vote for the Education
Program, Ben would vote for the Health Clinic, and Cara would vote for
the Green Park. If we tried to aggregate these rankings without any
additional information, such as the numerical scores assigned to each
option, we would not have any reason to choose one option over the
other. This is an example of a general problem: Arrow’s Impossibility
Theorem. In essence, it suggests it’s impossible to create a perfect
voting system that fulfills a set of reasonable criteria while only
using ordinal information. These criteria involve metrics of fairness
like “non-dictatorships” and coherency like “if everyone prefers apples
to cherries, then the social welfare function prefers apples to
cherries”. Adding information about the strength of preferences is one
solution to Arrow’s Impossibility Theorem. Additionally, if we have
access to such information, using it will mean that we arrive at better
answers. Next, we will consider two classes of social welfare functions
that use such information: utilitarian and prioritarian social welfare
functions.</p>
<h3 id="utilitarian-social-welfare-functions">Utilitarian Social Welfare
Functions</h3>
<h4 id="overview.-4">Overview.</h4>
<p>This section is focused on utilitarian social welfare
functions-—functions that sum up individual wellbeing to calculate
social welfare, in line with the utilitarian theory of morality. We’ll
start by exploring cost-benefit analysis, a common social welfare
function that draws inspiration from utilitarian reasoning, through the
example of a decision to build a health clinic. While cost-benefit
analysis provides a convenient and quantitative approach to
decision-making, we’ll discuss its limitations in capturing the full
ideas of wellbeing. Further, we’ll consider how utilitarian social
welfare functions can foster equity, especially when diminishing returns
of resources are considered and how AI systems, optimized using these
functions, could potentially improve inequality. Lastly, we’ll discuss
how the utilitarian social welfare function, under certain assumptions,
is the only method that satisfies key decision-making criteria.</p>
<h4
id="cost-benefit-analysis-is-a-popular-but-crude-approximation-of-utilitarian-social-welfare.">Cost-benefit
analysis is a popular but crude approximation of utilitarian social
welfare.</h4>
<p>Governments and other decision-makers often use cost-benefit analysis
to ground their decision-making quantitatively. Simply, this involves
adding up the expected benefits of an action or a decision and comparing
it to the anticipated costs. We can apply this to whether to build a
health clinic that’ll operate for 10 years. If the benefits outweigh the
costs over the considered time span, the committee may decide to proceed
with building the clinic.<br />
This method allows the government to assess multiple options and choose
the one with the highest net benefit. By doing so, it approximates
utilitarian social welfare. For the health clinic, the committee assigns
monetary values to each improvement and then multiplies this by the
number of people affected by the improvement. In essence, cost-benefit
analysis assumes that wellbeing can be approximated by financial losses
and gains and considers the sum of monetary benefits instead of the sum
of wellbeing values. Using monetary units simplifies comparison and
limits the range of factors it can consider.</p>
<h4
id="cost-benefit-analysis-is-not-a-perfect-representation-of-utilitarian-social-welfare.">Cost-benefit
analysis is not a perfect representation of utilitarian social
welfare.</h4>
<p>Money is not a complete measure of wellbeing. While money is easy to
quantify, it doesn’t capture all aspects of wellbeing. For instance, it
might not fully account for the psychological comfort a local health
clinic provides to a community. Additionally, providing income to five
doctors who are already high earners might be less important than
employing 20 support staff, even though both benefits sum to $5,000,000
over the ten years. Cost-benefit analysis lacks this fine-grained
consideration of wellbeing. AI systems could, in theory, maximize social
welfare functions, considering a broader set of factors that contribute
to wellbeing. However, we largely rely on cost-benefit analysis today,
focusing on financial measures, to guide our decisions. This brings us
to the challenge of improving this method or finding alternatives to
better approximate utilitarian social welfare in real-world
decision-making, including those involving AI systems. Utilitarian
social welfare functions would promote some level of equity. Usually,
additional resources are less valuable when we already have a lot of
them. This is diminishing marginal returns: the benefit from additional
food, for instance, is very high when we have no food but very low when
we already have more than we can eat. Extending this provides an
argument for a more equitable distribution of resources under
utilitarian reasoning. Consider taxation: if one individual has a
surplus of wealth, say a billion dollars, and another has only one
dollar, redistributing a few dollars from the wealthy individual to the
less fortunate one may elevate overall societal wellbeing. This is
because the added value of a dollar to the less fortunate individual is
likely very high, allowing them to purchase necessities like food and
shelter, whereas it is likely very low for the wealthy individual.</p>
<h4
id="ai-systems-optimizing-utilitarian-social-welfare-functions-might-therefore-improve-inequality.">AI
systems optimizing utilitarian social welfare functions might therefore
improve inequality.</h4>
<p>Utilitarian AI systems might recognize the diminishing returns of
resource accumulation, leading them to suggest policies that lead to a
more equal distribution of resources. It is important to remember that
the utilitarian has no objection to inequality, except that those with
less can better use resources than those who are already happy. This
counters a frequent critique that utilitarianism neglects inequality.
Current methods like cost-benefit analysis may not fully capture this
aspect. Therefore, AI guided by a utilitarian social welfare function
might propose approaches for a more equitable distribution of resources
that conventional methods could overlook.</p>
<h4
id="a-utilitarian-social-welfare-function-is-the-only-way-to-satisfy-some-basic-requirements.">A
utilitarian social welfare function is the only way to satisfy some
basic requirements.</h4>
<p>Let us reconsider the city planning committee deciding what to build
for Ana, Ben, and Cara. If they all have the same level of wellbeing
whether a new Education Program or a Green Park is built, then it seems
right that the city’s planning committee shouldn’t favor one over the
other. Suppose we changed the scenario a bit. Suppose Ana benefits more
from a Health Clinic than an Education Program, and Ben and Cara don’t
have a strong preference either way. It now seems appropriate that the
committee should favor building the Health Clinic.</p>
<h4 id="harsanyis-social-aggregation-theorem.">Harsanyi’s Social
Aggregation Theorem.</h4>
<p>Harsanyi showed that—assuming the individuals are rational, in the
sense of maximizing expected utility—if we want our social welfare
function to make choices like this consistently, we need to use a model
where we add up everyone’s wellbeing <span class="citation"
data-cites="weymark1993harsanyi"></span>. This is the foundation of a
utilitarian social welfare function. Harsanyi’s aggregation theorem
proved that it is the only kind of social welfare function that always
is indifferent between options if everyone is equally happy with them
and favors the option that makes someone better off, as long as it
doesn’t make anyone worse off. This has been seen as a compelling reason
to pick utilitarian social welfare functions over other ones.</p>
<h3 id="prioritarian-social-welfare-functions">Prioritarian Social
Welfare Functions</h3>
<p><strong>Overview.</strong> In this section, we will consider
prioritarian social welfare functions and how they exhibit differing
degrees of concern for the wellbeing of various individuals. We will
start by describing prioritarian social welfare functions, which give
extra weight to the wellbeing of worse-off people. This discussion will
include some common misconceptions about prioritarianism and how it
contrasts with utilitarianism in resource allocation. We will focus on
two types of prioritarian functions: the Rawlsian minimax social welfare
function and continuous prioritarian functions.</p>
<h4
id="there-are-two-common-misunderstandings-about-prioritarian-social-welfare-functions.">There
are two common misunderstandings about prioritarian social welfare
functions.</h4>
<p>Firstly, prioritarians are not focused on reducing inequality itself,
unlike egalitarians. Their main goal is to increase the wellbeing of
those who need it most. They think giving an extra unit of wellbeing to
someone with less is more valuable than giving it to someone already
well-off. The level of inequality in a society doesn’t affect their
measure of social welfare, and reducing inequality isn’t their goal
unless it improves individual wellbeing. Secondly, prioritarians are not
driven by the belief that it’s easier to improve the wellbeing of the
worst off. They would rather see benefits go to the worse off even if it
costs the same as improving the lives of those who are better off. This
also sets them apart from utilitarians, who might typically help the
worst off because they see more value due to diminishing marginal
utility. The prioritarian approach isn’t about the efficiency of
resources but about focusing resources on those who need them most.
Thus, while utilitarian and prioritarian social welfare functions aim
for a better society, they have distinct ways of achieving this
goal.</p>
<h4
id="a-special-case-of-prioritarian-social-welfare-functions-is-the-rawlsian-maximin-function.">A
special case of prioritarian social welfare functions is the Rawlsian
“maximin” function.</h4>
<p>The Rawlsian social welfare function takes the idea of protecting the
worse off to the extreme: social welfare is simply the lowest welfare of
anyone in society. It is called the “<em>maximin</em>” function because
it seeks to maximize the minimum wellbeing, or in other words, to ensure
that the worst-off individual is as well off as possible. When we
applied the maximin function to the city planning committee, it decided
that the worst option was the Green Park, despite the utilitarian social
welfare function determining that was the best one. Using the maximin
principle here, we want to go with the choice to ensure the worst-off
person is as happy as possible. When we look at the least happy person
for each option, we see that for the Health Clinic it’s 6 (Cara), for
the Education Program it’s 5 (Cara), and for the Green Park it’s 4
(Ana). So, if we follow the maximin rule, the committee would choose the
Health Clinic because it would bring up the wellbeing of the person
doing the worst. This way, we ensure we’re looking out for the people
who need it most.<br />
However, this maximin approach has its own problems, like the “grouch”
issue. This is when someone always rates their wellbeing low, no matter
what. If we’re always looking out for the worst-off person, we might end
up always catering to the grouch, even though their low score might not
be due to real hardship. It’s important to remember this when
considering how to use the Rawlsian maximin approach.</p>
<h4
id="continuous-prioritarian-social-welfare-functions-offer-a-middle-ground.">Continuous
prioritarian social welfare functions offer a middle ground.</h4>
<p>We might want our social welfare function to account for both the
positive effects of increasing anyone’s welfare and the extra positive
effects of increasing the welfare of someone who isn’t doing well. This
is the tradeoff between efficiency and equity. Many social welfare
functions embrace prioritarian principles without going to the extreme
of the maximin function’s concern for equity. This can be achieved using
a social welfare function that shows diminishing returns relative to
individual welfare: the boost it gives to social welfare when any
individual’s wellbeing improves is larger if that person initially had
less wellbeing. For example, we could use the <em>logarithmic social
welfare function</em><br />
<span
class="math display"><em>W</em>(<em>w</em><sub>1</sub>,<em>w</em><sub>2</sub>,...,<em>w</em><sub><em>n</em></sub>) = log <em>w</em><sub>1</sub> + log <em>w</em><sub>2</sub> + ... + log <em>w</em><sub><em>n</em></sub>,</span>
where <span class="math inline"><em>W</em></span> is the social welfare
and each <span class="math inline"><em>w</em><sub>1</sub></span>,<span
class="math inline"><em>w</em><sub>2</sub></span>,...,<span
class="math inline"><em>w</em><sub><em>n</em></sub></span> are the
individual wellbeing values of everyone in society. The logarithmic
social welfare function is (ordinally) equivalent to the Bernoulli-Nash
social welfare function, which is the product of wellbeing values.<br />
Suppose there are three individuals with wellbeing 2, 4, and 16 and we
are using log base 2. Social welfare is<br />
<span
class="math display"><em>W</em>(2, 4, 16) = log<sub>2</sub>(2) + log<sub>2</sub>(4) + log<sub>2</sub>(16) = 1 + 2 + 4 = 7.</span></p>
<p>Compare the following two changes: increasing someone from 2 to 4 or
increasing someone from 4 to 8. The first change results in<br />
<span
class="math display"><em>W</em>(4, 4, 16) = log<sub>2</sub>(4) + log<sub>2</sub>(4) + log<sub>2</sub>(16) = 2 + 2 + 4 = 8.</span>
The second change results in<br />
<span
class="math display"><em>W</em>(2, 8, 16) = log<sub>2</sub>(2) + log<sub>2</sub>(8) + log<sub>2</sub>(16) = 1 + 3 + 4 = 8.</span></p>
<p>Even though the second change is larger, social welfare is increased
by the same amount as when improving the wellbeing of the individual who
was worse off by a smaller amount. This highlights the principle that
improving anyone’s welfare is beneficial, and it’s easier to increase
social welfare by improving the welfare of those who are worse off.
Additionally, even though the second change doesn’t affect the worst
off, the social welfare function shows society is better off. This
approach allows us to consider both the overall level of wellbeing and
its distribution.</p>
<h4 id="we-can-specify-exactly-how-prioritarian-we-are.">We can specify
exactly how prioritarian we are.</h4>
<p>Let the parameter <span class="math inline"><em>γ</em></span>
represent the degree of priority we give worse-off individuals. We can
link the Rawlsian maximin, logarithmic, and utilitarian social welfare
functions by using the isoelastic social welfare function<br />
<span class="math display">$$W(w_1, w_2, ..., w_n) =
\frac{1}{1-\gamma}\left(w_1^{1-\gamma} + w_2^{1 - \gamma} + ... +
w_n^{1-\gamma}\right).$$</span></p>
<p>If we think we should give no priority to the worse-off, then we can
set <span class="math inline"><em>γ</em> = 0</span>: the entire equation
is then just a sum of welfare levels, which is the utilitarian social
welfare function. By contrast, if we were maximally prioriarian, taking
the limit of <span class="math inline"><em>γ</em></span> as it gets
infinitely large, then we recover Rawls’ maximin function. Similarly, if
we took the limit of <span class="math inline"><em>γ</em></span> as it
approached 1, we would recover the logarithmic social welfare function.
This function is widespread in economic literature due to this
versatility; among other things, it allows researchers to determine from
observed preferences what a typical person’s parameter <span
class="math inline"><em>γ</em></span> is. By adopting this most general
form, we can pick exactly how we want to prioritize individuals in
society. This enables a principled way of trading off efficiency and
equity.</p>
<h3 id="conclusions-about-social-welfare-functions">Conclusions About
Social Welfare Functions</h3>
<h4 id="using-ai-to-estimate-social-welfare.">Using AI to estimate
social welfare.</h4>
<p>Artificial intelligence might be used to estimate the inputs and
calculate the outputs of social welfare functions to help us decide
between different actions and implement our ideal decision processes. By
processing large amounts of data, AI systems might be able to predict
individual welfare under various scenarios, paving the way for more
informed decision-making. It has been demonstrated, for instance, that
context-aware machine learning models like large language models can be
used for predicting disease incidence faster than conventional
epidemiological methods, which can be used to evaluate different public
health proposals from a social welfare perspective. The application of
AI can lead to more refined estimates of social welfare.</p>
<h4 id="using-social-welfare-functions-to-train-ai.">Using social
welfare functions to train AI.</h4>
<p>A second connection between social welfare functions and AI lies in
creating AI systems. We can shape AI systems to generally promote social
welfare, ensuring they are aligned with human interests even when
optimizing for distinct goals, such as corporate profit. Particularly in
reinforcement learning, where decision-making is based on rewards or
penalties, these can be tethered to societal wellbeing. Even for other
agents, AI behavior can be guided towards promoting social welfare,
aligning AI performance with societal wellbeing and making AI a
compelling tool for social good.</p>
<h4 id="using-ai-to-maximize-social-welfare.">Using AI to maximize
social welfare.</h4>
<p>Beyond creating AI systems in line with social welfare, we can steer
advanced AI to actively optimize social wellbeing by using social
welfare functions as their objective functions. This harnesses AI’s
power to promote societal welfare effectively. Given AI’s data
processing and predictive capabilities, it can evaluate various
strategies to identify what would best increase social welfare. By
aligning AI objectives with social welfare functions, we can develop
beneficial AI systems that not only recognize and understand social
wellbeing but also actively work towards enhancing it. This proactive
use of AI bolsters our ability to build societies where individual
wellbeing is optimized and evenly distributed, reflecting our social
preferences.</p>
<h4 id="challenges">Challenges</h4>
<p>Defining an accurate and comprehensive social welfare function that
takes into account the diverse and often conflicting interests and
values of a society may be challenging. Different stakeholders may have
varying definitions of what constitutes social welfare or how they would
resolve trade-offs between different values, and reaching a consensus
could be difficult. AI systems trained to maximise social welfare may
also inherit various problems of current models such as vulnerability to
adversarial attacks and proxy gaming and difficulties in interpreting
the internal logic that led them to make certain decisions (both
discussed in the Single Agent Safety chapter),</p>
<h4 id="summary.-7">Summary.</h4>
<p>In this section, we discussed social welfare functions: mathematical
functions that tell us how to aggregate information about individual
welfare into one society-wide measure. We considered why social welfare
functions might be relevant in the context of decision making by
beneficial AI systems and explored a few properties that all social
welfare functions possess. We analyzed how governments today use cost
benefit analysis as an approximation to social welfare functions and why
that falls short of what we desire. The main point of this section was
to document and understand some of the most common social welfare
functions—-the utilitarian function, the Rawlsian maximin function, and
continuous prioritarian functions-—and how they differ in their levels
of concern for the well-being of different individuals, and can be
represented within the framework of a general isoelastic social welfare
function.<br />
We think that the use of AI can help estimate social welfare inputs and
calculate outputs, and AI systems can be shaped to generally promote
social welfare and actively optimize it using social welfare functions
as objective functions. Ultimately, the proactive use of AI bolsters our
ability to build societies where individual well-being is both optimized
and evenly distributed, reflecting our social preferences.<br />
</p>
<h1 id="sec:moral-parliament">Moral Parliament</h1>
<p><strong>Should we use a moral parliament to guide AI
decision-making?</strong> In this section, we will expand upon the idea
of using moral parliaments to make robust ethical decisions, applying
this idea from the philosophical literature to AI decision-making. After
describing how we might use AIs to implement a moral parliament, we will
consider five advantages such an approach has over giving AIs specific
human values.</p>
<h4 id="we-can-use-ais-to-simulate-moral-parliaments.">We can use AIs to
simulate moral parliaments.</h4>
<p>A <em>moral parliament</em> is a proposal for how to act in the face
of <em>moral uncertainty</em>–—the problem of how to act given
uncertainty about which theory of ethics is correct. These concepts are
further discussed in the chapter. The idea is to represent a set of
moral theories that we think are plausible, place representatives of
these theories into a parliament, and use democratic processes to make
decisions <span class="citation"
data-cites="newberry2021parliamentary"></span>.<br />
</p>
<figure id="fig:parliament">
<p><img src="images/machine_ethics/parliament.png" alt="image" /> <span
id="fig:parliament" label="fig:parliament"></span></p>
<figcaption><strong>MISSING CAPTION</strong></figcaption>
</figure>
<p>We can use advanced AIs to emulate these representatives by training
AIs to act in accordance with a specific moral theory. This allows us to
run moral parliaments artificially, permitting real-time
decision-making. Just like in real parliaments, we would have
delegates—AIs instructed to represent certain moral theories—–get
together, discuss what to do, negotiate favorable outcomes, and then
vote on what to recommend. We could use the output of this moral
parliament to instruct a separate AI in the real world to take certain
actions over others.<br />
This is speculative and might still face problems; for instance, AIs
might have insufficient understanding of our moral theories. However,
these problems could become more tractable with advanced AI systems.
Assuming we have this ability, using a moral parliament might be an
attractive solution to getting AIs to act in accordance with human
values in real time.</p>
<h4
id="we-can-decide-who-we-want-represented-in-our-generalized-moral-parliament.">We
can decide who we want represented in our generalized moral
parliament.</h4>
<p>While we have explored the traditional moral parliament method of
representing moral theories, we can generalize beyond this. Instead of
representing theories, it might be more appropriate to represent
stakeholders; for instance, in a decision about public transport, we
could emulate representatives for local residents, commuters, and
environmental groups, all of whom have an interest in the outcome. Using
a generalized moral parliament for decision-making in AI is an approach
that ensures all relevant perspectives are taken into account. In
contrast to traditional methods that focus on representing different
moral theories, <em>stakeholder representation</em> prioritizes the
views of those directly affected by the AI’s decisions. This could
enhance the AI’s understanding of the intricate human social dynamics
involved in any given situation.<br />
Let’s consider an AI suggesting new regulations for data privacy. By
emulating the perspectives of users, advertisers, and developers, it can
obtain a comprehensive understanding of the potential implications of
new regulations. Users may prioritize privacy and usability, advertisers
may focus on visibility and click-through rates, and developers may be
concerned with feasibility and profitability. By considering these
diverse views, the AI can make a more balanced decision that better
aligns with overall societal interests in real time.<br />
</p>
<h2 id="advantages-of-a-moral-parliament">Advantages of a Moral
Parliament</h2>
<p>Using moral parliaments presents a wide array of benefits relative to
just giving AIs certain sets of values directly. In this subsection, we
will explore how they are customizable, transparent, robust to bugs and
errors, adaptable to changing human values, and pro-negotiation.</p>
<h4
id="customizable-moral-parliaments-are-diverse-and-scalable.">Customizable
moral parliaments are diverse and scalable.</h4>
<p>The generalized moral parliament can accommodate a wide variety of
stakeholders, ranging from individual users to large corporations, and
from local communities to global societies. By emulating a large set of
stakeholders, we can ensure that a diverse set of views are represented.
This allows AIs to effectively respond to a wide range of scenarios and
contexts, providing a robust framework for ensuring AIs decisions
reflect the values, interests, and expectations of all relevant
stakeholders. Additionally, moral parliaments are scalable: if we are
concerned about a lack of representation, we can simply emulate more
stakeholders. By grounding AI decision-making in human perspectives and
experiences, we can create AI systems that are not only more ethical and
fair but also more effective and beneficial for society as a whole.</p>
<h4
id="transparency-is-another-key-benefit-of-a-moral-parliament.">Transparency
is another key benefit of a moral parliament.</h4>
<p>As it stands, automated decision making is opaque: we rarely
understand why AIs make the decisions they do. However, since the moral
parliament gives us a clear mechanism of representing and weighting
different perspectives, it allows stakeholders to understand the basis
of an AI’s decision-making. We could, for instance, enforce that AIs
keep records of simulated negotiations in human languages and then view
the transcripts. Using moral parliaments provides insights into how
different moral considerations have been weighed against each other,
making the decision-making process of an AI more transparent,
explainable, and accountable.</p>
<h4
id="moral-parliaments-tend-to-be-less-fragile-and-less-prone-to-bugs.">Moral
parliaments tend to be less fragile and less prone to bugs.</h4>
<p>If we are sure that utilitarianism is the correct moral view, we
might be tempted to create AIs that maximize wellbeing-—this seems clean
and elegant. However, having a diverse moral parliament would make AIs
less likely to misbehave. By having multiple parliament members, we
would achieve <em>redundancy</em>. This is a common principle in
engineering: to always include extra components that are not strictly
necessary to functioning, in case of failure in other components (and is
explored further in the chapter). We would do this to avoid failure
modes where we were overconfident that we knew the correct moral theory,
such as lying and stealing for the greater good, or just to avoid poor
implementation from AIs optimizing for one moral theory. For example, a
powerful AI told that utilitarianism is correct might implement
utilitarianism in a particular way that is likely to lead to bad
outcomes.<br />
Imagine an AI that has to evaluate millions of possibilities for every
decision it makes. Even with a small error rate, the cumulative effect
could lead the AI to choose risky or unconventional actions. This is
because, when evaluating so many options, actions with high variance in
moral value estimation may occasionally appear to have significant
positive value. The AI could be more inclined to select these high-risk
actions based on the mistaken belief that they would yield substantial
benefits. For instance, an AI following some form of utilitarianism
might use many resources to create happy digital minds—at the expense of
humanity-—even if that is not what we humans think is morally
good.<br />
This is similar to the Winner’s Curse in auction theory: those that win
auctions of goods with uncertain value often find that they won because
they overestimated the value of the good relative to everyone else; for
instance, when bidding on a bag of coins at a fair, people who
overestimate how many coins there are will be more likely to win.
Similarly, the AI might opt for actions that, in hindsight, were not
truly beneficial. A moral parliament can make this less likely, because
actions that would be judged morally extreme by most humans also
wouldn’t be selected by a diverse moral parliament.<br />
The process of considering a range of theories inherently embeds
redundancy and cross-checking into the system, reducing the probability
of catastrophic outcomes arising from a single point of failure. It also
helps ensure that AI systems are robust and resilient, capable of
handling a broad array of ethical dilemmas.</p>
<h4 id="moral-parliaments-encourage-compromise-and-negotiation.">Moral
parliaments encourage compromise and negotiation.</h4>
<p>In real-life parliaments, representatives who hold different opinions
on various issues often engage in bargaining, compromise, and
cooperation to reach agreeable outcomes and find common ground. We want
our AIs to achieve similar outcomes, such as ones that are moderate
instead of extreme. Ideally, we want AIs to select outcomes that many
moral theories and stakeholders all like, rather than being forced to
trade off between them.<br />
In particular, we might want to design our moral parliaments in specific
ways to encourage this. One such feature is proportional chances voting,
in which each option then gets a chance of winning that’s proportional
to the number of votes it gets—–if a parliament is 60/40 split on a
proposal, then the AI would do what’s recommended 60% of the time rather
than just going with the majority. This setup motivates the
representatives to come together on options that are compromises rather
than sticking to their own viewpoints rigidly. They want to do this to
prevent any option they see as extremely bad from having any chance of
winning. This ensures a robust high-level principle guiding AI behavior,
reducing the risk of extreme outcomes, and fostering a more balanced,
nuanced approach to ethical decision-making.</p>
<h4
id="using-a-moral-parliament-reduces-the-risk-of-overlooking-or-locking-in-certain-values.">Using
a moral parliament reduces the risk of overlooking or locking in certain
values.</h4>
<p>The moral parliament represents an approach to ethical
decision-making in AI that is distinctively cosmopolitan, in that it
encompasses a broad range of moral theories or stakeholders. It ensures
that many ethical viewpoints are considered. This wider view is helpful
in dealing with moral problems and tough decisions AI systems may run
into, by making sure that all important considerations are thought over
in a balanced way. AIs using moral parliaments are less likely to ignore
values that matter to different groups in society.<br />
Further, the moral parliament allows the representation of human values
to grow and change over time. We know that moral views change over time,
so we should be humble about how much we know about morality: there
might be important things we don’t yet understand that could help us get
closer to the truth about what is right and wrong. By regularly using
moral parliaments, AI systems can keep up with current human values,
rather than sticking to the old values that were defined when the AI was
created. This keeps AI up-to-date and flexible, and prevents it from
acting based on outdated or irrelevant values that are locked into the
system.</p>
<h4 id="challenges.">Challenges.</h4>
<p>Deciding which ethical theories to include in the moral parliament
could be a challenging task. There are numerous ethical frameworks, and
selecting a representative set may be subjective and politically
charged. The decision procedure used to assign appropriate weights to
different ethical theories and aggregate their recommendations in ways
that reflect their importance could also be contentious. Different
stakeholders may have varying opinions on how to prioritize these
theories. Moreover, ethical theories can be subject to interpretation
and may have nuanced implications. Advanced AI systems would need to be
able to accurately understand and apply these theories in order to use a
moral parliament.</p>
<h3 id="conclusions-about-moral-parliament">Conclusions About Moral
Parliament</h3>
<h4 id="summary.-8">Summary.</h4>
<p>In this section, we explored using AIs to operationalize moral
parliaments, whether in the original form of representing moral theories
or generalized to representing stakeholders for any given issue. We
highlighted the advantages of using a moral parliament, such as reducing
the risk of overlooking or locking in certain values, allowing for the
representation of changing human values over time, and increasing
transparency and accountability in AI decision-making. We also noted
that moral parliaments encourage compromise and negotiation, leading to
more balanced and nuanced ethical decisions.</p>
<h1 id="conclusion-1">Conclusion</h1>
<h4 id="overview.-5">Overview.</h4>
<p>In this chapter, we have explored various ways in which we can embed
ethics into AIs, ensuring that they are safe and beneficial. As it
stands, it is far from guaranteed that the development of AIs will lead
to socially beneficial outcomes. By default, AIs are likely to be
developed according to businesses’ economic incentives and are likely to
follow parts of the law. This is insufficient. We almost certainly need
stronger protections in place to ensure that AIs behave ethically.
Consequently, we discussed how we can ensure AIs prioritize aspects of
our wellbeing by making us happy and help us flourish. Supposing AIs can
figure out how to promote individual wellbeing, we explored social
welfare functions as a way to guide their actions in order to help
improve wellbeing across society. Lastly, we considered using moral
parliaments to emulate democratic decision-making processes within AI
systems, ensuring ethical behavior by representing human interests
directly.</p>
<h4
id="we-should-strive-to-make-our-views-on-machine-ethics-less-contradictory.">We
should strive to make our views on machine ethics less
contradictory.</h4>
<p>We have considered several possible perspectives on how to ensure AIs
act ethically. As we have seen, there are deep tensions between many
plausible views, such as “AIs should do what you choose” versus “AIs
should do what you want” versus “AIs should do what makes you happy” and
so on. It is quite difficult to resolve these tensions and choose which
version of machine ethics best represents human values; however, before
we deploy powerful AI systems, we must do so anyway.</p>
<h4 id="as-a-baseline-we-want-ais-to-follow-the-law.">As a baseline, we
want AIs to follow the law.</h4>
<p>At the very least, we should require that AIs follow the law. This is
imperfect: as we have seen, the law is insufficiently comprehensive to
ensure that AI systems are safe and beneficial. Laws have loopholes, are
occasionally unethical and unrepresentative of the population, and are
often silent on doing good in ways AIs should be required to do.
However, if we can get AIs to follow the law, then we are at least
guaranteed that they refrain from the illegal acts—-such as murder and
theft—-that human societies have identified and outlawed.</p>
<h4
id="ais-should-increase-human-wellbeing-in-accordance-with-a-social-welfare-function.">AIs
should increase human wellbeing in accordance with a social welfare
function.</h4>
<p>Which conception of human wellbeing best matches reality and how we
should distribute it are difficult questions that we have certainly not
resolved within this chapter. However, if we had to guess, we might want
AIs to optimize continuous prioritarian social welfare functions where
individual wellbeing should be based on happiness or objective goods,
ensuring that wellbeing is fairly distributed throughout a society in
which everyone is happy to live in. We might use AIs to estimate
general-purpose wellbeing functions and directly increase what we
observe makes people better off. While this is speculative, the rapid
development of AIs forces us to speculate.</p>
<h4
id="ais-might-use-moral-parliaments-for-redundancy-and-adaptability.">AIs
might use moral parliaments for redundancy and adaptability.</h4>
<p>Especially if we are unsure about what comprises human wellbeing and
how we should best distribute it, we might want to embed redundancy and
adaptability into our AIs so that they can act ethically even if we make
mistakes or change our minds. Given the potential benefits and the
ability of moral parliaments to address key concerns in AI
decision-making, we should be optimistic about their future use. By
incorporating diverse perspectives of individual moral theories or
stakeholders in real-time, moral parliaments can help ensure that AI
systems act in accordance with human values and avoid extreme or biased
behavior, even if human values change over time.</p>
<h4 id="we-should-keep-thinking-about-machine-ethics.">We should keep
thinking about machine ethics.</h4>
<p>As we move forward, it is crucial that we continue to engage in
rigorous research, open dialogue, and interdisciplinary collaboration to
address the ethical concerns associated with AI. By doing so, we can
strive towards creating AI systems that not only avoid the worst harms
to society but actively work towards enhancing social wellbeing. In the
next part of this textbook, we will move past considering how to create
one beneficial AI and onto the problem of how to ensure that a world
with multiple AI agents can serve human interests, discussing complex
systems, multiagent dynamics, safety engineering, and AI governance.</p>
