<h3 id="sec:wellbeing">Wellbeing</h3>
<p><strong>Wellbeing is how well a person’s life is going for
them.</strong> It is commonly considered to be intrinsically good,
though there are different accounts of precisely what wellbeing is and
how we can evaluate it. Generally, a person’s wellbeing seems to depend
on the extent to which that person is happy, healthy, and fulfilled.
Three common accounts of wellbeing characterize it as 1) net pleasure
over pain, 2) preference satisfaction, or 3) a collection of objective
goods. Each account is elaborated below.</p>
<p>Some philosophers, known as <em>hedonists</em>, argue that wellbeing
is the achievement of the greatest balance of pleasure and happiness
over pain and suffering. (For simplicity we do not distinguish, in this
chapter, between “pleasure” and “happiness” or between “pain” and
“suffering,” though neither pair is interchangeable.) All else equal,
individuals who experience more pleasure have higher wellbeing. All else
equal, individuals who experience more pain have lower wellbeing.</p>
<p><strong>According to hedonism, pleasure is the only intrinsic
good.</strong> Goods like health, knowledge, and love are instrumentally
valuable. That is, they are only good insofar as they lead to pleasure.
It may feel as though other activities are intrinsically valuable. For
instance, someone who loves literature may feel that studying classic
works is valuable for its own sake. Yet, if the literature lover were
confronted with proof that reading the classics makes them less happy
than they otherwise would be, they might no longer value studying
literature. Hedonists believe that when we think we value certain
activities, we actually value the pleasure they bring us, not the
activities themselves.<p>
Hedonism is a relatively clear and intuitive account of wellbeing. It
seems to apply equally to everyone. That is, while we all may have
different preferences and desires, pleasure seems to be universally
valued. However, some philosophers argue that hedonism is an incomplete
account of wellbeing. They argue there may be other factors that
influence wellbeing, such as the pursuit of knowledge.</p>
<p>Some philosophers claim that what really matters for wellbeing is
that our preferences are satisfied, even if satisfying preferences does
not always lead to pleasurable experiences.<p>
One difficulty for preference-based theories is that there are different
kinds of preferences, and it’s unclear which ones matter. Preferences
can be split into three categories: stated preferences, revealed
preferences, and idealized preferences. Each of these categories can be
informative in different contexts.<p>
To illustrate different kinds of preferences, consider voter preferences
in a democratic election.<p>
In a democratic election, citizens choose which candidate they want to
elect by casting their vote on a ballot. Their choice to vote for a
given candidate can be impacted by a number of different factors.
Perhaps they have an existing political affiliation, are influenced by
social pressures, believe in the candidate’s policies, or maybe they
just like one candidate’s demeanor and personality. Importantly,
citizens may not always vote for the candidate they outwardly support,
and the choice to vote for a specific candidate can change when voters
discover new information.<p>
A voter’s <em>stated preference</em> is the candidate that they state
they support. Voters may express their stated preferences in
conversations, polls, and while campaigning.<p>
When a voter casts their vote on a ballot, they express their
<em>revealed preference</em>. Generally, a voter’s revealed preferences
align with their interests. For example, a voter who supports increased
funding for education might vote for a candidate who wants to increase
budgets for local public schools. A revealed preference is expressed by
your actions, not your words.<p>
People change their preferences upon learning new information.
Uninformed preferences can be reached quickly. For example, a voter
might have an uninformed preference based on a “gut reaction” to a
candidate. Voters can arrive at more <em>idealized preferences</em> once
they have gathered and evaluated all relevant information. They might
not actually do this—few people have the time or ability to perfectly
gather and evaluate all the relevant information that they would require
to find their idealized preferences. However, their preferences can
become more idealized over time. A voter might have an uninformed
preference for Candidate A and, after learning new information about
each candidate’s platform, they may arrive at a more informed preference
for a different candidate. In other words, preferences can change, and
they often do change as people become more informed.</p>
<p><strong>It is easy to learn about people’s stated preferences–—simply
ask them.</strong> Political polls and surveys, for example, are an easy
way to gather information about people’s stated preferences. However,
stated preferences may not always predict what people will actually
choose. A voter may outwardly express support for Candidate X, but when
it comes to casting their ballot, they may vote for Candidate Y.
Similarly, someone might express a stated preference to eat healthier,
but that doesn’t necessarily mean that they will. Their behavior (such
as eating only chocolate for a week) may indicate that their revealed
preference is for unhealthy food.<p>
Revealed preferences can be harder to observe, but they are generally
more useful for predicting people’s behavior. Someone with a stated
preference for vegetables but a revealed preference for chocolate is
more likely to purchase and consume chocolate than vegetables. When
researching consumer behavior, economists often prefer to study
consumers’ revealed preferences (i.e. what they buy) rather than stated
preferences (i.e. what they say they’d like to buy).</p>
<p>Others believe that wellbeing is the achievement of an objective set
of “goods” or “values.” These goods are considered necessary for living
a good life regardless of a person’s individual experiences or
preferences. There is disagreement about which particular goods are
necessary for wellbeing. Commonly proposed goods include happiness,
health, relationships, knowledge, and more. Objective goods theorists
consider these values to be important for wellbeing independently of
individual beliefs and preferences.<p>
There is no uncontroversial way to determine which goods are important
for living a good life. However, this uncertainty is not a unique
problem for objective goods theory. It can be difficult for hedonists to
explain why happiness is the only value that’s important for wellbeing
and for preference satisfaction theorists to determine which preferences
matter most.<p>
While people disagree about which account of wellbeing is correct, most
people agree that wellbeing is an important moral consideration. All
else equal, taking actions that promote wellbeing is generally
considered morally superior to taking actions that reduce
wellbeing.<p>
In the future, it is conceivable that AIs might be conscious and have
preferences but not experience pleasure, which would mean they could
have wellbeing according to the preference satisfaction theorists but
not hedonists. It is also possible that in the future AIs may have
wellbeing according to all three accounts of wellbeing. This would
require that we dramatically reassess our relationship with AIs.</p>
<h2 id="constraints-and-special-obligations">7.3.2 Constraints and Special
Obligations</h2>
<p>We’ve covered the moral consideration of intrinsic goods, and focused
on the intrinsic good wellbeing. Special obligations and constraints are
key considerations when we make ethical decisions.</p>
<p><strong>Special obligations are duties arising from
relationships.</strong> We can incur special obligations when we promise
someone to do something, take a professional position with
responsibilities, have a child, make a romantic commitment to a partner,
and so on. Sometimes we can have special obligations that we did not
volunteer for—a child to its parents, or our duties to fellow
citizens.</p>
<p><strong>Constraints are actions that we are morally prohibited from
taking.</strong> A constraint is something that places limits on our
actions. For example, many people think we’re morally prohibited from
lying, stealing, cheating, harming others, and more.</p>
<p><strong>Constraints often come in the form of rights.</strong> Rights
are claims that individuals may have over their community. For instance,
many people believe that humans have the rights to life, freedom,
privacy, and so on. Some people argue that any individual with the
capacity for experiencing pleasure and pain has rights. Non-human
individuals (including animals and AI systems) might also have certain
rights.<p>
An individual’s rights may require that society intervene in certain
ways to ensure that those rights are fulfilled. For instance, an
individual’s right to food, shelter, or education may require the rest
of society to pay taxes so that the government can ensure that
everyone’s rights are fulfilled. Rights that require certain actions
from others are called positive rights.<p>
Other rights may require that society abstain from certain actions. For
instance, an individual’s right to free speech, privacy, or freedom from
discrimination may require the rest of society to refrain from
censorship, spying, and discriminating. Rights that require others to
abstain from certain behaviors are called negative rights.<p>
Many AI researchers think that, for now, we should avoid accidentally
creating AIs that deserve rights <span class="citation"
data-cites="sebo2022chatbot">[1]</span>; for instance, perhaps all
entities that can experience suffering have natural rights to protect
them from it. Some think we should especially avoid giving them positive
rights; it might be fine to give them rights against being tortured but
not the right to vote. If they come to deserve rights, this would create
many complications and undermine our claim to control.</p>
<h2 id="what-does-it-mean-for-an-action-to-be-right-or-wrong">7.3.3 What does
it mean for an action to be right or wrong?</h2>
<p>Some of the first questions we might ask about ethics are: Are all
actions either right or wrong? Are some simply neutral? Are there other
distinctions we might want to draw between the morality of different
actions?<p>
The answers to these questions, like most moral questions, are the
subject of much debate. Here, we will simply examine what it might mean
for an action to be right or wrong. We will also draw some other useful
distinctions, like the distinction between obligatory and non-obligatory
actions, and between permissible and impermissible actions. These
distinctions will be useful in the following section, when we discuss
the considerations that inform our moral judgments.</p>
<h3 id="options">Options</h3>
<p>Special obligations and constraints tell us what we should not do,
and sometimes, what we must do. Intrinsic goods tell us about things
that would be good, should they happen. But philosophers debate how much
good we are required to do.</p>
<p><strong>Options are moral actions which we are neither required to do
nor forbidden from doing.</strong> Even though it would be good to
donate money, many people do not think people are morally required to
donate. This is an ethical option. If we believe in options, not all
actions are either required or forbidden.<p>
We now break down actions onto a spectrum on which we will simply
examine what it might mean for an action to be right or wrong. We will
also draw some other useful distinctions, like the distinction between
obligatory and non-obligatory actions and between permissible and
impermissible actions.</p>
<p><strong>Obligatory actions are those that we are morally obligated or
required to perform.</strong> We have a moral duty or obligation to
carry out obligatory actions, based on ethical principles. For example,
it is generally considered obligatory to help someone in distress, or
refrain from hurting others.</p>
<p><strong>Non-obligatory actions are actions that are not morally
required or necessary.</strong> Non-obligatory actions may still be
morally good, but they are not considered to be obligatory. For example,
volunteering at a charity organization or donating to a good cause may
be good, but most people don’t consider them to be obligatory.</p>
<p><strong>Permissible actions may be morally good or simply neutral
(i.e. not good or bad).</strong> In general, any action that is not
impermissible is permissible. Moral obligations, of course, are
permissible. We can consider four other actions: volunteering, donating
to charity, eating a sandwich, and taking a walk. These seem
permissible, and can be classified into two categories.<p>
One class of permissible actions is called <em>supererogatory
actions</em>. These may include volunteering or giving to charity. They
are generally considered good; in fact, we tend to believe that the
people who do them deserve praise. On the other hand, we typically don’t
consider the failure to do these actions to be bad. We might think of
supererogatory actions as those that are morally good, but optional;
they go “above and beyond” what is morally required.<p>
Another class of permissible actions is called <em>morally neutral
actions</em>. These may include seemingly inconsequential activities
like eating a sandwich or taking a walk. Most people probably believe
that actions like these are neither right nor wrong.</p>
<p><strong>Impermissible actions are those that are morally prohibited
or unacceptable.</strong> These actions violate moral laws or principles
and are considered wrong. Stealing or attacking someone are generally
considered to be impermissible actions.<p>
</p>
<figure id="fig:action-types">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/action_types.png" class="tb-img-full"/>
<p class="tb-caption">Classes of permissible and non-obligatory actions</p>
<!--<figcaption>Classes of permissible and non-obligatory actions-->
<!--</figcaption>-->
</figure>
<p>Some philosophers believe that all actions fit on a scale like the
one above. At one end of the scale are impermissible actions, like
murder, theft, or exploitation. At the other end are obligatory actions,
like honesty, respect, and not harming others. In between are neutral
and supererogatory actions. These are neither impermissible nor
obligatory. Many people believe that the vast majority of our actions
fall into these two categories. Crucially, in designing ethical AI
systems that operate in the real world, it is important to determine
which actions are obligatory and which actions are impermissible.<p>
However, some philosophers do not believe in options; rather that
actions are all on a spectrum from the least moral to the most moral. We
will learn more about these positions, and others, when we discuss moral
theories later in this chapter.</p>
<h3 id="from-considerations-to-theories">From Considerations to
Theories</h3>
<p><strong>Moral considerations can guide our day-to-day decision
making.</strong> Understanding which factors are morally relevant can
help us think more clearly about what we should do. Of course, we don’t
always stop to consider every factor before making a decision. Rather,
we tend to draw broader conclusions or moral principles based on our
evaluations of specific cases. For instance, once we consider a few
examples of the ways in which stealing can harm others, we might draw
the conclusion that we shouldn’t steal.<p>
The considerations discussed in this section provide a basis on which we
can develop more practical, action-guiding theories about how we should
behave. The types of fundamental considerations in this section comprise
a subfield of ethics called <em>metaethics</em>. Metaethics is the
consideration of questions like “What makes an action right or wrong?”
and “What does it mean to say that an action is right or wrong?” <span
class="citation" data-cites="fisher2014metaethics">[2]</span><p>
These considerations are important in the context of designing AI
systems. In order to respond to situations in an appropriate way, AI
systems need to be able to identify morally relevant features and detect
situations where certain moral principles apply. They would also need to
be able to evaluate and compare the moral worth of potential actions,
taking into account various purported intrinsic goods as well as
normative factors such as special obligations and constraints. The
challenges of designing objectives for AI systems that respect moral
principles are further discussed in the Machine Ethics chapter.<p>
In the following section, we will discuss some popular moral
theories.</p>

<br>
<br>
<h3>References</h3>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-sebo2022chatbot" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] J.
Sebo, <span>“Op-ed: What should we do if a chatbot has thoughts and
feelings?”</span> [Online]. Available: <a
href="https://www.latimes.com/opinion/story/2022-06-16/artificial-intelligence-morals-ethics-sentience-thinking">https://www.latimes.com/opinion/story/2022-06-16/artificial-intelligence-morals-ethics-sentience-thinking</a></div>
</div>
<div id="ref-fisher2014metaethics" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] A.
Fisher, <em>Metaethics: An introduction</em>. Routledge, 2011.</div>
</div>
</div>


<br>
<div class="visionbox">
<legend class="visionboxlegend">
    <p><span><b>A Note on Wellbeing in Social Science</b></span></p>
</legend>
Philosophers continue to debate what wellbeing is, or what it means to live a good
life. However, over the years, researchers have developed ways to
approximate wellbeing for practical purposes. Psychologists, economists,
philanthropists, policy makers, and other professionals need–—at
least–—a working definition of wellbeing in order to study, measure, and
promote it. Here, we describe some common views of wellbeing and
illustrate how they are used in social science.</p>
<strong>Preference satisfaction.</strong>
Preference theorists view wellbeing as fulfilling desires or
satisfying preferences, even if doing so does not always induce
pleasure. Nonetheless, it remains an open question whether all desires
are tied to wellbeing or just certain kinds, like higher-order or
informed desires.<p>
<strong>Economics.</strong> <em>Standard economics</em> often uses
preference satisfaction theories to study wellbeing. Revealed
preferences can be observed by studying the decisions people make. If
people desire higher incomes, for example, economists can promote
wellbeing by researching the impact of different economic policies on
gross domestic product (<em>GDP</em>).<p>
<strong>Social Surveys and Psychology.</strong> Traditionally,
psychological surveys evaluate wellbeing in terms of <em>life
satisfaction</em>. Life satisfaction is a measure of people’s stated
preferences—preferences or thoughts that individuals outwardly
express—regarding how their lives are going for them. Life satisfaction
surveys typically focus on tangible characteristics of people’s lives,
such as financial security, health, and personal achievements. They are
well suited to understanding the effects of concrete economic factors,
such as income and education, on an individual’s psychological
wellbeing. For example, to promote wellbeing, psychologists might
research the effects of access to education on one’s ability to achieve
the goals they set for themselves.</p>
<strong>Hedonism.</strong>
Under hedonist theories of wellbeing, an individual’s wellbeing is
determined by their mental states. In particular, the balance of
positive mental states (like pleasure or happiness) over negative mental
states (like pain or suffering).<p>
<strong>Economics.</strong> <em>Welfare economics</em> uses hedonism to
evaluate the wellbeing of populations. To estimate gross national
happiness (<em>GNH</em>)—-an indicator of national welfare—-it considers
the effects of several factors from psychological wellbeing to
ecological diversity and resilience on individuals’ mental states.
Welfare economists might prefer this framework because it is more
holistic - it evaluates both material and non-material aspects of
everyday life as they contribute to national welfare.<p>
<strong>Social Surveys and Psychology.</strong> Many psychologists also
use hedonist theories to understand and promote wellbeing. They may work
to identify the emotional correlates of happiness through surveys that
measure people’s stated emotions – unlike life satisfaction surveys,
these surveys do not reveal mental states. They reveal emotions that
people <em>remember</em>, not emotions they currently
<em>experience</em>. Researchers continue to look for ways to directly
observe emotions as they are experienced. For example, some studies use
cell phone apps to periodically prompt participants to record their
current emotions. Such research tactics may provide us with more precise
measures of individuals’ overall <em>happiness</em> by evaluating the
emotional responses to their everyday experiences in near real-time.</p>
<strong>Objective goods.</strong>
Under objective goods theories, wellbeing is determined by a certain
number of observable factors, independent of individuals’ preferences or
experiences. There are multiple theories about what those factors may
be. One of the most widely supported theories is <em>human
flourishing</em>. Under this view, wellbeing is more than just the
balance of pleasure over suffering, or the fulfillment of one’s
preferences – “the good life” should be full, virtuous, and meaningful,
encapsulating psychological, hedonistic, and social wellbeing all at
once.<p>
<strong>Economics.</strong> In economics, the <em>capabilities
approach</em> defines wellbeing as having access to a set of
capabilities that allow one to live the kind of life they value. It
emphasizes two core ideas: <em>functionings</em> and
<em>capabilities</em>. Functionings include basic and complex human
needs, ranging from good health to meaningful relationships.
Capabilities refer to the ability people have to choose and achieve the
functionings they value – they may include being able to move freely or
participate in the political process. This approach has significantly
influenced human development indicators, such as the Human Development
Index (<em>HDI</em>) – it allows developmental economists to measure and
compare wellbeing across different populations while also evaluating the
effectiveness of public policies.<p>
<strong>Psychology.</strong> Positive psychologists do not collapse
wellbeing into one dimension, rather, they argue for a
<em>psychologically rich life</em> – one that is happy, meaningful, and
engaging. Some psychologists use <em>PERMA theory</em> to evaluate
psychological richness, which considers five categories essential to
human flourishing: 1) experience of <em>positive emotions</em>, 2)
<em>engagement</em> with one’s interests, 3) maintenance of personal,
professional, and social <em>relationships</em>, 4) the search for
<em>meaning</em> or purpose, and 5) <em>accomplishments</em>, or the
pursuit of one’s goals. This framework is particularly useful in
evaluating wellbeing because it is universal–—it can be applied
cross-culturally—-and practical—-it can guide interventions aiming to
improve emotional wellbeing, social relationships, or activities that
provide a sense of meaning or accomplishment.<p>
While we don’t have a complete understanding of the nature of wellbeing,
we can use these theories as useful research tools. They can help us to
(a) understand how different factors contribute to wellbeing and (b)
evaluate the effectiveness of policies and other interventions aimed at
improving wellbeing.</p>
</div>

<h1 id="preferences">6.5 Preferences</h1>
<strong>Should we have
AIs satisfy people’s preferences?</strong>
A preference is a tendency to favor one thing over another. Someone
might prefer chocolate ice cream over vanilla ice cream, or they might
prefer that one party wins the election rather than another. These
preferences will influence actions. If someone prefers chocolate ice
cream over vanilla, they’re more likely to choose the former. Similarly,
if someone prefers one political party over another, they will likely
vote accordingly. In this manner, our preferences shape our behavior,
guiding us toward certain choices and actions over others. Preference is
similar to desire but always comparative. Someone might desire something
in itself—a new book, a vacation, or a delicious meal—but a preference
always involves a comparison between two or more alternatives.</p>
<strong>Overview.</strong>
In this section, we will consider whether preferences may have an
important role to play in machine ethics. In particular, if we want to
design an advanced AI system, the preferences of the people affected by
its decisions should plausibly help guide its decision-making. In fact,
some people (such as preference utilitarians) would say that preferences
are all we need. However, even if we don’t take this view, we should
recognize that preferences are still important. Preferences are further
explored in the next two chapters. First, in the chapter on ethics, we
discussed the idea that wellbeing might consist in having one’s
preferences satisfied. Second, in the chapter on utility functions, we
explain how preferences can be used to construct a utility function that
assigns numerical values to options and outcomes.<p>
To use preferences as the basis for increasing social wellbeing, we must
somehow combine the conflicting preferences of different people. We’ll
come to this later in this chapter, in a section on social welfare
functions. Before that, however, we must answer a more basic question:
what exactly does it mean to say that someone prefers one thing over
another? Moreover, we must decide why we think that satisfying someone’s
preferences is good for them and whether all kinds of preferences are
equally valuable. This section considers three different types of
preferences that could all potentially play a role in decision-making by
AI systems: revealed preferences, stated preferences, and idealized
preferences.</p>
<h2 id="revealed-preferences">6.5.1 Revealed Preferences</h2>
<p><strong>Preferences can be inferred from behavior.</strong> One set
of techniques for getting AI systems to behave as we want—inverse
reinforcement learning—is to have them deduce <em>revealed
preferences</em> from our behavior. We say that someone has a revealed
preference for X over Y if they choose X when Y is also available. In
this way, preference is revealed through choice. Consider, for example,
someone deciding what to have for dinner at a restaurant. They’re given
a menu, a list of various dishes they could order. The selection they
make from the menu is seen as a demonstration of their preference. If
they choose a grilled chicken salad over a steak or a plate of
spaghetti, they’ve just revealed their preference for grilled chicken
salad, at least in that specific context and time.<p>
While all theories of preferences agree that there is an important link
between preference and choice, the revealed preference account goes one
step further and claims that preference simply <em>is</em> choice.</p>
<strong>Revealed preferences
preserve autonomy.</strong>
One advantage of revealed preferences is that we don’t have to guess
what someone prefers. We can simply look at what they choose. In this
way, revealed preferences can help us avoid paternalism. Paternalism is
when leaders use their sovereignty to make decisions for their subjects,
limiting their freedom or choices, believing it is for the subjects’ own
good. However, we may think that typically people are themselves the
best judges of what is good for them. If so, then by relying on people’s
actions to reveal their preferences, we avoid the risk of
paternalism.</p>
<strong>However,
there are problems with revealed preferences.</strong>
The next few subsections will explore the challenges of
misinformation, weakness of will, and manipulation in the context of
revealed preferences. We will discuss how misinformation can lead to
choices that do not accurately reflect a person’s true preferences, and
how weakness of will can cause individuals to act against their genuine
preferences. Additionally, we will examine the various ways in which
preferences can be manipulated, ranging from advertising tactics to
extreme cases like cults, and the ethical implications of preference
manipulation.</p>
<h3 id="misinformation">Misinformation</h3>
<strong>Revealed
preferences can sometimes be based on misinformation.</strong>
If someone buys a used car that turns out to be defective, it doesn’t
mean they preferred a faulty car. They intended to buy a reliable car,
but due to a lack of information or deceit from the seller, they ended
up with a substandard one. Similarly, losing at chess doesn’t indicate a
preference for losing; rather, it’s an outcome of facing a stronger
player or making mistakes during the game. This means that we cannot
always infer someone’s preferences from the choices they make. Choice
does not reveal a preference between things as they actually are, but
between things as the person understands them. Therefore, we can’t rely
on revealed preferences if they are based on misinformation.</p>
<h3 id="weakness-of-will">Weakness of Will</h3>
<p><strong>Choices can be due to a lack of willpower rather than
considered preferences.</strong> Consider a smoker who wants to quit.
Each time they light a cigarette, they may be acting against their
genuine preference to stop smoking, succumbing instead to the power of
addiction. Therefore, it would be erroneous to conclude from their
behavior that they think that continuing to smoke would be best for
their wellbeing.</p>
<h3 id="manipulation">Manipulation</h3>
<p><strong>Revealed preferences can be manipulated in various
ways.</strong> Manipulations like persuasive advertising might
manipulate people into buying products they don’t actually want.
Similarly, revealed preferences might be the result of social pressure
rather than considered judgment, such as when buying a particular style
of clothing. In such cases, the manipulation may not be especially
malicious. At the other extreme, however, cults may brainwash their
members into preferring things they would not otherwise want, even
committing suicide. In the context of decision-making by advanced AI
systems, manipulation is a serious risk. An AI advisor system could
attempt to influence our decision-making for a variety of
reasons—perhaps its designer wants to promote specific products—by
presenting options in some particular way; for instance, by presenting a
list of options that looks exhaustive and excluding something it doesn’t
want us to consider.</p>
<strong>If
preference satisfaction is important, perhaps manipulation is
acceptable.</strong>
In at least some of these cases, it seems clear that preference
manipulation is bad. However, it may be less clear exactly why it is
bad. A natural answer is to say that people might be manipulated into
preferring things that are bad for them. Someone who is manipulated by
advertising into preferring junk food might thereby suffer negative
health consequences. However, if we think that wellbeing simply consists
in preference satisfaction, it doesn’t make sense to say that we might
prefer what is bad for us. On this account, having one’s preferences
satisfied is by definition good, regardless of whether those preferences
have been manipulated. This might lead us to think that what matters is
not (or at least not only) preference satisfaction, but happiness or
enjoyment. We’ll discuss this in the section on happiness.</p>
<strong>Disliking
manipulation suggests that wellbeing requires autonomy.</strong>
On the other hand, some may find that manipulation is bad even if the
person is manipulated into doing something that is good for them. For
example, suppose a doctor lies to her patient, telling him that unless
he loses weight, he will likely die soon. As a result, the patient
becomes greatly motivated to lose weight and successfully does so. This
provides a range of health benefits, even if his doctor never had any
reason to believe he would have died otherwise. If we think manipulation
is still bad, lack of enjoyment can’t be the whole story. This suggests
that we object to manipulation in part because it violates autonomy. We
might then think that autonomy—the ability to decide important matters
for oneself, without coercion—is objectively valuable regardless of what
the agent prefers. We’ll discuss this in the section on objective
goods.</p>
<h3 id="inverse-reinforcement-learning">Inverse Reinforcement
Learning</h3>
<strong>Inverse
Reinforcement Learning (IRL) relies on revealed preferences.</strong>
IRL is a powerful technique in the field of machine learning, which
focuses on extracting an agent’s objectives or preferences by observing
its behaviors. In more technical terms, IRL is about reverse engineering
the reward function—an internal ranking system that an agent uses to
assess the value of different outcomes—that the agent appears to be
optimizing, given a set of its actions and a model of the environment.
This technique can help ensure the alignment of AI system’s behaviors
with human values and preferences. However, leveraging revealed
preferences or observable choices of humans to train AI systems using
IRL poses significant challenges pertaining to AI safety.</p>
<strong>Using
revealed preferences as a training mechanism for IRL can be risky.</strong>
Reconsider the chess example: losing a game does not mean that we
prefer to lose. This interpretation could be a misrepresentation of the
player’s true preferences, potentially leading to undesirable outcomes.
Furthermore, extending observed preferences to unfamiliar situations
poses another hurdle. I may prefer to eat ice cream for dessert, but
that doesn’t mean I prefer to eat it for every meal. Similarly, I may
prefer to wear comfortable shoes for hiking, but that doesn’t mean I
want to wear them to a formal event. An AI system could inaccurately
extrapolate preferences from limited or context-specific data and
misapply these to other scenarios. Therefore, while revealed preferences
can offer significant insights for training AI, it is vital to
understand their limitations to safeguard the safety and efficiency of
AI systems.</p>
<strong>Summary.</strong>
Revealed preferences can be a powerful tool, as they allow an
individual’s actions to speak for themselves, reducing the risk of
paternalistic intervention. However, revealed preferences have inherent
shortcomings such as susceptibility to misinformation and manipulation,
which can mislead an AI system. This emphasizes the caution needed in
relying solely on revealed preferences for AI training. It underscores
the importance of supplementing revealed preferences with other methods
to ensure a more comprehensive and accurate understanding of a user’s
true preferences.</p>
<h2 id="stated-preferences">6.5.2 Stated Preferences</h2>
<p><strong>Preferences can be straightforwardly queried.</strong>
Another set of techniques for getting AI systems to behave as we
want—human supervision and feedback—rely on people to state their
preferences. A person’s <strong>stated preferences</strong> are the
preferences they would report if asked. For example, someone might ask a
friend which movie they want to see. Similarly, opinion polls might ask
people which party they would vote for. In both cases, we rely on what
people say as opposed to what they do, as was the case with revealed
preferences.<p>
Stated preferences overcome some of the difficulties with revealed
preferences. For example, stated preferences are less likely to be
subject to weakness of will: when we are further removed from the
situation, we are less inclined to fall for temptations. Therefore,
stated preferences are more likely to reflect our stable, long-term
interests.</p>
<strong>Stated preferences are
still imperfect.</strong>
Stated preferences do not overcome all difficulties with revealed
preferences. Stated preferences can still be manipulated. Further,
individuals might state preferences they believe to be socially
acceptable or admirable rather than what they truly prefer, particularly
when the topics are sensitive or controversial. Someone might overstate
their commitment to recycling in a survey, for instance, due to societal
pressure and norms around environmental responsibility.</p>
<h3 id="preference-accounting">Preference Accounting</h3>
<p>One set of problems with stated preferences concern which types of
preferences should be satisfied.</p>
<strong>First,
someone might never know their preference was fulfilled.</strong>
Suppose someone is on a trip far away. On a bus journey, they
exchange a few glances with a stranger whom they’ll never meet again.
Nevertheless they form the preference that the stranger’s life goes
well. Should this preference be taken into account? By assumption, they
will never be in a position to know whether the preference has been
satisfied or not. Therefore, they will never experience any of the
enjoyment associated with having their preference satisfied.</p>
<strong>Second,
we may or may not care about the preferences of the dead.</strong>
Suppose someone in the 18th century wanted to be famous long after
their death. Should such preferences count? Do they give us reason to
promote that person’s fame today? As in the previous example, the
satisfaction of such preferences can’t contribute any enjoyment to the
person’s life. Could it be that what we really care about is enjoyment
or happiness, and that preferences are a useful but imperfect guide
toward what we enjoy? We will return to this in the section on
happiness.</p>
<strong>Third,
preferences can be about others’ preferences being fulfilled.</strong>Suppose a parent prefers that their children’s preferences are
satisfied. Should this preference count, in addition to their children’s
preferences themselves? If we say yes, it follows that it is more
important to satisfy the preferences of those who have many people who
care for them than of those who do not. One might think that this is a
form of double counting, and claim that it is unfair to those with fewer
who care for them. On the other hand, one might take fairness to mean
that we should treat everyone’s preferences equally—including their
preferences about other people’s preferences.</p>
<strong>Fourth, preferences might
be harmful.</strong>
Suppose someone hates their neighbor, and prefers that they meet a
cruel fate. We might think that such malicious or harmful preferences
should not be included. On this view, we should only give weight to
preferences that are in some sense morally acceptable. However,
specifying exactly which preferences should be excluded may be
difficult. There are many cases where satisfying one person’s
preferences may negatively impact others. For example, whenever some
good is scarce, giving more of it to one person necessarily implies that
someone else will get less. Therefore, some more detailed account of
which preferences should be excluded is needed.</p>
<strong>Fifth, we
might be confused about our preferences.</strong>
Suppose a mobile app asks its users to choose between two privacy
settings upon installation: allowing the app to access their location
data all the time, or allowing the app to access their location data
only while they’re using the app. While these options seem
straightforward, the implications of this choice are much more complex.
To make a truly informed decision, users need to understand how location
data is used, how it can be combined with other data for targeted
advertising or profiling, what the privacy risks of data breaches are,
and how the app’s use of data aligns with local data protection laws.
However, we may not fully understand the alternatives we’re choosing
between.</p>
<strong>Sixth,
preferences can be inconsistent over time.</strong>
It could be that the choice we make will change us in some
fundamental way. When we undergo such transformative experiences <span
class="citation" data-cites="paul2014transformative">[1]</span>, our
preferences might change. Some claim that becoming a parent,
experiencing severe disability, or undergoing a religious conversion can
be like this. If this is right, how should we evaluate someone’s
preference between becoming a parent and not becoming a parent? Should
we focus on their current preferences, prior to making the choice, or on
the preferences they will develop after making the choice? In many cases
we may not even know what those new preferences will be.<p>
As technology advances, we may increasingly have the option to bring
about transformative experiences <span class="citation"
data-cites="paul2014transformative">[1]</span>. For this reason, it is
important that advanced AI systems tasked with decision-making are able
to reason appropriately about transformative experiences. For this, we
cannot rely on people’s stated preferences alone. By definition, stated
preferences can only reflect the person’s identity at the time. Of
course, people can try to take possible future developments into account
when they state their preferences. However, if they undergo a
transformative experience their preferences might change in ways they
cannot anticipate.</p>
<h3 id="human-supervision">Human Supervision</h3>
<p><strong>Stated preferences are used to train some AI
systems.</strong> In reinforcement learning with human feedback (RLHF),
standard reinforcement learning is augmented by human feedback from
people who rank the outputs of the system. In RLHF, humans evaluate and
rank the outputs of the system based on quality, usefulness, or another
defined criterion, providing valuable data to guide the system’s
iterative learning process. This ranking serves as a form of reward
function that the system uses to adjust its behavior and improve future
outputs.<p>
Imagine that we are teaching a robot how to make a cup of coffee. In the
RLHF process, the AI would attempt to output a cup of coffee, and then
we would provide feedback on how well it did. We could rank different
attempts and the robot would use this information to understand how to
make better coffee in the future. The feedback helps the robot learn not
just from its own trial and error, but also from our expertise and
judgment. However, this approach has some known difficulties.</p>
<strong>First,
as AI systems become more powerful, human feedback might be
infeasible.</strong>
As the problems AI solve become increasingly difficult, using human
supervision and feedback to ensure that those systems behave as desired
becomes difficult as well. In complex tasks like creating bug-free and
secure code, generating arguments that are not only persuasive but true,
or forecasting long-term implications of policy decisions, it may be too
time-consuming or even impossible for humans to evaluate and guide AI
behavior. Moreover, there are inherent risks from depending on human
reliability: human feedback may be systematically biased in various
ways. For example, inconvenient but true things may often be labeled as
bad. In addition to any bias, relying on human feedback will inevitably
mean some rate of human error.</p>
<strong>Second, RLHF
usually does not account for ethics.</strong>
Approaches based on human supervision and feedback are very broad.
These approaches primarily focus on task-specific performance, such as
generating accurate book summaries or bug-free code. However, these
task-specific evaluations may not necessarily translate into a
comprehensive understanding of ethical principles or human values.
Rather, they improve general capabilities since humans prefer smarter
models.<p>
Take, for instance, feedback on code generation. A human supervisor
might provide feedback based on the code’s functionality, efficiency, or
adherence to best programming practices. While this feedback helps in
creating better code, it doesn’t necessarily guide the AI system in
understanding broader ethical considerations, such as ensuring privacy
protection or maintaining fairness in algorithmic decisions.
Specifically, while RLHF is effective for improving AI performance in
specific tasks, it does not inherently equip AI systems with what’s
needed to grapple with moral questions. This gap underscores the need
for specific machine ethics research.</p>
<strong>Summary.</strong>
We’ve seen that stated preferences have certain advantages over
revealed preferences. However, stated preferences still have issues of
their own. It may not be clear how we should account for all different
kinds of preferences, such as ones that are only satisfied after the
person has died, or ones that fundamentally alter who we are. For these
reasons, we should be wary of using stated preferences alone to train
AI.</p>
<h2 id="idealized-preferences">6.5.3 Idealized Preferences</h2>
<p><strong>We could idealize preferences to avoid problems like weakness
of will.</strong> A third approach to getting AI systems to behave as we
want is to make them able to infer what we would prefer if our
preferences weren’t subject to the various distorting forces we’ve come
across. Someone’s <strong>idealized preferences</strong> are the
preferences they would have if they were suitably informed. Idealized
preferences avoid many of the problems of both revealed preferences and
stated preferences. Idealized preferences would not be based on false
beliefs, nor would they be subject to weakness of will, manipulation, or
framing effects. This makes it clearer how idealized preferences might
be linked to wellbeing, and therefore something we might ask an AI
system to implement.</p>
<strong>It is unclear how we
idealize preferences.</strong>
What exactly do we need to do to figure out what someone’s idealized
preferences are, based on their revealed preferences or their stated
preferences? It’s clear that the idealized preferences should not be
based on any false beliefs. We might imagine a person’s idealized
preferences as ones they would have if they fully grasped the options
they faced and were able to think through the situation in great detail.
However, this description is rather vague. It may be that it doesn’t
uniquely narrow down a set of idealized preferences. That is, there may
be multiple different ways of idealizing someone’s preferences, each of
which is one possible way that the idealized deliberation could go. If
so, idealized preferences may not help us decide what to do in such
cases.<p>
Additionally, some may argue that in addition to removing any dependence
on false belief or other misapprehensions, idealized preferences should
also take moral considerations into account. For example, perhaps
malicious preferences of the kind discussed earlier would not remain
after idealization. These may not be insurmountable problems for the
view that advanced AI systems should be tasked with satisfying people’s
idealized preferences. However, it shows that the view stands in need of
further elaboration, and that different people may disagree over what
exactly should go into the idealization procedure.</p>
<strong>We might think
that preferences are pointless.</strong>
Suppose someone’s only preference, even after idealization, is to
count the blades of grass on some lawn. This preference may strike us as
valueless, even if we suppose that the person in question derives great
enjoyment from the satisfaction of their preferences. It is unclear
whether such preferences should be taken into account. The example may
seem far-fetched, but it raises the question of whether preferences need
to meet some additional criteria in order to carry weight. Perhaps
preferences, at least in part, must be aimed at some worthy goal in
order to count. If so, we might be drawn toward an objective goods view
of wellbeing, according to which achievements are important objective
goods.<p>
On the other hand, we may think that judging certain preferences as
lacking value reveals an objectionable form of elitism. It is unfair to
impose our own judgments of what is valuable on other people using
hypothetical thought experiments, especially when we know their actual
preferences. Perhaps we should simply let people pursue their own
conception of what is valuable.</p>
<strong>We might
disagree with our idealized preferences.</strong>
Suppose someone mainly listens to country music, but it turns out
that their idealized preference is to listen to opera. When they
themselves actually listen to opera music, they have a miserable
experience. It seems unlikely that we should insist that listening to
opera is, in fact, good for them despite the absence of any enjoyment.
This gives rise to an elitism objection like before. If they don’t enjoy
satisfying their idealized preferences, why should those preferences be
imposed on them? This might lead us to think that what ultimately
matters is enjoyment or happiness, rather than preference satisfaction.
Alternatively, it might lead us to conclude that autonomy matters in
addition to preference satisfaction. If idealized preferences are
imposed on someone who would in fact rather choose contrary to those
idealized preferences, this would violate their autonomy.<p>
One might think that with the correct idealization procedure, this could
never happen. That is, whatever the idealization procedure does––remove
false beliefs and other misconceptions, increase awareness and
understanding––it should never result in anything so alien that the
actual person would not enjoy it. On the other hand, it’s difficult to
know exactly how much our preferences would change when idealized.
Perhaps removing false beliefs and acquiring detailed understanding of
the options would be a transformative experience that fundamentally
alters our preferences. If so, idealized preferences may well be so
alien from the person’s actual preferences that they would not enjoy
having them satisfied.</p>
<h3 id="ai-ideal-advisor">AI Ideal Advisor</h3>
<strong>One
potential application of idealized preferences is the AI ideal
advisor.</strong>
Suppose someone who hates exploitation and takes serious
inconvenience to avoid emissions would ideally want to buy food that has
been ethically produced, but does not realize that some of their
groceries are unethically produced. An AI ideal advisor would be
equipped with detailed real-world knowledge, such as the details of
supply chains, that could help them make this decision. In addition to
providing factual information, the AI ideal advisor would be
disinterested: it wouldn’t favor any specific entity, object, or course
of action solely due to their particular qualities (such as nationality
or brand), unless explicitly directed to do so. It would also be
dispassionate, meaning that it wouldn’t let its advice be swayed by
emotion. Finally, it would be consistent, applying the same set of moral
principles across all situations <span class="citation"
data-cites="giubilini2018artificial">[2]</span>.<p>
Such an AI ideal advisor could possibly help us better satisfy the moral
preferences we already have. Something close to the AI ideal advisor has
previously been discussed in the context of AI safety under the names of
“coherent extrapolated volition” and “indirect normativity.” In all
cases, the fundamental idea is to take an agent’s actual preferences,
idealize them in certain ways, and then use the result to guide
decision-making by advanced AI systems. Of course, having such an
advisor requires that we solve many of the challenges that we presented
in the Single Agent Safety chapter, as well as settle on a clear way to identify and idealize
individual preferences.</p>
<strong>Summary.</strong>
Idealized preferences overcome many of the difficulties of revealed
and stated preferences. Because idealized preferences are free from the
misconceptions that may affect these other types of preferences, they
are more plausibly ones that we would want an AI system to satisfy.
However, figuring out what people’s preferences would in fact be after
idealization can be difficult. Moreover, it could be that the
preferences are without value even after idealization, or that the
actual person would not appreciate having their idealized preferences
satisfied. An AI ideal advisor might be difficult to create, but sounds
highly appealing.</p>
<h3 id="conclusions-about-preferences">Conclusions About
Preferences</h3>
<p><strong>Preferences seem relevant to wellbeing—but we don’t know
which ones.</strong> The preferences people reveal through choice often
provide evidence about what is good for them, but they can be distorted
by misinformation, manipulation, and other factors. In some cases,
people’s stated preferences may be a better guide to what is good for
them, though it is not always clear how to account for stated
preferences. If we are looking for a notion of preference that plausibly
captures what is good for someone, idealized preferences are a better
bet. However, it can be difficult to figure out what someone’s idealized
preferences would be. It seems, then, that preferences-—while important
to wellbeing and useful to train AI in accordance with human values—are
not a sufficient basis for a comprehensive account of machine
ethics.</p>
<strong>Moving from preferences
to happiness.</strong>
People’s preferences can be easily influenced by external factors.
For instance, individuals may be swayed by persuasive advertising and
purchase products that do not truly fulfill their needs or bring them
lasting happiness. Similarly, social media platforms often present an
idealized version of other people’s lives, leading many to compare
themselves to others and feel inadequate or unhappy with their own
circumstances. Inconsistent preferences pose a challenge for the
preference satisfaction view, as discussed in the previous
section.<p>
One way of ensuring people’s preferences reflect their wellbeing is to
use idealized preferences. The preferences that people would have if
they were suitably informed and rational would be well-defined and
aligned with their interests. However, as we explored in the previous
section, this view faces another problem: if satisfying someone’s
idealized preferences does not bring them any pleasure or enjoyment, why
should it matter that they are satisfied?<p>
This objection suggests that what we ultimately care about is happiness
rather than preference satisfaction. If so, we should use a framework
that places happiness front and center. By instructing an AI system to
increase happiness, we might aim to overcome the human biases and
limitations that stop us from pursuing our happiness and enable the
system to make decisions that have a positive impact on overall
wellbeing.</p>

<br>
<br>
<h3>References</h3>

<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-paul2014transformative" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] L.
A. Paul, <em>Transformative experience</em>. in Oxford scholarship
online. Oxford University Press, 2014. Available: <a
href="https://books.google.com.au/books?id=zIXjBAAAQBAJ">https://books.google.com.au/books?id=zIXjBAAAQBAJ</a></div>
</div>
<div id="ref-giubilini2018artificial" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] A.
Guibilini, <span>“The artificial moral advisor. The <span>‘ideal
observer’</span> meets artificial intelligence,”</span> <em>Philosophy
&amp; Technology</em>, 2018, doi: <a
href="https://doi.org/10.1007/s13347-017-0285-z">https://doi.org/10.1007/s13347-017-0285-z</a>.</div>
</div>
</div>
