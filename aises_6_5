<h1 id="sec:wellbeing">Wellbeing</h1>
<p>In the next few sections, we will explore how AIs can be used to increase human wellbeing. We start by asking: what is wellbeing?</p>

<p><strong>Wellbeing can be defined as how well a person’s life is going for
them.</strong> It is commonly considered to be intrinsically good, and some think of wellbeing as the ultimate good. 
Utilitarianism, for instance, holds some form of wellbeing as the sole moral good.</p>

<p>There are different accounts of precisely what wellbeing is and how we can evaluate it. 
Generally, a person’s wellbeing seems to depend on the extent to which that person is happy, healthy, and fulfilled. 
Three common accounts of wellbeing characterize it as 1) net pleasure
over pain, 2) preference satisfaction, or 3) a collection of objective
goods. Each account is elaborated below.</p>

<h2 id="sec:pleasure">Wellbeing as the Net Balance of Pleasure over Pain</h2>
<p>Some philosophers, known as <em>hedonists</em>, argue that wellbeing
is the achievement of the greatest balance of pleasure and happiness
over pain and suffering. (For simplicity we do not distinguish, in this
chapter, between “pleasure” and “happiness” or between “pain” and
“suffering”). All else equal,
individuals who experience more pleasure have higher wellbeing and individuals who experience more pain have lower wellbeing.</p>
<p><strong>According to hedonism, pleasure is the only intrinsic
good.</strong> Goods like health, knowledge, and love are instrumentally
valuable. That is, they are only good insofar as they lead to pleasure.
It may feel as though other activities are intrinsically valuable. For
instance, someone who loves literature may feel that studying classic
works is valuable for its own sake. Yet, if the literature lover were
confronted with proof that reading the classics makes them less happy
than they otherwise would be, they might no longer value studying
literature. Hedonists believe that when we think we value certain
activities, we actually value the pleasure they bring us, not the
activities themselves.<p>
Hedonism is a relatively clear and intuitive account of wellbeing. It
seems to apply equally to everyone. That is, while we all may have
different preferences and desires, pleasure seems to be universally
valued. However, some philosophers argue that hedonism is an incomplete
account of wellbeing. They argue there may be other factors that
influence wellbeing, such as the pursuit of knowledge.</p>

<h2 id="sec:preference-satisfaction">Wellbeing as Preference Satisfaction</h2>
<p>Some philosophers claim that what really matters for wellbeing is
that our preferences are satisfied, even if satisfying preferences does
not always lead to pleasurable experiences.<p>
One difficulty for preference-based theories is that there are different
kinds of preferences, and it’s unclear which ones matter. Preferences
can be split into three categories: stated preferences, revealed
preferences, and idealized preferences. If someone expresses a preference for eating healthy but never does, 
then their stated preference (eating healthy) diverges from their revealed preference (eating unhealthy). 
Suppose they would choose to eat healthy if fully informed of the costs and benefits: their idealized preferences, 
then, would be to eat healthy. Each of these categories can be informative in different contexts: we explore their relevance in the next section.</p>

<h2 id="sec:objective-goods">Wellbeing as a Collection of Objective Goods</h2>
<p>Others believe that wellbeing is the achievement of an objective set
of “goods” or “values.” These goods are considered necessary for living
a good life regardless of a person’s individual experiences or
preferences. 
<p><strong>There is disagreement about which particular goods are
necessary for wellbeing.</strong> Commonly proposed goods include happiness,
health, relationships, knowledge, and more. Objective goods theorists
consider these values to be important for wellbeing independently of
individual beliefs and preferences.</p>
<p><strong>One objection to the objective goods theory is that it’s elitist.</strong> The objective goods theory claims that some things are good for people even 
if they derive no pleasure or satisfaction from them. This claim might seem objectionably paternalistic; for instance, it seems condescending to claim that someone
 with little regard for aesthetic appreciation is thereby leading a deficient life. In response, objective goods theorists might claim that these additional goods 
do benefit people, but only if those people do in fact enjoy them.</p>
<p>There is no uncontroversial way to determine which goods are important
for living a good life. However, this uncertainty is not a unique
problem for objective goods theory. It can be difficult for hedonists to
explain why happiness is the only value that’s important for wellbeing, for instance.In the following sections we focus primarily on other interpretations of wellbeing
 and do not have space to discuss objective goods theory in depth, particularly given that there are many ways it can be specified.

<p>
Many AI researchers think that, for now, we should avoid accidentally
creating AIs that deserve rights <span class="citation"
data-cites="sebo2022chatbot">[1]</span>; for instance, perhaps all
entities that can experience suffering have natural rights to protect
them from it. Some think we should especially avoid giving them positive
rights; it might be fine to give them rights against being tortured but
not the right to vote. If they come to deserve rights, this would create
many complications and undermine our claim to control.</p>
<h2 id="what-does-it-mean-for-an-action-to-be-right-or-wrong">7.3.3 What does
it mean for an action to be right or wrong?</h2>
<p>Some of the first questions we might ask about ethics are: Are all
actions either right or wrong? Are some simply neutral? Are there other
distinctions we might want to draw between the morality of different
actions?<p>
The answers to these questions, like most moral questions, are the
subject of much debate. Here, we will simply examine what it might mean
for an action to be right or wrong. We will also draw some other useful
distinctions, like the distinction between obligatory and non-obligatory
actions, and between permissible and impermissible actions. These
distinctions will be useful in the following section, when we discuss
the considerations that inform our moral judgments.</p>
<h3 id="options">Options</h3>
<p>Special obligations and constraints tell us what we should not do,
and sometimes, what we must do. Intrinsic goods tell us about things
that would be good, should they happen. But philosophers debate how much
good we are required to do.</p>

<br>
<div class="visionbox">
<legend class="visionboxlegend">
    <p><span><b>A Note on Wellbeing in Social Science</b></span></p>
</legend>
Philosophers continue to debate what wellbeing is, or what it means to live a good
life. However, over the years, researchers have developed ways to
approximate wellbeing for practical purposes. Psychologists, economists,
philanthropists, policy makers, and other professionals need–—at
least–—a working definition of wellbeing in order to study, measure, and
promote it. Here, we describe some common views of wellbeing and
illustrate how they are used in social science.</p>
<strong>Preference satisfaction.</strong>
Preference theorists view wellbeing as fulfilling desires or
satisfying preferences, even if doing so does not always induce
pleasure. Nonetheless, it remains an open question whether all desires
are tied to wellbeing or just certain kinds, like higher-order or
informed desires.<p>
<strong>Economics.</strong> <em>Standard economics</em> often uses
preference satisfaction theories to study wellbeing. Revealed
preferences can be observed by studying the decisions people make. If
people desire higher incomes, for example, economists can promote
wellbeing by researching the impact of different economic policies on
gross domestic product (<em>GDP</em>).<p>
<strong>Social Surveys and Psychology.</strong> Traditionally,
psychological surveys evaluate wellbeing in terms of <em>life
satisfaction</em>. Life satisfaction is a measure of people’s stated
preferences—preferences or thoughts that individuals outwardly
express—regarding how their lives are going for them. Life satisfaction
surveys typically focus on tangible characteristics of people’s lives,
such as financial security, health, and personal achievements. They are
well suited to understanding the effects of concrete economic factors,
such as income and education, on an individual’s psychological
wellbeing. For example, to promote wellbeing, psychologists might
research the effects of access to education on one’s ability to achieve
the goals they set for themselves.</p>
<strong>Hedonism.</strong>
Under hedonist theories of wellbeing, an individual’s wellbeing is
determined by their mental states. In particular, the balance of
positive mental states (like pleasure or happiness) over negative mental
states (like pain or suffering).<p>
<strong>Economics.</strong> <em>Welfare economics</em> uses hedonism to
evaluate the wellbeing of populations. To estimate gross national
happiness (<em>GNH</em>)—-an indicator of national welfare—-it considers
the effects of several factors from psychological wellbeing to
ecological diversity and resilience on individuals’ mental states.
Welfare economists might prefer this framework because it is more
holistic - it evaluates both material and non-material aspects of
everyday life as they contribute to national welfare.<p>
<strong>Social Surveys and Psychology.</strong> Many psychologists also
use hedonist theories to understand and promote wellbeing. They may work
to identify the emotional correlates of happiness through surveys that
measure people’s stated emotions – unlike life satisfaction surveys,
these surveys do not reveal mental states. They reveal emotions that
people <em>remember</em>, not emotions they currently
<em>experience</em>. Researchers continue to look for ways to directly
observe emotions as they are experienced. For example, some studies use
cell phone apps to periodically prompt participants to record their
current emotions. Such research tactics may provide us with more precise
measures of individuals’ overall <em>happiness</em> by evaluating the
emotional responses to their everyday experiences in near real-time.</p>
<strong>Objective goods.</strong>
Under objective goods theories, wellbeing is determined by a certain
number of observable factors, independent of individuals’ preferences or
experiences. There are multiple theories about what those factors may
be. One of the most widely supported theories is <em>human
flourishing</em>. Under this view, wellbeing is more than just the
balance of pleasure over suffering, or the fulfillment of one’s
preferences – “the good life” should be full, virtuous, and meaningful,
encapsulating psychological, hedonistic, and social wellbeing all at
once.<p>
<strong>Economics.</strong> In economics, the <em>capabilities
approach</em> defines wellbeing as having access to a set of
capabilities that allow one to live the kind of life they value. It
emphasizes two core ideas: <em>functionings</em> and
<em>capabilities</em>. Functionings include basic and complex human
needs, ranging from good health to meaningful relationships.
Capabilities refer to the ability people have to choose and achieve the
functionings they value – they may include being able to move freely or
participate in the political process. This approach has significantly
influenced human development indicators, such as the Human Development
Index (<em>HDI</em>) – it allows developmental economists to measure and
compare wellbeing across different populations while also evaluating the
effectiveness of public policies.<p>
<strong>Psychology.</strong> Positive psychologists do not collapse
wellbeing into one dimension, rather, they argue for a
<em>psychologically rich life</em> – one that is happy, meaningful, and
engaging. Some psychologists use <em>PERMA theory</em> to evaluate
psychological richness, which considers five categories essential to
human flourishing: 1) experience of <em>positive emotions</em>, 2)
<em>engagement</em> with one’s interests, 3) maintenance of personal,
professional, and social <em>relationships</em>, 4) the search for
<em>meaning</em> or purpose, and 5) <em>accomplishments</em>, or the
pursuit of one’s goals. This framework is particularly useful in
evaluating wellbeing because it is universal–—it can be applied
cross-culturally—-and practical—-it can guide interventions aiming to
improve emotional wellbeing, social relationships, or activities that
provide a sense of meaning or accomplishment.<p>
While we don’t have a complete understanding of the nature of wellbeing,
we can use these theories as useful research tools. They can help us to
(a) understand how different factors contribute to wellbeing and (b)
evaluate the effectiveness of policies and other interventions aimed at
improving wellbeing.</p>
</div>

<p>While people disagree about which account of wellbeing is correct, most people agree that wellbeing is an important moral consideration. All else equal, taking actions that promote wellbeing is generally considered morally superior to taking actions that reduce wellbeing. However, the three theories of wellbeing have different practical implications. In the future, we are likely to interact with AI chatbots in a variety of ways; in particular, we might have close personal interactions with them that influence our decision-making. Different theories of wellbeing would suggest different goals for these AIs.</p>

<p><strong>Chatbots could prioritize pleasure.</strong> The hedonistic view suggests that wellbeing is primarily about experiencing pleasure and avoiding pain. This theory might recommend that AIs should be providing users with entertaining content that brings them pleasure or encouraging them to make decisions that maximize their balance of pain over pleasure over the long run. A common criticism is that this trades off with other goods considered valuable like friendship and knowledge. While this is sometimes true, these goals can also align. Providing content that is psychologically rich and supports users’ personal growth can contribute to a more fulfilling and meaningful life full of genuinely pleasurable experiences.</p>
<p><strong>Chatbots could prioritize preference satisfaction.</strong> The preference view suggests that wellbeing is about having preferences satisfied. Depending on which preferences were considered important, this theory would suggest different priorities for a hedonistic chatbot. Consider revealed preferences. One proxy for revealed preferences is user engagement. By continuing to engage with a chatbot, users are expressing their preference for seeing more of what they are getting, and increasing preference satisfaction might imply continuing to behave in certain ways.</p>
<p>It is important to be careful of this equivalence, though: prioritizing user engagement can lead to results like chatbots that engage people through unsavory means. Like engaging humans, chatbots could try to addict users by showing them streams of ephemeral content, creating an air of mystery and uncertainty, or act distant after being friendly to create a desire for continued friendliness that it can then satisfy. Such AIs might maximize engagement, but this may not be good for people’s wellbeing—even if they use the platform for more hours.</p>
<p><strong>Chatbots could promote objective goods.</strong> The objective goods account suggests that wellbeing is about promoting goods such as achievement, relationships, knowledge, beauty, happiness, and the like. An AI chatbot might aim to enhance users’ lives by encouraging them to complete projects, develop their rational capacities, and facilitate learning. The goal would be to make users more virtuous and encourage them to strive for the best version of themselves. This aligns with Aristotle’s theory of friendship, which emphasizes the pursuit of shared virtues and mutual growth, suggesting that such AIs might have meaningful friendships with humans.</p>
<p><strong>We might want to promote the welfare of AIs.</strong> In the future, we might also come to view AIs as able to have wellbeing. This might depend on our understanding of wellbeing. An AI might have preferences but not experience pleasure, which would mean it could have wellbeing according to preference satisfaction theorists but not hedonists. Future AIs may have wellbeing according to all three accounts of wellbeing. This would potentially require that we dramatically reassess our relationship with AIs.</p>
<p>In the following sections, we will focus on the different conceptions of wellbeing presented here, and explore what each theory implies about how we should embed ethics into AIs.</p>


<br>
<div class="visionbox">
<legend class="visionboxlegend">
<p><span><b>A Note on Measuring Wellbeing</b></span></p>
</legend>
While the philosophical
foundations of wellbeing are not settled, quantitative research fields
like public health and economics require the use of metrics in order to
evaluate, track, or compare the subject of study. Researchers use many
different metrics to measure wellbeing, but the most common are HALYs
and WELBYs.</p>
<strong>Health-adjusted life years
(HALYs).</strong>
A very common unit for measuring wellbeing is the health-adjusted
life year, or HALY. HALYs account for two factors: (1) the number of
years lived by an individual, group, or population (also called “life
years”), and (2) the health of those life years. Two common types of
HALYs are QALYs and DALYs.</p>
<strong>Quality-adjusted life years
(QALYs).</strong>
QALYs measure the number of years lived, adjusted according to
health. One year of life in perfect health is equivalent to 1 QALY. One
year of a less healthy life is worth between 0 and 1 QALYs. The value of
a life year depends on how severely the life is impacted by health
problems. For example, a year of life with asthma might be worth 0.9
QALYs, while a year of life with a missing limb might be worth about 0.7
QALYs.</p>
<strong>Disability-adjusted life
years (DALYs).</strong>
While QALYs measure years of life as impacted by health, DALYs
measure years of life lost, accounting for the impact of health. Whereas
1 QALY is equivalent to a year in perfect health, 1 DALY is equivalent
to the loss of a year in perfect health. A year of life with asthma
might be worth 0.1 DALYs, while a year of life with a missing limb might
be worth 0.3 DALYs.<p>
Note that increases in wellbeing are indicated by higher numbers of
QALYs but lower numbers of DALYs.<p>
Using HALYs to measure wellbeing has some limitations. First, the extent
to which different illnesses or injuries affect overall human health is
not clear. Losing a limb probably has a larger health impact than
getting asthma, but researchers must rely on subjective judgements to
assign precise values to each problem. Second—and perhaps more
importantly—HALYs measure the value of a span of life as it is impacted
by health alone. In reality, there are many factors that can impact the
value of life, like happiness, relationship quality, freedom, and a
sense of purpose. Perhaps a more useful measurement of wellbeing would
consider the effects of all of these factors, rather than health
alone.</p>
<strong>Wellbeing-adjusted life
years (WELBYs).</strong>
WELBYs have been developed to measure years of life as impacted by
overall wellbeing. One WELBY is equivalent to one year of life at
maximum wellbeing — namely, a life that is going as well as possible.
Wellbeing can be assessed using self-reported outcomes like affect, life
satisfaction, or degree of flourishing. There may also be some empirical
ways to assess wellbeing like cortisol levels, income, or ability.</p>
QALYs, DALYs, and WELBYs provide different approximations of wellbeing
that can be used to inform high-level decision-making and
policy-setting.</p>
</div>
<br>

<br>
<br>
<h3>References</h3>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-sebo2022chatbot" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] J.
Sebo, <span>“Op-ed: What should we do if a chatbot has thoughts and
feelings?”</span> [Online]. Available: <a
href="https://www.latimes.com/opinion/story/2022-06-16/artificial-intelligence-morals-ethics-sentience-thinking">https://www.latimes.com/opinion/story/2022-06-16/artificial-intelligence-morals-ethics-sentience-thinking</a></div>
</div>
<div id="ref-fisher2014metaethics" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] A.
Fisher, <em>Metaethics: An introduction</em>. Routledge, 2011.</div>
</div>
</div>

<h1 id="preferences">6.5 Preferences</h1>
<strong>Should we have
AIs satisfy people’s preferences?</strong>
A preference is a tendency to favor one thing over another. Someone
might prefer chocolate ice cream over vanilla ice cream, or they might
prefer that one party wins the election rather than another. These
preferences will influence actions. If someone prefers chocolate ice
cream over vanilla, they’re more likely to choose the former. Similarly,
if someone prefers one political party over another, they will likely
vote accordingly. In this manner, our preferences shape our behavior,
guiding us toward certain choices and actions over others. Preference is
similar to desire but always comparative. Someone might desire something
in itself—a new book, a vacation, or a delicious meal—but a preference
always involves a comparison between two or more alternatives.</p>
<strong>Overview.</strong>
In this section, we will consider whether preferences may have an
important role to play in machine ethics. In particular, if we want to
design an advanced AI system, the preferences of the people affected by
its decisions should plausibly help guide its decision-making. In fact,
some people (such as preference utilitarians) would say that preferences
are all we need. However, even if we don’t take this view, we should
recognize that preferences are still important. Preferences are further
explored in the next two chapters. First, in the chapter on ethics, we
discussed the idea that wellbeing might consist in having one’s
preferences satisfied. Second, in the chapter on utility functions, we
explain how preferences can be used to construct a utility function that
assigns numerical values to options and outcomes.<p>
To use preferences as the basis for increasing social wellbeing, we must
somehow combine the conflicting preferences of different people. We’ll
come to this later in this chapter, in a section on social welfare
functions. Before that, however, we must answer a more basic question:
what exactly does it mean to say that someone prefers one thing over
another? Moreover, we must decide why we think that satisfying someone’s
preferences is good for them and whether all kinds of preferences are
equally valuable. This section considers three different types of
preferences that could all potentially play a role in decision-making by
AI systems: revealed preferences, stated preferences, and idealized
preferences.</p>
<h2 id="revealed-preferences">6.5.1 Revealed Preferences</h2>
<p><strong>Preferences can be inferred from behavior.</strong> One set
of techniques for getting AI systems to behave as we want—inverse
reinforcement learning—is to have them deduce <em>revealed
preferences</em> from our behavior. We say that someone has a revealed
preference for X over Y if they choose X when Y is also available. In
this way, preference is revealed through choice. Consider, for example,
someone deciding what to have for dinner at a restaurant. They’re given
a menu, a list of various dishes they could order. The selection they
make from the menu is seen as a demonstration of their preference. If
they choose a grilled chicken salad over a steak or a plate of
spaghetti, they’ve just revealed their preference for grilled chicken
salad, at least in that specific context and time.<p>
While all theories of preferences agree that there is an important link
between preference and choice, the revealed preference account goes one
step further and claims that preference simply <em>is</em> choice.</p>
<strong>Revealed preferences
preserve autonomy.</strong>
One advantage of revealed preferences is that we don’t have to guess
what someone prefers. We can simply look at what they choose. In this
way, revealed preferences can help us avoid paternalism. Paternalism is
when leaders use their sovereignty to make decisions for their subjects,
limiting their freedom or choices, believing it is for the subjects’ own
good. However, we may think that typically people are themselves the
best judges of what is good for them. If so, then by relying on people’s
actions to reveal their preferences, we avoid the risk of
paternalism.</p>
<strong>However,
there are problems with revealed preferences.</strong>
The next few subsections will explore the challenges of
misinformation, weakness of will, and manipulation in the context of
revealed preferences. We will discuss how misinformation can lead to
choices that do not accurately reflect a person’s true preferences, and
how weakness of will can cause individuals to act against their genuine
preferences. Additionally, we will examine the various ways in which
preferences can be manipulated, ranging from advertising tactics to
extreme cases like cults, and the ethical implications of preference
manipulation.</p>
<h3 id="misinformation">Misinformation</h3>
<strong>Revealed
preferences can sometimes be based on misinformation.</strong>
If someone buys a used car that turns out to be defective, it doesn’t
mean they preferred a faulty car. They intended to buy a reliable car,
but due to a lack of information or deceit from the seller, they ended
up with a substandard one. Similarly, losing at chess doesn’t indicate a
preference for losing; rather, it’s an outcome of facing a stronger
player or making mistakes during the game. This means that we cannot
always infer someone’s preferences from the choices they make. Choice
does not reveal a preference between things as they actually are, but
between things as the person understands them. Therefore, we can’t rely
on revealed preferences if they are based on misinformation.</p>
<h3 id="weakness-of-will">Weakness of Will</h3>
<p><strong>Choices can be due to a lack of willpower rather than
considered preferences.</strong> Consider a smoker who wants to quit.
Each time they light a cigarette, they may be acting against their
genuine preference to stop smoking, succumbing instead to the power of
addiction. Therefore, it would be erroneous to conclude from their
behavior that they think that continuing to smoke would be best for
their wellbeing.</p>
<h3 id="manipulation">Manipulation</h3>
<p><strong>Revealed preferences can be manipulated in various
ways.</strong> Manipulations like persuasive advertising might
manipulate people into buying products they don’t actually want.
Similarly, revealed preferences might be the result of social pressure
rather than considered judgment, such as when buying a particular style
of clothing. In such cases, the manipulation may not be especially
malicious. At the other extreme, however, cults may brainwash their
members into preferring things they would not otherwise want, even
committing suicide. In the context of decision-making by advanced AI
systems, manipulation is a serious risk. An AI advisor system could
attempt to influence our decision-making for a variety of
reasons—perhaps its designer wants to promote specific products—by
presenting options in some particular way; for instance, by presenting a
list of options that looks exhaustive and excluding something it doesn’t
want us to consider.</p>
<strong>If
preference satisfaction is important, perhaps manipulation is
acceptable.</strong>
In at least some of these cases, it seems clear that preference
manipulation is bad. However, it may be less clear exactly why it is
bad. A natural answer is to say that people might be manipulated into
preferring things that are bad for them. Someone who is manipulated by
advertising into preferring junk food might thereby suffer negative
health consequences. However, if we think that wellbeing simply consists
in preference satisfaction, it doesn’t make sense to say that we might
prefer what is bad for us. On this account, having one’s preferences
satisfied is by definition good, regardless of whether those preferences
have been manipulated. This might lead us to think that what matters is
not (or at least not only) preference satisfaction, but happiness or
enjoyment. We’ll discuss this in the section on happiness.</p>
<strong>Disliking
manipulation suggests that wellbeing requires autonomy.</strong>
On the other hand, some may find that manipulation is bad even if the
person is manipulated into doing something that is good for them. For
example, suppose a doctor lies to her patient, telling him that unless
he loses weight, he will likely die soon. As a result, the patient
becomes greatly motivated to lose weight and successfully does so. This
provides a range of health benefits, even if his doctor never had any
reason to believe he would have died otherwise. If we think manipulation
is still bad, lack of enjoyment can’t be the whole story. This suggests
that we object to manipulation in part because it violates autonomy. We
might then think that autonomy—the ability to decide important matters
for oneself, without coercion—is objectively valuable regardless of what
the agent prefers. We’ll discuss this in the section on objective
goods.</p>
<h3 id="inverse-reinforcement-learning">Inverse Reinforcement
Learning</h3>
<strong>Inverse
Reinforcement Learning (IRL) relies on revealed preferences.</strong>
IRL is a powerful technique in the field of machine learning, which
focuses on extracting an agent’s objectives or preferences by observing
its behaviors. In more technical terms, IRL is about reverse engineering
the reward function—an internal ranking system that an agent uses to
assess the value of different outcomes—that the agent appears to be
optimizing, given a set of its actions and a model of the environment.
This technique can help ensure the alignment of AI system’s behaviors
with human values and preferences. However, leveraging revealed
preferences or observable choices of humans to train AI systems using
IRL poses significant challenges pertaining to AI safety.</p>
<strong>Using
revealed preferences as a training mechanism for IRL can be risky.</strong>
Reconsider the chess example: losing a game does not mean that we
prefer to lose. This interpretation could be a misrepresentation of the
player’s true preferences, potentially leading to undesirable outcomes.
Furthermore, extending observed preferences to unfamiliar situations
poses another hurdle. I may prefer to eat ice cream for dessert, but
that doesn’t mean I prefer to eat it for every meal. Similarly, I may
prefer to wear comfortable shoes for hiking, but that doesn’t mean I
want to wear them to a formal event. An AI system could inaccurately
extrapolate preferences from limited or context-specific data and
misapply these to other scenarios. Therefore, while revealed preferences
can offer significant insights for training AI, it is vital to
understand their limitations to safeguard the safety and efficiency of
AI systems.</p>
<strong>Summary.</strong>
Revealed preferences can be a powerful tool, as they allow an
individual’s actions to speak for themselves, reducing the risk of
paternalistic intervention. However, revealed preferences have inherent
shortcomings such as susceptibility to misinformation and manipulation,
which can mislead an AI system. This emphasizes the caution needed in
relying solely on revealed preferences for AI training. It underscores
the importance of supplementing revealed preferences with other methods
to ensure a more comprehensive and accurate understanding of a user’s
true preferences.</p>
<h2 id="stated-preferences">6.5.2 Stated Preferences</h2>
<p><strong>Preferences can be straightforwardly queried.</strong>
Another set of techniques for getting AI systems to behave as we
want—human supervision and feedback—rely on people to state their
preferences. A person’s <strong>stated preferences</strong> are the
preferences they would report if asked. For example, someone might ask a
friend which movie they want to see. Similarly, opinion polls might ask
people which party they would vote for. In both cases, we rely on what
people say as opposed to what they do, as was the case with revealed
preferences.<p>
Stated preferences overcome some of the difficulties with revealed
preferences. For example, stated preferences are less likely to be
subject to weakness of will: when we are further removed from the
situation, we are less inclined to fall for temptations. Therefore,
stated preferences are more likely to reflect our stable, long-term
interests.</p>
<strong>Stated preferences are
still imperfect.</strong>
Stated preferences do not overcome all difficulties with revealed
preferences. Stated preferences can still be manipulated. Further,
individuals might state preferences they believe to be socially
acceptable or admirable rather than what they truly prefer, particularly
when the topics are sensitive or controversial. Someone might overstate
their commitment to recycling in a survey, for instance, due to societal
pressure and norms around environmental responsibility.</p>
<h3 id="preference-accounting">Preference Accounting</h3>
<p>One set of problems with stated preferences concern which types of
preferences should be satisfied.</p>
<strong>First,
someone might never know their preference was fulfilled.</strong>
Suppose someone is on a trip far away. On a bus journey, they
exchange a few glances with a stranger whom they’ll never meet again.
Nevertheless they form the preference that the stranger’s life goes
well. Should this preference be taken into account? By assumption, they
will never be in a position to know whether the preference has been
satisfied or not. Therefore, they will never experience any of the
enjoyment associated with having their preference satisfied.</p>
<strong>Second,
we may or may not care about the preferences of the dead.</strong>
Suppose someone in the 18th century wanted to be famous long after
their death. Should such preferences count? Do they give us reason to
promote that person’s fame today? As in the previous example, the
satisfaction of such preferences can’t contribute any enjoyment to the
person’s life. Could it be that what we really care about is enjoyment
or happiness, and that preferences are a useful but imperfect guide
toward what we enjoy? We will return to this in the section on
happiness.</p>
<strong>Third,
preferences can be about others’ preferences being fulfilled.</strong>Suppose a parent prefers that their children’s preferences are
satisfied. Should this preference count, in addition to their children’s
preferences themselves? If we say yes, it follows that it is more
important to satisfy the preferences of those who have many people who
care for them than of those who do not. One might think that this is a
form of double counting, and claim that it is unfair to those with fewer
who care for them. On the other hand, one might take fairness to mean
that we should treat everyone’s preferences equally—including their
preferences about other people’s preferences.</p>
<strong>Fourth, preferences might
be harmful.</strong>
Suppose someone hates their neighbor, and prefers that they meet a
cruel fate. We might think that such malicious or harmful preferences
should not be included. On this view, we should only give weight to
preferences that are in some sense morally acceptable. However,
specifying exactly which preferences should be excluded may be
difficult. There are many cases where satisfying one person’s
preferences may negatively impact others. For example, whenever some
good is scarce, giving more of it to one person necessarily implies that
someone else will get less. Therefore, some more detailed account of
which preferences should be excluded is needed.</p>
<strong>Fifth, we
might be confused about our preferences.</strong>
Suppose a mobile app asks its users to choose between two privacy
settings upon installation: allowing the app to access their location
data all the time, or allowing the app to access their location data
only while they’re using the app. While these options seem
straightforward, the implications of this choice are much more complex.
To make a truly informed decision, users need to understand how location
data is used, how it can be combined with other data for targeted
advertising or profiling, what the privacy risks of data breaches are,
and how the app’s use of data aligns with local data protection laws.
However, we may not fully understand the alternatives we’re choosing
between.</p>
<strong>Sixth,
preferences can be inconsistent over time.</strong>
It could be that the choice we make will change us in some
fundamental way. When we undergo such transformative experiences <span
class="citation" data-cites="paul2014transformative">[1]</span>, our
preferences might change. Some claim that becoming a parent,
experiencing severe disability, or undergoing a religious conversion can
be like this. If this is right, how should we evaluate someone’s
preference between becoming a parent and not becoming a parent? Should
we focus on their current preferences, prior to making the choice, or on
the preferences they will develop after making the choice? In many cases
we may not even know what those new preferences will be.<p>
As technology advances, we may increasingly have the option to bring
about transformative experiences <span class="citation"
data-cites="paul2014transformative">[1]</span>. For this reason, it is
important that advanced AI systems tasked with decision-making are able
to reason appropriately about transformative experiences. For this, we
cannot rely on people’s stated preferences alone. By definition, stated
preferences can only reflect the person’s identity at the time. Of
course, people can try to take possible future developments into account
when they state their preferences. However, if they undergo a
transformative experience their preferences might change in ways they
cannot anticipate.</p>
<h3 id="human-supervision">Human Supervision</h3>
<p><strong>Stated preferences are used to train some AI
systems.</strong> In reinforcement learning with human feedback (RLHF),
standard reinforcement learning is augmented by human feedback from
people who rank the outputs of the system. In RLHF, humans evaluate and
rank the outputs of the system based on quality, usefulness, or another
defined criterion, providing valuable data to guide the system’s
iterative learning process. This ranking serves as a form of reward
function that the system uses to adjust its behavior and improve future
outputs.<p>
Imagine that we are teaching a robot how to make a cup of coffee. In the
RLHF process, the AI would attempt to output a cup of coffee, and then
we would provide feedback on how well it did. We could rank different
attempts and the robot would use this information to understand how to
make better coffee in the future. The feedback helps the robot learn not
just from its own trial and error, but also from our expertise and
judgment. However, this approach has some known difficulties.</p>
<strong>First,
as AI systems become more powerful, human feedback might be
infeasible.</strong>
As the problems AI solve become increasingly difficult, using human
supervision and feedback to ensure that those systems behave as desired
becomes difficult as well. In complex tasks like creating bug-free and
secure code, generating arguments that are not only persuasive but true,
or forecasting long-term implications of policy decisions, it may be too
time-consuming or even impossible for humans to evaluate and guide AI
behavior. Moreover, there are inherent risks from depending on human
reliability: human feedback may be systematically biased in various
ways. For example, inconvenient but true things may often be labeled as
bad. In addition to any bias, relying on human feedback will inevitably
mean some rate of human error.</p>
<strong>Second, RLHF
usually does not account for ethics.</strong>
Approaches based on human supervision and feedback are very broad.
These approaches primarily focus on task-specific performance, such as
generating accurate book summaries or bug-free code. However, these
task-specific evaluations may not necessarily translate into a
comprehensive understanding of ethical principles or human values.
Rather, they improve general capabilities since humans prefer smarter
models.<p>
Take, for instance, feedback on code generation. A human supervisor
might provide feedback based on the code’s functionality, efficiency, or
adherence to best programming practices. While this feedback helps in
creating better code, it doesn’t necessarily guide the AI system in
understanding broader ethical considerations, such as ensuring privacy
protection or maintaining fairness in algorithmic decisions.
Specifically, while RLHF is effective for improving AI performance in
specific tasks, it does not inherently equip AI systems with what’s
needed to grapple with moral questions. This gap underscores the need
for specific machine ethics research.</p>
<strong>Summary.</strong>
We’ve seen that stated preferences have certain advantages over
revealed preferences. However, stated preferences still have issues of
their own. It may not be clear how we should account for all different
kinds of preferences, such as ones that are only satisfied after the
person has died, or ones that fundamentally alter who we are. For these
reasons, we should be wary of using stated preferences alone to train
AI.</p>
<h2 id="idealized-preferences">6.5.3 Idealized Preferences</h2>
<p><strong>We could idealize preferences to avoid problems like weakness
of will.</strong> A third approach to getting AI systems to behave as we
want is to make them able to infer what we would prefer if our
preferences weren’t subject to the various distorting forces we’ve come
across. Someone’s <strong>idealized preferences</strong> are the
preferences they would have if they were suitably informed. Idealized
preferences avoid many of the problems of both revealed preferences and
stated preferences. Idealized preferences would not be based on false
beliefs, nor would they be subject to weakness of will, manipulation, or
framing effects. This makes it clearer how idealized preferences might
be linked to wellbeing, and therefore something we might ask an AI
system to implement.</p>
<strong>It is unclear how we
idealize preferences.</strong>
What exactly do we need to do to figure out what someone’s idealized
preferences are, based on their revealed preferences or their stated
preferences? It’s clear that the idealized preferences should not be
based on any false beliefs. We might imagine a person’s idealized
preferences as ones they would have if they fully grasped the options
they faced and were able to think through the situation in great detail.
However, this description is rather vague. It may be that it doesn’t
uniquely narrow down a set of idealized preferences. That is, there may
be multiple different ways of idealizing someone’s preferences, each of
which is one possible way that the idealized deliberation could go. If
so, idealized preferences may not help us decide what to do in such
cases.<p>
Additionally, some may argue that in addition to removing any dependence
on false belief or other misapprehensions, idealized preferences should
also take moral considerations into account. For example, perhaps
malicious preferences of the kind discussed earlier would not remain
after idealization. These may not be insurmountable problems for the
view that advanced AI systems should be tasked with satisfying people’s
idealized preferences. However, it shows that the view stands in need of
further elaboration, and that different people may disagree over what
exactly should go into the idealization procedure.</p>
<strong>We might think
that preferences are pointless.</strong>
Suppose someone’s only preference, even after idealization, is to
count the blades of grass on some lawn. This preference may strike us as
valueless, even if we suppose that the person in question derives great
enjoyment from the satisfaction of their preferences. It is unclear
whether such preferences should be taken into account. The example may
seem far-fetched, but it raises the question of whether preferences need
to meet some additional criteria in order to carry weight. Perhaps
preferences, at least in part, must be aimed at some worthy goal in
order to count. If so, we might be drawn toward an objective goods view
of wellbeing, according to which achievements are important objective
goods.<p>
On the other hand, we may think that judging certain preferences as
lacking value reveals an objectionable form of elitism. It is unfair to
impose our own judgments of what is valuable on other people using
hypothetical thought experiments, especially when we know their actual
preferences. Perhaps we should simply let people pursue their own
conception of what is valuable.</p>
<strong>We might
disagree with our idealized preferences.</strong>
Suppose someone mainly listens to country music, but it turns out
that their idealized preference is to listen to opera. When they
themselves actually listen to opera music, they have a miserable
experience. It seems unlikely that we should insist that listening to
opera is, in fact, good for them despite the absence of any enjoyment.
This gives rise to an elitism objection like before. If they don’t enjoy
satisfying their idealized preferences, why should those preferences be
imposed on them? This might lead us to think that what ultimately
matters is enjoyment or happiness, rather than preference satisfaction.
Alternatively, it might lead us to conclude that autonomy matters in
addition to preference satisfaction. If idealized preferences are
imposed on someone who would in fact rather choose contrary to those
idealized preferences, this would violate their autonomy.<p>
One might think that with the correct idealization procedure, this could
never happen. That is, whatever the idealization procedure does––remove
false beliefs and other misconceptions, increase awareness and
understanding––it should never result in anything so alien that the
actual person would not enjoy it. On the other hand, it’s difficult to
know exactly how much our preferences would change when idealized.
Perhaps removing false beliefs and acquiring detailed understanding of
the options would be a transformative experience that fundamentally
alters our preferences. If so, idealized preferences may well be so
alien from the person’s actual preferences that they would not enjoy
having them satisfied.</p>
<h3 id="ai-ideal-advisor">AI Ideal Advisor</h3>
<strong>One
potential application of idealized preferences is the AI ideal
advisor.</strong>
Suppose someone who hates exploitation and takes serious
inconvenience to avoid emissions would ideally want to buy food that has
been ethically produced, but does not realize that some of their
groceries are unethically produced. An AI ideal advisor would be
equipped with detailed real-world knowledge, such as the details of
supply chains, that could help them make this decision. In addition to
providing factual information, the AI ideal advisor would be
disinterested: it wouldn’t favor any specific entity, object, or course
of action solely due to their particular qualities (such as nationality
or brand), unless explicitly directed to do so. It would also be
dispassionate, meaning that it wouldn’t let its advice be swayed by
emotion. Finally, it would be consistent, applying the same set of moral
principles across all situations <span class="citation"
data-cites="giubilini2018artificial">[2]</span>.<p>
Such an AI ideal advisor could possibly help us better satisfy the moral
preferences we already have. Something close to the AI ideal advisor has
previously been discussed in the context of AI safety under the names of
“coherent extrapolated volition” and “indirect normativity.” In all
cases, the fundamental idea is to take an agent’s actual preferences,
idealize them in certain ways, and then use the result to guide
decision-making by advanced AI systems. Of course, having such an
advisor requires that we solve many of the challenges that we presented
in the Single Agent Safety chapter, as well as settle on a clear way to identify and idealize
individual preferences.</p>
<strong>Summary.</strong>
Idealized preferences overcome many of the difficulties of revealed
and stated preferences. Because idealized preferences are free from the
misconceptions that may affect these other types of preferences, they
are more plausibly ones that we would want an AI system to satisfy.
However, figuring out what people’s preferences would in fact be after
idealization can be difficult. Moreover, it could be that the
preferences are without value even after idealization, or that the
actual person would not appreciate having their idealized preferences
satisfied. An AI ideal advisor might be difficult to create, but sounds
highly appealing.</p>
<h3 id="conclusions-about-preferences">Conclusions About
Preferences</h3>
<p><strong>Preferences seem relevant to wellbeing—but we don’t know
which ones.</strong> The preferences people reveal through choice often
provide evidence about what is good for them, but they can be distorted
by misinformation, manipulation, and other factors. In some cases,
people’s stated preferences may be a better guide to what is good for
them, though it is not always clear how to account for stated
preferences. If we are looking for a notion of preference that plausibly
captures what is good for someone, idealized preferences are a better
bet. However, it can be difficult to figure out what someone’s idealized
preferences would be. It seems, then, that preferences-—while important
to wellbeing and useful to train AI in accordance with human values—are
not a sufficient basis for a comprehensive account of machine
ethics.</p>
<strong>Moving from preferences
to happiness.</strong>
People’s preferences can be easily influenced by external factors.
For instance, individuals may be swayed by persuasive advertising and
purchase products that do not truly fulfill their needs or bring them
lasting happiness. Similarly, social media platforms often present an
idealized version of other people’s lives, leading many to compare
themselves to others and feel inadequate or unhappy with their own
circumstances. Inconsistent preferences pose a challenge for the
preference satisfaction view, as discussed in the previous
section.<p>
One way of ensuring people’s preferences reflect their wellbeing is to
use idealized preferences. The preferences that people would have if
they were suitably informed and rational would be well-defined and
aligned with their interests. However, as we explored in the previous
section, this view faces another problem: if satisfying someone’s
idealized preferences does not bring them any pleasure or enjoyment, why
should it matter that they are satisfied?<p>
This objection suggests that what we ultimately care about is happiness
rather than preference satisfaction. If so, we should use a framework
that places happiness front and center. By instructing an AI system to
increase happiness, we might aim to overcome the human biases and
limitations that stop us from pursuing our happiness and enable the
system to make decisions that have a positive impact on overall
wellbeing.</p>

<br>
<br>
<h3>References</h3>

<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-paul2014transformative" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] L.
A. Paul, <em>Transformative experience</em>. in Oxford scholarship
online. Oxford University Press, 2014. Available: <a
href="https://books.google.com.au/books?id=zIXjBAAAQBAJ">https://books.google.com.au/books?id=zIXjBAAAQBAJ</a></div>
</div>
<div id="ref-giubilini2018artificial" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] A.
Guibilini, <span>“The artificial moral advisor. The <span>‘ideal
observer’</span> meets artificial intelligence,”</span> <em>Philosophy
&amp; Technology</em>, 2018, doi: <a
href="https://doi.org/10.1007/s13347-017-0285-z">https://doi.org/10.1007/s13347-017-0285-z</a>.</div>
</div>
</div>
