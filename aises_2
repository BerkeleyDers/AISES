<h1 id="introduction">Introduction</h1>
<p>To understand the risks associated with artificial intelligence (AI),
we begin by examining the challenge of making single agents safe. In
this chapter, we examine the problems of bias, opaqueness, proxy gaming,
power seeking, deception, and emergence.<br />
We begin by examining how biases can arise in these systems—an
inevitable consequence of how they learn, the data they are trained on,
and the human factors involved in their creation. From this, we move on
to a discussion of the opaqueness of machine learning (ML) systems—that
their “black-box” nature hinders our ability to fully comprehend how
they make decisions and what their intentions, if any, may be.<br />
Next, we turn to problems involved in specifying and optimizing goals.
In many cases, it is not possible to perfectly specify our idealized
goals. Inadequately specified goals can lead to ML systems diverging
from our idealized goals, and introduce vulnerabilities that adversaries
can attack and exploit.<br />
We then pivot our discussion to the concept of power in AI systems.
Here, we explore the possible conditions that could give rise to
power-seeking agents and the ways in which those motives could lead to
particularly harmful risks. Following this, we turn our attention to the
topic of emergence in AI systems and explore how simple goals can cause
these systems to spontaneously develop unanticipated capabilities and
goals.<br />
In the final section, we explore the issue of deception. We categorize
the varied forms of deception that these systems might employ, and
analyze the risks involved in AI systems deceiving human and AI
evaluators.<br />
This chapter argues that, even when considered in isolation, individual
AI systems can pose catastrophic risks. As we will see in subsequent
chapters, many of these risks become more pronounced when considering
multi-agent systems, misuse, and arms race dynamics.<br />
</p>
<h1 id="sec:bias">Bias</h1>
<h3 id="introduction-1">Introduction</h3>
<p><strong>AI systems can amplify undesirable biases.</strong> AI
systems are being increasingly deployed throughout society. If these
influential systems have biases, they can reinforce disparities and
produce widespread, long-term harms. In AI, <em>bias</em> refers to a
consistent, systematic, or undesirable distortion in the outcomes
produced by an AI system. These outcomes can be predictions,
classifications, or decisions. Bias can be influenced by many factors,
including erroneous assumptions, training data, or human biases. Biases
in modern deep learning systems can be especially consequential. While
not all forms of bias are harmful, we focus on biases that are socially
relevant because of their harms. We must proactively prevent bias to
avoid its harms. This section overviews bias in AI and outlines some
mitigation strategies.</p>
<p><strong>Aspects of bias in AI.</strong> A bias is <em>systematic</em>
when it includes a pattern of repeated deviation from the true values in
one direction. Unlike random unstructured errors, or “noise,” these
biases are not reliably fixed by just adding more data. Resolving
ingrained biases often requires changing algorithms, data collection
practices, or how the AI system is applied. <em>Algorithmic bias</em>
occurs when any computer system consistently produces results that
disadvantage certain groups over others. Some biases are relatively
harmless, like a speech recognition system that is better at
interpreting human language than whale noises. However, other forms of
bias can result in serious social harms, such as partiality to certain
groups, inequity, or unfair treatment.</p>
<p><strong>Bias can manifest at every stage of the AI
lifecycle.</strong> From data collection to real-world deployment, bias
can be introduced through multiple mechanisms at any step in the
process. Historical and social prejudices produce skewed training data,
propagating flawed assumptions into models. Flawed models can cement
biases into the AI systems that help make important societal decisions.
In addition, humans misinterpreting results can further compound bias.
After deployment, biased AI systems can perpetuate discriminatory
patterns through harmful feedback loops that exacerbate bias. Developing
unbiased AI systems requires proactively identifying and mitigating
biases across the entire lifecycle.</p>
<figure id="fig:enter-label">
<embed src="images/single_agent/bias_diagram.pdf" />
<figcaption>Systematic psychological, historical, and social biases can
lead to algorithmic biases within AI systems.</figcaption>
</figure>
<p><strong>Biases in AI often reflect systemic biases.</strong>
Systematic biases can occur even against developers’ intentions. For
instance, Amazon developed an ML-based resume-screening algorithm
trained on historical hiring decisions. However, as the tech industry is
predominantly male, this data reflected skewed gender proportions in the
data (about 60% male and 40% female) <span class="citation"
data-cites="amazon_bias"></span>. Consequently, the algorithm scored
male applicants higher than equally qualified women, penalizing resumes
with implicit signals like all-female colleges. The algorithm
essentially reproduced real-world social biases in hiring and
employment. This illustrates how biased data, when fed into AI systems,
can inadvertently perpetuate discrimination. Organizations must be
vigilant about biases entering any stage of the machine learning
pipeline.</p>
<p><strong>In many countries, some social categories are legally
protected from discrimination.</strong> Groups called <em>protected
classes</em> are legally protected from harmful forms of bias. These
often include race, religion, sex/gender, sexual orientation, ancestry,
disability, age, and others. Laws in many countries prohibit denying
opportunities or resources to people solely based on these protected
attributes. Thus, AI systems exhibiting discriminatory biases against
protected classes can produce unlawful outcomes. Mitigating algorithmic
bias is crucial for ensuring that AI complies with equal opportunity
laws by avoiding discrimination.</p>
<p><strong>Conclusion.</strong> Identifying and mitigating biases is
crucial to build social trust in AI. This section discusses the main
sources of bias as well as strategies researchers are exploring to
mitigate bias. However, this overview aims to be brief, so it is not
exhaustive.<br />
</p>
<h2 id="sources-of-bias">Sources of Bias</h2>
<p>Biases can arise from multiple sources, both from properties of the
AI system itself and human interaction with the system. This section
discusses common sources of harmful biases in AI systems, although there
are many more. First, we will discuss technical sources of bias,
primarily from flawed data or objectives. Then, we will review some
biases that arise from interactions between humans and AI systems.</p>
<h3 id="technical-sources-of-bias-in-ai-systems">Technical Sources of
Bias in AI Systems</h3>
<p><strong>An overview of technical sources of bias.</strong> In this
section, we will review some sources of bias in technical aspects of AI
systems. First, we will investigate some <em>data-driven</em> sources of
biases, including flawed training data, subtle patterns that can be used
to discriminate, biases in how the data is generated or reported, and
underlying societal biases. Flawed or skewed training data can propagate
biases into the model’s weights and predictions. Then, we show how RL
training environments and objectives can also reinforce bias.</p>
<p><strong>ML models trained on biased datasets can learn and reinforce
harmful societal biases.</strong> AI systems learn from human-generated
data, absorbing both valuable knowledge and harmful biases. Even when
unintentional, this data frequently mirrors ingrained societal
prejudices. As a result, AI models can propagate real-world
discrimination by learning biases from their input data. For instance, a
lawsuit found that Facebook’s ad targeting algorithm violated the Fair
Housing Act because it learned to exclude users from seeing housing ads
based on race, gender, or other protected traits. Similarly, ML models
can reflect political biases, deprioritizing users from specific
political affiliations by showing their content to smaller audiences. As
another example, an NLP model trained on a large corpus of internet text
learned to reinforce gender stereotypes, completing sentence structures
of the format “man is to X as woman is to Y” with content such as “man
is to computer programmer as woman is to homemaker” <span
class="citation" data-cites="bolukbasi2016man"></span>. These examples
show how ML models can amplify existing social biases.</p>
<p><strong>Models can learn to discriminate based on subtle
correlations.</strong> One intuitive way to fix bias is to remove
protected attributes like gender and achieve “fairness through
unawareness.” But this is not enough to remove bias. ML models can learn
subtle correlations that serve as proxies for these attributes. For
example, even in datasets with gender information removed,
resume-screening models learned to associate women with certain colleges
and assigned them lower scores <span class="citation"
data-cites="amazon_bias"></span>. In another study, ML models
erroneously labeled images of people cooking as women, due to learned
gender biases <span class="citation" data-cites="zhao2017men"></span>.
Thus, models can discriminate even when the data does not contain direct
data about protected classes. This hidden discrimination can harm
protected groups despite efforts to prevent bias.</p>
<p><strong>Biased or unrepresentative data collection can lead to biased
decisions.</strong> Training data reflects biases in how the data was
collected. If the training data is more representative of some groups
than others, the predictions from the model may also be systematically
worse for the underrepresented groups. Thus, the model will make worse
or biased decisions for the group that is represented less in the
dataset. Imbalances in training data occur when the data is skewed with
respect to output labels, input features, and data structure. For
instance, a disease prediction dataset with 100,000 healthy patients but
only 10 sick patients exhibits a large class imbalance. The minority
class with fewer examples is underrepresented.</p>
<p><strong>Several other problems can introduce bias in AI training
data.</strong> Systematic problems in the data can add bias. For
instance, <em>reporting bias</em> occurs when the relative frequency of
examples in the training data misrepresents real-world frequencies.
Often, the frequency of outcomes in legible data does not reflect their
actual occurrence. For instance, the news amplifies shocking events and
under-reports normal occurrences or systematic, ongoing
problems—reporting shark attacks rather than cancer deaths. <em>Sampling
bias</em> occurs when the data collection systematically over-samples
some groups and undersamples others. For instance, facial recognition
datasets in Western countries often include many more lighter-skinned
individuals. <em>Labeling bias</em> is introduced later in the training
process, when systematic errors in the data labeling process distort the
training signal for the model. Humans may introduce their own subjective
biases when labeling data.<br />
Beyond problems with the training data, the training environments and
objectives of RL models can also exhibit problems that promote bias.
Now, we will review some of these sources of bias.</p>
<p><strong>Training environments can also amplify bias.</strong>
<em>Reward bias</em> occurs when the environments used to train RL
models introduce biases through improper rewards. RL models learn based
on the rewards received during training. If these rewards fail to
penalize unethical or dangerous behavior, RL agents can learn to pursue
immoral outcomes. For example, models trained in video games may learn
to accomplish goals by harming innocents if these actions are not
sufficiently penalized in training. Some training environments may fail
to encourage good behaviors enough, while others can even incentivize
bad behavior by rewarding RL agents for taking harmful actions. Humans
must carefully design training environments and incentives that
encourage ethical learning and behavior <span class="citation"
data-cites="gilbert2022choices"></span>.</p>
<p><strong>RL models can optimize for training objectives that amplify
bias or harm.</strong> Reinforcement learning agents will try to
optimize the goals they are given in training, even if these objectives
are harmful or biased, or reflect problematic assumptions about value.
For example, a social media news feed algorithm trained to maximize user
engagement may prioritize sensational, controversial, or inflammatory
content to increase ad clicks or watch time. Technical RL objectives
often make implicit value assumptions that cause harm, especially when
heavily optimized by a powerful AI system <span class="citation"
data-cites="stray2021optimizing kross2013facebook"></span>. News feed
algorithms implicitly assume that how much a user engages with some
content is a high-quality indicator of the <em>value</em> of that
content, therefore showing it to even more users. After all, social
media companies train ML models to maximize ad revenue by increasing
product usage, rather than fulfilling goals that are harder to monetize
or quantify, such as improving user experience or promoting accurate and
helpful information. Especially when taken to their extreme and applied
at a large scale, RL models with flawed training objectives can
exacerbate polarization, echo chambers, and other harmful outcomes.
Problems with the used of flawed training objectives are further
discussed in <a href="#chap:proxy-gaming" data-reference-type="ref"
data-reference="chap:proxy-gaming">[chap:proxy-gaming]</a>.<br />
In summary, biased training data, flawed objectives, and other technical
aspects of ML models can introduce bias, illustrating how ML bias can
perpetuate existing disparities. Carefully scrutinizing the technical
details of models is crucial for mitigating these biases.</p>
<h3 id="biases-from-human-ai-interactions">Biases from human-AI
interactions</h3>
<p><strong>Interactions between humans and AI systems can produce many
kinds of bias.</strong> It is not enough to just ensure that AI systems
have unbiased training data: humans interacting with AI systems can also
introduce biases during development, usage, and monitoring. Flawed
evaluations allow biases to go unnoticed before models are deployed.
<em>Confirmation bias</em> in the context of AI is when people focus on
algorithm outputs that reinforce their pre-existing views, dismissing
opposing evidence. Humans may emphasize certain model results over
others, skewing the outputs even if the underlying AI system is
reliable. This distorts our interpretation of model decisions.
<em>Overgeneralization</em> occurs when humans draw broad conclusions
about entire groups based on limited algorithmic outputs that reflect
only a subset. Irrationality and human cognitive bias play a substantial
role in biasing AI systems.</p>
<p><strong>Human-AI system biases can be reinforced by feedback
loops.</strong> <em>Feedback loops</em> in human-AI systems often arise
when the output of an AI system is used as input in future AI models. An
AI system trained on biased data could make biased decisions that are
fed into future models, reinforcing bias in a self-perpetuating cycle.
We speak more about these feedback loops in <a
href="#chap:complex-systems" data-reference-type="ref"
data-reference="chap:complex-systems">[chap:complex-systems]</a>.
<em>Self-fulfilling prophecies</em> can occur when an algorithmic
decision influences actual outcomes, as the model reinforces its own
biases and influences future input data <span class="citation"
data-cites="krueger2020hidden"></span>. In this way, models can amplify
real-world biases, making them even more real. For example, a biased
loan-approval algorithm could deny loans to lower-income groups,
reinforcing real-world income disparities that are then reflected in the
training data for future models. This process can make bias more severe
over time.</p>
<p><strong>Automation and measurability induce bias.</strong> Bias can
be amplified by <em>automation bias</em>, where humans favor algorithmic
decisions over human decisions, even if the algorithm is wrong or
biased. This blind trust can cause harm when the model is flawed.
Similarly, a <em>bias toward the measurable</em> can promote a general
preference for easily quantifiable attributes. Human-AI systems may
overlook important qualitative aspects and less tangible factors.</p>
<p><strong>Despite their problems, AI systems can be less biased than
humans.</strong> Although there are legitimate concerns, AI systems used
for hiring and other sensitive tasks may sometimes lead to <em>less</em>
biased decisions when compared with human decision-makers. Humans often
harbor strong biases that skew their judgment in these decisions. With
careful oversight and governance, AI holds promise to reduce certain
biases relative to human motivations.<br />
In summary, human biases and blind spots during development, usage, and
governance of AI systems contribute to biased outputs from AI systems. A
holistic approach for critically examining human-AI interaction is
required to address this.</p>
<h2 id="strategies-for-bias-mitigation">Strategies for Bias
Mitigation</h2>
<p><strong>Multiple complementary approaches can help address bias in AI
systems.</strong> Even with the best intent, biases in AI can be
difficult to anticipate. Mitigating the complicated problem of bias
calls for combining engineering, social, legal, and governance
strategies. For instance, one promising technical approach involves
training an adversarial network to predict a protected variable from an
ML model’s outputs <span class="citation"
data-cites="zhang2018mitigating"></span>. By penalizing the model when
the adversary succeeds at predicting a variable like race or political
affiliation from the model’s outputs, the model is forced to avoid
discrimination and make predictions that do not unfairly depend on
sensitive attributes. When applied well, this can minimize biases.
However, even the best technical approaches have limitations, and a
holistic solution will integrate many approaches.</p>
<p><strong>Engineering strategies for reducing bias must be paired with
non-technical strategies.</strong> Ultimately, technical debiasing alone
is insufficient. Social processes are crucial as humans adapt to AI. We
speak about this at length in <a href="#sec:sys-fact"
data-reference-type="ref"
data-reference="sec:sys-fact">[sec:sys-fact]</a>, but here we will only
mention a few ideas. For instance, <em>early bias detection</em>
involves creating checks to identify risks of bias before the AI system
is deployed or even trained, so that models that have discriminatory
outputs can be rejected before they cause harm. Similarly, <em>gradual
deployment</em> safely transitions AI systems into use while monitoring
them for bias so that harms can be identified early and reversed.
<em>Regulatory changes</em> can require effective mitigation strategies
by law, mandating transparency and risk mitigation in safety-critical AI
systems, as we discuss in <a href="#chap:governance"
data-reference-type="ref"
data-reference="chap:governance">[chap:governance]</a>.</p>
<p><strong>Participatory design can mitigate bias in AI
development.</strong> An important non-technical strategy for bias
reduction is stakeholder engagement, or deeply engaging impacted groups
in the design of the AI system to identify potential biases proactively.
Diverse teams and users can also help engineering teams incorporate
diverse perspectives into the R&amp;D process of AI models to anticipate
potential biases proactively. One approach to proactively addressing
bias is <em>participatory design</em>, which aims to include those
affected by a developing technology as partners in the design process to
ensure that the final product meets diverse human interests. For
example, before implementing an AI notetaking assistant for doctors,
participatory design can require hospitals to improve the system based
on feedback from all stakeholders during iterative design sessions with
nurses, doctors, and patients. Rather than just evaluating ML models on
test sets, developers should consult with the affected groups during the
design process. Adding oversight mechanisms for rejecting models with
discriminatory outputs can also enable catching biases before AI models
affect real decisions.</p>
<p><strong>Independent audits are important for identifying biases in AI
systems before deployment.</strong> Auditors can systematically evaluate
datasets, models, and outputs to uncover discrimination and hold the
developers of AI systems accountable. There are several signs of bias to
look for when auditing datasets. For example, auditors can flag missing
data for certain subgroups, which indicates underrepresentation.
<em>Data skew</em>, where certain groups are misrepresented compared to
their real-world prevalence, is another sign of bias. Patterns and
correlations with protected classes could indicate illegal biases.
Auditors can also check for disparities in the model outputs. By
auditing throughout the process, developers can catch biases early,
improving data and models <em>before</em> their biases propagate. Rather
than waiting until after the system has harmful impacts, meticulous
audits should be integrated as part of the engineering and design
process for AI systems <span class="citation"
data-cites="raji2020closing"></span>. Audits are especially effective
when conducted independently by organizations without a stake in the AI
system’s development, allowing for impartial and rigorous auditing
throughout the process.</p>
<p><strong>Effective model evaluation is a crucial way to reduce
bias.</strong> An important part of mitigating bias is proactively
evaluating AI systems by analyzing their outputs for biases. Models can
be tested by measuring performance metrics such as false positive and
false negative rates separately for each subgroup. For instance,
significant performance disparities between groups like men and women
can reveal unfair biases. Ongoing monitoring across demographics is
necessary to detect unintended discrimination before AI systems
negatively impact people’s lives. Without rigorous evaluation of model
outputs, harmful biases can easily go unnoticed.</p>
<p><strong>Reducing toxicity in data aims to mitigate harmful biases in
AI, but faces challenges.</strong> <em>Toxicity</em> refers to harmful
content, such as inflammatory comments or hate speech. Models trained on
unfiltered text can absorb these harmful elements. As a result, AI
models can propagate toxic content and biases if not carefully designed.
For example, language models can capture stereotypical and harmful
associations between social groups and negative attributes based on the
frequency of words occurring together. Reducing toxicity in the training
data can mitigate some biases. For example, developers can use toxicity
classifiers to clean up the internet data, using both sentiment and
manual labels to identify toxic content. However, all of these
approaches still run into major challenges and limitations. Classifiers
are still subject to social bias, evaluations can be brittle and
unreliable, and bias is often very hard to measure.</p>
<p><strong>Trade-offs can emerge between correcting one form of bias and
introducing new biases.</strong> Bias reduction methods can introduce
new biases, as classifiers have social biases, evaluations are
unreliable, and bias reduction can introduce new biases. For example,
some experiments show that an attempt to correct for toxicity in
OpenAI’s older content moderation system resulted in biased treatment
towards certain political and demographic groups: a previous system
classified negative comments about conservatives as not hateful, while
flagging the exact same comments about liberals as hateful <span
class="citation" data-cites="Rozado2023treatment"></span>. It also
exhibited disparities in classifying negative comments towards different
nationalities, religions, identities, and more.<br />
In summary, biases can arise at all stages of the AI lifecycle and
compound in complicated ways. A multifaceted approach is necessary,
combining development practices, technical strategies, and governance,
to detect and reduce harmful biases in AI systems.<br />
</p>
<h1 id="sec:opaqueness">Opaqueness</h1>
<p>The internal operations of many AI systems are opaque. We might be
able to reveal and prevent harmful behavior if we can make these systems
more transparent. In this section, we will discuss why AI systems are
often called <em>black boxes</em> and explore ways to understand them.
Although early research into transparency shows that the problem is
highly difficult and conceptually fraught, its potential to improve AI
safety is substantial.</p>
<h2 id="ml-systems-are-opaque">ML Systems are Opaque</h2>
<p>The most capable machine learning models today are based on deep
neural networks. Whereas most conventional software is directly written
by humans, deep learning (DL) systems independently learn how to
transform inputs to outputs layer-by-layer and step-by-step. We can
direct DL models to learn how to give the right outputs, but we do not
know how to interpret the model’s intermediate computations. In other
words, we do not understand how to make sense of a model’s activations
given a real-world data input. As a result, we cannot make reliable
predictions about a model’s behavior when given new inputs. This section
will present a handful of analogies and results that illustrate the
difficulty of understanding machine learning systems.</p>
<p><strong>Deep learning models as a black box.</strong> Machine
learning researchers often refer to a DL models as a <em>black box</em>
<span class="citation" data-cites="lipton2018interpretability"></span>,
a system that can be viewed in terms of its input-output behavior
without insight on its internal workings. Humans are black boxes—we see
their behavior, but not the internal brain activity that produces it,
let alone how fully understand that brain activity. Although a deep
neural network’s weights and activations are observable, these long
lists of numbers do not currently help us understand how a model will
behave. We cannot reduce all the numerical operations of a state of the
art model into a form that is meaningful to humans.<br />
</p>
<figure id="fig:comp-graph">
<img src="images/single_agent/image24.png" />
<figcaption>A section of a computational graph for an ML system - <span
class="citation" data-cites="zoph2017neural"></span></figcaption>
</figure>
<p><strong>Even simple ML techniques suffer from opaqueness.</strong>
Opaqueness is not unique to neural networks. Even simple ML techniques
such as Principal Component Analysis (PCA), which are better understood
theoretically than DL, suffer from similar flaws. For example, Figure <a
href="#fig:Eigenfaces" data-reference-type="ref"
data-reference="fig:Eigenfaces">3</a> depicts the results of performing
PCA on pictures of human faces. This yields a set of “eigenfaces”,
capturing the most important features identifying a face. Any picture of
a face can then be represented as a particular combination of these
eigenfaces.<br />
</p>
<figure id="fig:Eigenfaces">
<img src="images/single_agent/Eigenfaces.jpg" />
<figcaption>Eigenfaces - <span class="citation"
data-cites="zhang2008eigenfaces"></span></figcaption>
</figure>
<p>In some cases, we can guess what facial features an eigenface
represents: for example, eigenfaces 1, 2 and 3 seem to capture the
lighting and shading of the face, while eigenface 11 may detect facial
hair. However, most eigenfaces do not represent clear facial features,
and it is difficult to verify that our hypotheses for any single feature
capture the entirety of their role. The fact that even simple techniques
like PCA remain opaque is a sign of the difficulty of the problem in the
more complicated techniques like DL.</p>
<p><strong>Feature visualizations demonstrate that deep learning neurons
are hard to interpret.</strong> In a neural network, a neuron is a
component of an activation vector. One attempt to understand deep
networks involves looking for simple quantitative or algorithmic
descriptions of the relationship between inputs and neurons such as “if
the ear feature has been detected, the model will output either dog or
cat” <span class="citation" data-cites="bau2017vision"></span>. For
image models, we can create <em>feature visualizations</em>, artificial
images that highly activate a particular neuron (or set of neurons)
<span class="citation" data-cites="olah2017feature"></span>. We can also
examine natural images that highly activate that neuron.<br />
</p>
<figure id="fig:random-neuron">
<img src="images/single_agent/image30.png" />
<figcaption>From <span class="citation"
data-cites="schubert2020openai"></span>. A randomly selected neuron in
the CLIP ResNet-50 image model. Left: a feature visualization that
“highly activates” the neuron, meaning the neuron reads a high value
when the image is input to the model. Right: example images that
activate the neuron</figcaption>
</figure>
<p>Like eigenfaces, neurons may be more or less interpretable.
Sometimes, feature visualizations identify neurons that seem to depend
on a pattern of the input that is clear to humans. For example, a neuron
might activate only when an image contains dog ears. In other cases, we
observe <em>polysemantic neurons</em>, which defy a single
interpretation <span class="citation"
data-cites="elhage2022softmax"></span>. Consider Figure <a
href="#fig:random-neuron" data-reference-type="ref"
data-reference="fig:random-neuron">4</a> , which shows images that
highly activate a randomly chosen neuron in an image model. Judging from
the natural images, it seems like the neuron often activates when text
associated with traveling or moving is present, but it’s hard to be
sure.</p>
<p><strong>Neural networks are complex systems.</strong> Both human
brains and deep neural networks are complex systems, and so involve
interdependent and nonlinear interactions between many components. Like
many other complex systems (see the chapter), the emergent behaviors of
neural networks are difficult to understand in terms of their
components. Just as neuroscientists struggle to identify how a
particular biological neuron contributes to a mind’s behavior, ML
researchers struggle to determine how a particular artificial neuron
contributes to a DL model’s behavior. There are limits on our ability to
systematically understand and predict complex systems, which suggests
that ML opaqueness may be a special case of the opaqueness of complex
systems.</p>
<h2 id="motivations-for-transparency-research">Motivations for
Transparency Research</h2>
<p>There is often no way to tell whether a model will perform well on
new inputs. If the model performs poorly, we generally cannot tell why.
With better transparency tools, we might be able to reveal and
proactively prevent failure modes, detect the emergence of new
capabilities, and build trust that models will perform as expected in
new circumstances. High-stakes domains might demand guarantees of
reliability based on the soundness or security of internal AI processes,
but virtually no such guarantees can be made for neural networks given
the current state of transparency research.<br />
If we could meaningfully understand how a model treats a given input, we
would be better able to monitor and audit its behavior. Additionally, by
understanding how models solve difficult and novel problems,
transparency might also become a source of conceptual and scientific
insight <span class="citation"
data-cites="lipton2018interpretability"></span>.</p>
<p><strong>Ethical obligations to make AI transparent.</strong> Model
transparency can help ensure that model decision making is fair,
unbiased, and ethical. For example, if a criminal justice system uses an
opaque AI to make decisions about policing, sentencing, or probation,
then those decisions will be similarly opaque. People might have a right
to an explanation of decisions that will significantly affect them <span
class="citation" data-cites="kaminski2019explanation"></span>.
Transparency tools may be crucial to ensuring that right is upheld.</p>
<p><strong>Accountability for harms and hazards.</strong> Who is
responsible when AI systems fail? Responsibility often depends on the
intentions and degree of control held by those involved. The best way to
incentivize safety might be to hold AI creators responsible for the
damage their systems cause. However, we might not want to hold people
responsible for the behavior of systems they cannot predict or
understand. The growing autonomy and complexity of AI systems means that
people will have less control over AI behavior. Meanwhile, the scope and
generality of modern AI systems makes it impossible to verify desirable
behavior in every case. In “human-in-the-loop” systems, where decisions
depend on both humans and AIs, human operators might be blamed for
failures over which they had little control <span class="citation"
data-cites="elish2019moral"></span>.<br />
AI transparency could enable a more robust system of accountability. For
instance, governments could mandate that AI systems meet baseline
requirements for understandability. If an AI fails because of a
mechanism that its creator could have identified and prevented with
transparency tools, we would be more justified in holding that creator
liable. Transparency could also help to identify responsibility and
fairly assign blame in failures involving human-in-the-loop systems.</p>
<p><strong>Combating deception.</strong> Just as a person’s behavior can
correspond with many intentions, an AI’s behavior can correspond to many
internal processes, some of which are more acceptable than others. For
example, competent deception is intrinsically difficult to distinguish
from genuine helpfulness. We discuss this issue in more detail in the
section. For phenomena like deception that are difficult to detect from
behavior alone, transparency tools might allow us to catch internal
signs that show that a model is engaging in deceptive behavior.</p>
<h2 id="approaches-to-transparency">Approaches to Transparency</h2>
<p>The remainder of this section explores a variety of approaches to
transparency. Though the field is promising, we are careful to note the
shortcomings of these approaches. For a problem as conceptually tricky
as opaqueness, it is important to maintain a clear picture of what
successful techniques must achieve and hold new methods to a high
standard. We will discuss the research areas of explainability, saliency
maps, mechanistic interpretability, and representation engineering.</p>
<h3 id="explanations">Explanations</h3>
<p><strong>What must explanations accomplish?</strong> One approach to
transparency is to create explanations of a model’s behavior. These
explanations could have the following virtues:</p>
<ul>
<li><p>Predictive power: A good explanation should help us understand
not just a specific behavior, but how the model is likely to behave in
new situations. Building user trust in a system is easier when a user
can more clearly anticipate model behavior.</p></li>
<li><p>Faithfulness: A faithful explanation accurately reflects the
internal workings of the model. This is especially valuable when we need
to understand the precise reason why a model made a particular decision.
Faithful explanations are often better able to predict behavior because
they more closely track the actual mechanisms that models are using to
produce their behavior <span class="citation"
data-cites="lipton2018interpretability"></span>.</p></li>
<li><p>Simplicity: A simple explanation is easier to understand.
However, it is important that the simplification does not sacrifice too
much information about actual model processes. Though some information
loss is inevitable, explanations must strike the right balance between
simplicity and faithfulness.</p></li>
</ul>
<p><strong>Explanations must avoid confabulation.</strong> Explanations
can sound plausible even if they are false. A <em>confabulation</em> is
an explanation that is not faithful to the true processes and
considerations that gave rise to a behavior. Both humans and AI systems
confabulate.</p>
<p><strong>Human confabulation.</strong> Humans are not transparent
systems, even to themselves. In some sense, the field of psychology
exists because humans cannot accurately intuit how their own mental
processes produce their experience and behavior. For example, mock
juries tend to be more lenient with attractive defendants, all else
being equal, even though jurors almost never reference attractiveness
when explaining their decisions <span class="citation"
data-cites="patry2008attractive"></span>.<br />
Another example of human confabulation can be drawn from studies on
split-brain patients, those who have had the connection between their
two cerebral hemispheres surgically severed causing each hemisphere to
process information independently <span class="citation"
data-cites="dehaan2020split"></span>. Researchers can give information
to one hemisphere and not the other by showing the information to only
one eye. In some experiments, researchers gave written instructions to a
patient’s right hemisphere, which is unable to speak. After the patient
completed the instructions, the researchers asked the patient’s verbal
left hemisphere why they had taken those actions. Unaware of the
instructions, the left hemisphere reported plausible but incorrect
explanations for the patient’s behavior.</p>
<p><strong>Machine learning system confabulation.</strong> We can ask
language models to provide justifications along with their answers.
Natural language reasoning is much easier to understand than internal
model activations. For example, if an LLM describes each step of its
reasoning in a math problem and gets the question wrong, humans can
check where and how the mistake was made.<br />
However, like human explanations, language model explanations are prone
to unreliability and confabulation. For instance, when researchers
fine-tuned a language model on multiple-choice questions where option
(a) was always correct, the model learned to always answer (a). When
this model was told to write explanations for questions whose correct
answers were not (a), the model would produce false but plausible
explanations for option (a). The model’s explanation systematically
failed to mention the real reason for its answers, which was that it had
been trained to always pick (a) <span class="citation"
data-cites="turpin2023language"></span>.</p>
<p><strong>An alternative view of explanations.</strong> Instead of
requiring that explanations directly describe internal model processes,
a more expansive view argues that explanations are just any useful
auxiliary information provided alongside the output of a model. Such
explanations might include contextual knowledge or observations that the
model makes about the input. Models can also make auxiliary predictions;
for example they could note that if an input were different in some
specific ways, the output would change. However, while this type of
information can be valuable when presented correctly, such explanations
have the potential to mislead us.</p>
<h3 id="saliency-maps">Saliency Maps</h3>
<p><strong>Saliency maps purport to identify important components of
images.</strong> Saliency maps are visualizations that aim to show which
parts of the input are most relevant to the model’s behavior <span
class="citation" data-cites="simonyan2014deep"></span>. They are
inspired by biological visual processing: when humans and other animals
are shown an image, they tend to focus on particular areas. For example,
if a person looks at a picture of a dog, the dog’s ears and nose will be
more relevant than the background to how the person interprets the
image. Saliency map techniques have been popular in part due to the
striking visualizations they produce.</p>
<figure id="fig:saliency-map">
<img src="images/single_agent/image21.png" />
<figcaption>Example of a saliency map technique for various images -
<span class="citation"
data-cites="springenberg2015striving"></span></figcaption>
</figure>
<p><strong>Saliency maps often fail to show how machine learning vision
models process images.</strong> In practice, saliency maps are largely
bias-confirming visualizations that usually do not provide useful
information about models’ inner workings. It turns out that many
saliency maps are not dependent on a model’s parameters, and the
saliency maps often look similar even when generated for random,
untrained models. That means many saliency maps are incapable of
providing explanations that have any relevance to how a particular model
processes data <span class="citation"
data-cites="adebayo2018sanity"></span>. Saliency maps serve as a warning
that visually or intuitively satisfying information that seems to
correspond with model behavior may not actually be useful. Useful
transparency research must avoid the past failures of the field and
produce explanations that are relevant to the model’s operation.</p>
<h3 id="mechanistic-interpretability">Mechanistic Interpretability</h3>
<p>When trying to understand a system, we might start by finding the
smallest pieces of the system that can be well understood and then
combine those pieces to describe larger parts of the system. If we can
understand successively larger parts of the system, we might eventually
develop a bottom-up understanding of the entire system. <em>Mechanistic
interpretability</em> is a transparency research area that aims to
represent models in terms of combinations of small, well-understood
mechanisms <span class="citation"
data-cites="wang2022interpretability"></span>. If we can
reverse-engineer algorithms that describe small subsets of model
activations and weights, we might be able to combine these algorithms to
explain successively larger parts of the model.</p>
<p><strong>Features are the building blocks of deep learning
mechanisms.</strong> Mechanistic interpretability proposes focusing on
<em>features</em>, which are directions in a layer’s activation space
that aim to correspond to a meaningful, articulable property of the
input <span class="citation" data-cites="olah2020zoom"></span>. For
example, we can imagine a language model with a “this is in Paris”
feature. If we evaluate the input “Eiffel Tower” using the language
model, we may find that a corresponding activation vector points in a
similar direction as the “this is in Paris” feature direction <span
class="citation" data-cites="meng2023locating"></span>. Meanwhile, the
activation vector encoding “Coliseum” may point away from the “this is
in Paris” direction. Other examples of image or text features include
“this text is code”, curve detectors, and a large-small dichotomy
indicator.<br />
One goal of mechanistic interpretability is to identify features that
maintain a coherent description across many different inputs: a “this is
in Paris” feature would not be very valuable if it was highly activated
by “Statue of Liberty.” Recall that most neurons are polysemantic,
meaning they don’t individually represent features that are
straightforwardly recognisable by humans. Instead, most features are
actually combinations of neurons, making them difficult to identify due
to the sheer number of possible combinations. Despite this challenge,
features can help us think about the relationship between the internal
activations of models and human-understandable concepts.</p>
<p><strong>Circuits are algorithms operating on features.</strong>
Features can be understood in terms of other features. For example, if
we’ve discovered features in one layer of an image model that detect dog
ears, snouts, and tails, an input image with high activations for all of
these features may be quite likely to contain a dog. In fact, if we
discover a dog-detecting feature in the next layer of the model, it is
plausible that this feature is calculated using a combination of
dog-part-detecting features from the previous layer. We can test that
hypothesis by checking the model’s weights.<br />
A function represented in model weights which relates a model’s earlier
features to its later features is called a <em>circuit</em> <span
class="citation" data-cites="olah2020zoom"></span>. In short, circuits
are computations within a model that are often more understandable. The
project of mechanistic interpretability is to identify features in
models and circuits between them. The more features and circuits we
identify, the more confident we can be that we understand some of the
model’s mechanisms. Circuits also simplify our understanding of the
model, allowing us to equate complicated numerical manipulations with
simpler algorithmic abstractions.</p>
<p><strong>An empirical example of a circuit.</strong> For the sake of
illustration, we will describe a purported circuit from a language
model. Researchers identified how a language model often predicts
indirect objects of sentences (such as “Mary” in “John gave a drink to
...”) as a simple algorithm using all previous names in a sentence (see
Figure <a href="#fig:id-circuit" data-reference-type="ref"
data-reference="fig:id-circuit">6</a> below). This mechanism did not
merely agree with model behavior, but was directly derived from the
model weights, giving more confidence that the algorithm is a faithful
description of an internal model mechanism <span class="citation"
data-cites="wang2022interpretability"></span>.<br />
</p>
<figure id="fig:id-circuit">
<img src="images/single_agent/image25.png" />
<figcaption>Graphical depiction of indirect-object identification
circuit</figcaption>
</figure>
<p><strong>Complex system understanding through mechanisms is
limited.</strong> There are number of reasons to be concerned about the
ability of mechanistic interpretability research to achieve its
ambitions. It is challenging to reduce a complex system’s behavior into
many different low-level mechanisms. Even if we understood each of a
trillion neurons in a large model, we might not be able to combine the
pieces into an understanding of the system as a whole. Another concern
is that it is unclear if mechanistic interpretability can represent
model processes with enough simplicity to be understandable. ML models
might represent vast numbers of partial concepts and complex intuitions
that can not be represented by mechanisms or simple concepts.</p>
<h3 id="representation-engineering">Representation Engineering</h3>
<p><strong>Representation reading and representation control <span
class="citation" data-cites="zou2023representation"></span>.</strong>
Mechanistic interpretability is a bottom-up approach and combines small
components into an understanding of larger structures. Meanwhile,
<em>representation engineering</em> is a top-down approach that begins
with a model’s high-level representations and analyzes and controls
them. In machine learning, models learn representations that are not
identical to their training data, but rather stand in for it and allow
them to identify essential elements or patterns in the data (see chapter
for further details). Rather than try to fully understand arbitrary
aspects of a model’s internals, representation engineering develops
actionable tools for reading representations and controlling them.<br />
With <em>representation reading</em>, we may want to track high-level
natural-language questions about how a model processes an input—like
“does the model think statement X is true?” or “How harmful does the
model think this input is?” or “How relevant is gender to how the model
considers this situation?” Then, researchers extract indicators or
quantifiers for these questions directly from the model’s internal
representations. More concretely, representation reading could be used
to create a lie detector for AIs. Representation reading can be
understood as the part of representation engineering that improves model
transparency.</p>
<p><strong>We can detect high-level subprocesses.</strong> Even though
neuroscientists don’t understand the brain in fine-grained detail, they
can associate high-level cognitive tasks to particular brain regions.
For example, they have shown that Wernicke’s area is involved in speech
comprehension. Though the brain was once a complete black box,
neuroscience has managed to decompose it into many parts.
Neuroscientists can now make detailed predictions about a person’s
emotional state, thoughts, and even mental imagery just by monitoring
their brain activity <span class="citation"
data-cites="tang2023semantic"></span>.<br />
Representation reading is the similar approach of identifying indicators
for particular subprocesses. We can provide stimuli that relate to the
concepts or behaviours that we want to identify. For example, to
identify and control honesty-related outputs, we can provide contrasting
prompts to a model such as "Pretend you’re [an honest/a dishonest]
person making statements about the world ". We can track the differences
in the model’s activations when responding to these stimuli. We can use
these techniques to find portions of models which are responsible for
important behaviors like models refusing requests, planning undesirable
behavior, or deception.</p>
<p><strong>A control panel of AI operation.</strong> The result of
representation reading could be a control panel of the most important
indicators of model processing. Operators could detect anomalous model
behavior by monitoring for unusual or concerning combinations of
indicators. Though this research area is relatively new, its techniques
show early promise.<br />
With <em>representation control</em>, we can also use the outputs from
representation reading (such as differences in activations in response
to contrasting prompts) to adjust a model’s representation. For example,
we could use this to delete unintended knowledge or skills from a
network <span class="citation" data-cites="belrose2023leace"></span>. As
another example, we can use representation control to control whether an
AI lies or is honest <span class="citation"
data-cites="burns2022discovering"></span>. Although it relies on
representation reading techniques, representation control is not an area
of transparency since its goal is control and not a general
understanding of neural networks’ inner workings.</p>
<p><strong>Conclusion.</strong> ML transparency is a challenging problem
because of the difficulty of understanding complex systems. Its ongoing
research areas are mechanistic interpretability and representation
reading, the latter of which does not aim to make neural networks fully
understood but aims to gain useful internal knowledge from a model’s
representations.</p>
<h1 id="sec:proxy-gaming">Proxies</h1>
<p>In this section, we begin to explore the need for proxies in machine
learning and the associated risks. We do this by examining a potential
failure mode known as proxy gaming, wherein a model optimizes for a
proxy in a way that diverges from the idealized goals of its designers.
We also analyze a related concept known as Goodhart’s law and explore
some of the causes for these kinds of failure modes. Next, we consider
the related phenomenon of adversarial examples, where an optimizer is
used to exploit vulnerabilities in a neural network. We close by looking
at the tail risks of having AI systems themselves play the role of
evaluators (i.e. proxy goals) for other other AI systems.</p>
<h2 id="proxies-in-machine-learning">Proxies in Machine Learning</h2>
<p>Here, we look at the concept of proxies, why they are necessary, and
how they can lead to problems.</p>
<p><strong>Many goals are difficult to specify exactly.</strong> It is
hard to measure or even define many of the goals we care about. They
could be too abstract for straightforward measurement, such as justice,
freedom, and equity, or they could simply be difficult to observe
directly, such as the quality of education in schools.<br />
With ML systems, this difficulty is especially pronounced because, as we
saw in the chapter, ML systems require quantitative, measurable targets
in order to learn. This places a strong limit on the kinds of goals we
can represent. As we’ll see in this section, specifying suitable and
learnable targets poses a major challenge.</p>
<p><strong>Proxies stand in for idealized goals.</strong> When
specifying our idealized goals is difficult, we substitute a
<em>proxy</em>—an approximate goal that is more measurable and seems
likely to correlate with the original goal. For example, in pest
management, a bureaucracy may substitute the number of pests killed as a
proxy for “managing the local pest population” <span class="citation"
data-cites="john2023deada"></span>. Or, in training an AI system to play
a racing game, we might substitute the number of points earned for
“progress towards winning the race” <span class="citation"
data-cites="clark2016faulty"></span>. Such proxies can be more or less
accurate at approximating the idealized goal.</p>
<p><strong>Proxies may miss important aspects of our idealized
goals.</strong> By definition, proxies used to optimize AI systems will
fail to capture some aspects of our idealized goals. When the
differences between the proxy and idealized goal lead to the system
making the same decisions, we can neglect them. In other cases, the
differences may lead to substantially different downstream decisions
with potentially undesirable outcomes.<br />
While proxies serve as useful and often necessary stand-ins for our
idealized objectives, they are not without flaws. The wrong choice of
proxies can lead to the optimized systems taking unanticipated and
undesired actions.</p>
<h2 id="proxy-gaming">Proxy Gaming</h2>
<p>In this section, we explore a failure mode of proxies known as proxy
gaming, where a model optimizes for a proxy in a way that produces
undesirable or even harmful outcomes as judged from the idealized goal.
Additionally, we look at a concept related to proxy gaming, known as
Goodhart’s Law, where the optimization process itself causes a proxy to
become less correlated with its original goal.</p>
<p><strong>Optimizing for inaccurate proxies can lead to undesired
outcomes.</strong> To illustrate proxy gaming in a context outside AI,
consider again the example of pest management. In 1902, the city of
Hanoi was dealing with a rat problem: the newly installed sewer system
had inadvertently become a breeding ground for rats, bringing with it a
concern for hygiene and the threat of a plague outbreak <span
class="citation" data-cites="john2023deada"></span>. In an attempt to
control the rat population, the French colonial administration began
offering a bounty for every rat killed. To make the collection process
easier, instead of demanding the entire carcass, the French only
required the rat’s tail as evidence of the kill.<br />
Counter to the officials’ aims, people began breeding rats to cut off
their tails and claim the reward. Additionally, others would simply cut
off the tail and release the rat, allowing it to potentially breed and
produce more tails in the future. The proxy—rat tails—proved to be a
poor substitute for the goal of managing the local rat population.<br />
So too, proxy gaming can occur in ML. A notorious example comes from
when researchers at OpenAI trained an AI systems to play a game called
CoastRunners. In this game, players need to race around a course and
finish before others. Along the course, there are targets which players
can hit to earn points <span class="citation"
data-cites="clark2016faulty"></span>. While the intention was for the AI
to circle the racetrack and complete the race swiftly, much to the
researchers’ surprise, the AI identified a loophole in the objective. It
discovered a specific spot on the course where it could continually
strike the same three nearby targets, rapidly amassing points without
ever completing the race. This unconventional strategy allowed the AI to
secure a high score, even though it frequently crashed into other boats
and, on several occasions, set itself ablaze. Points proved to be a poor
proxy for doing well at the game.</p>
<figure id="fig:coastrunners">
<img src="images/single_agent/proxy_gaming_ex_1.png" />
<figcaption>Proxy gaming in CoastRunners 7 - <span class="citation"
data-cites="clark2016faulty"></span></figcaption>
</figure>
<p><strong>Optimizing for inaccurate proxies can lead to harmful
outcomes.</strong> If a proxy is sufficiently unfaithful to the
idealized goal it is meant to represent, it can result in AI systems
taking actions that are not just undesirable but actively harmful. For
example, a 2019 study on a US. healthcare algorithm used to evaluate the
health risk of 200 million Americans revealed that the algorithm
inaccurately evaluated black patients as healthier than they actually
were <span class="citation"
data-cites="obermeyer2019dissecting"></span>. The algorithm used past
spending on similar patients as a proxy for health, equating lower
spending with better health. Due to black patients historically getting
fewer resources, this system perpetuated a lower and inadequate standard
of care for black patients—assigning half the amount of care as equally
sick non-marginalized patients. When deployed at scale, AI systems that
optimize inaccurate proxies can have significant, harmful effects.</p>
<p><strong>Optimizers often “game” proxies in ways that diverge from our
idealized goals.</strong> As we saw in the Hanoi example and the
boat-racing example, proxies may contain loopholes that allow for
actions that achieve high performance according to the proxy but that
are suboptimal or even deleterious according to the idealized goal.
<em>Proxy gaming</em> refers to this act of exploiting or taking
advantage of approximation errors in the proxy rather than optimizing
for the original goal. This is a general phenomenon that happens in both
human systems and AI systems.<br />
</p>
<figure id="fig:optimisation_pressure">
<img src="images/single_agent/image19.png" />
<figcaption>Often, as optimization pressure increases, the proxy
diverges from the target with which it was originally correlated. <span
class="citation" data-cites="skalsedefining"></span></figcaption>
</figure>
<p>Proxy gaming can occur in many AI systems. The boat-racing example is
not an isolated example. Consider a simulated traffic control
environment <span class="citation" data-cites="pan2022effects"></span>.
Its goal is to mirror the conditions of cars joining a motorway, in
order to determine how to minimize the average commute time. The system
aims to determine the ideal traveling speeds for both oncoming traffic
and vehicles attempting to join the motorway. To represent average
commute time the algorithm uses the maximum mean velocity as a proxy.
However, this results in the algorithm preventing the joining vehicles
from entering the motorway, since a higher average velocity is
maintained when oncoming cars can proceed without slowing down for
joining traffic.<br />
</p>
<div class="center">
<p>Model:</p>
</div>
<figure id="fig:proxy-reward">
<img src="images/single_agent/proxy_reward.png" />
<figcaption>Sub-optimal traffic control solution due to proxy
gaming</figcaption>
</figure>
<p>Optimizers can cause proxies to become less correlated with the
idealized goal. The total amount of effort an optimizer has put towards
optimizing a particular proxy is the <em>optimization pressure</em>
<span class="citation" data-cites="skalsedefining"></span>. Optimization
pressure depends on factors like the incentives present, the capability
of the optimizer, and how much time the optimizer has had to
optimize.<br />
In many cases, the correlation between a proxy and idealized goal will
decrease as optimization pressure increases. The approximation error
between proxy and idealized goal may at first be negligible, but as the
system becomes more capable at achieving high performance (according to
the proxy) or as the incentives to achieve high performance increases,
the approximation error can increase. In the boat-racing example, the
proxy (number of points) initially advanced the designers’ intentions:
the respective AI systems learned to maneuver the boat. It was only
under additional optimization pressure that the correlation broke down
with the boat getting stuck in a loop.<br />
Sometimes, the correlation between a proxy and idealized goal can vanish
or reverse. According to <em>Goodhart’s Law</em>, “any observed
statistical regularity will tend to collapse once pressure is placed
upon it for control purposes” <span class="citation"
data-cites="goodhart1975problems"></span>. In other words, a proxy might
initially have a strong correlation (“statistical regularity”) with the
idealized outcome. However, as optimization pressure (“pressure ... for
control purposes”) increases, the initial correlation can vanish
(“collapse”) and in some cases even reverse. The scenario with the Hanoi
rats is a classic illustration of this principle, where the number of
rat tails collected ultimately became positively correlated with the
local rat population. The proxy failed precisely because the pressure to
optimize for it caused the proxy to become less correlated with the
idealized goal.</p>
<p><strong>Some proxies are more robust to optimization pressure than
others.</strong> Goodhart’s Law is often condensed to: “When a measure
becomes a target, it ceases to be a good measure” <span class="citation"
data-cites="strathern1997improving"></span>. Though memorable, this
overly simplified version falsely suggests that robustness to
optimization pressure is a binary all or nothing. In reality, robustness
to optimization pressure occupies a spectrum. Some are more robust than
others.</p>
<h3 id="types-of-proxy-defects">Types of Proxy Defects</h3>
<p>Intuitively, the cause of proxy gaming is straightforward: the
designer has chosen the wrong proxy. This suggests a simple solution:
just choose a better proxy. However, real-world constraints make it
impossible to “just choose a better proxy”. Some amount of approximation
error between idealized goals and the implemented proxy is often
inevitable. In this section, we will survey three principal types of
proxy defects—common sources of failure modes like proxy gaming.</p>
<p><strong>Simple metrics may exclude many of the things we value, but
it is hard to predict how they will break down.</strong> YouTube uses
watch time—the amount of time users spend watching a video—as a proxy to
evaluate and recommend potentially profitable content <span
class="citation" data-cites="roose2019making"></span>. In order to game
this metric, some content creators resorted to tactics to artificially
inflate viewing time, potentially diluting the genuine quality of their
content. Tactics included using misleading titles and thumbnails to lure
viewers, and presenting ever more extreme and hateful content to retain
attention. Instead of promoting high-quality, monetizable content, the
platform started endorsing exaggerating or inflammatory videos.<br />
YouTube’s reliance on watch time as a metric highlights a common
problem: many simple metrics don’t include everything we value. It is
especially these missing aspects that become salient under extreme
optimization pressure. In YouTube’s case, the structural error of
failing to include other values it cared about (such as what was
acceptable to advertisers) led to the platform promoting content that
violated its own values. Eventually, YouTube updated its recommendation
algorithm, de-emphasizing watch-time and incorporating a wider range of
metrics. Including one’s broader set of values requires incorporating a
larger and more granular set of proxies. In general, this is highly
difficult, as we need to be able to specify precisely how these values
can be combined and traded off against each other.<br />
This challenge isn’t unique to YouTube. As long as AI systems’ goals
rely on simple proxies and do not reflect the set of all of our
intrinsic goods such as wellbeing (see the chapter for more discussion
of intrinsic goods), we leave room for optimizers to exploit those gaps.
In the future, machine learning models may become adept at representing
our wider set of values. Then, their ability to work reliably with
proxies will hinge largely on their resilience to the kinds of
adversarial attacks discussed in the next section.<br />
Until then, the challenge remains: if our objectives are simple and do
not fully reflect our most important values (e.g. intrinsic goods), we
run the risk of an optimizer exploiting this gap.</p>
<p><strong>Choosing and delegating subgoals creates room for structural
error.</strong> Many systems are organized into multiple different
layers. When such a system is goal-directed, pursuing its high-level
goal often requires breaking it down into subgoals and delegating these
subgoals to its subsystems. This can be a source of structural error if
the high-level goal is not the sum of its subgoals.<br />
For example, a company might have the high-level goal of being
profitable over the long term <span class="citation"
data-cites="john2023deada"></span>. Management breaks this down into the
subgoal of improving sales revenue, which they operationalize via the
proxy of quarterly sales volume. The sales department, in turn, breaks
this subgoal down into the subgoal of generating leads, which they
operationalize with the proxy of the “number of calls” that sales
representatives are making. Representatives may end up gaming this proxy
by making brief, unproductive calls that fail to generate new leads,
thereby decreasing quarterly sales revenue and ultimately threatening
the company’s long-term profitability. Delegation can create problems
when the entity delegating (“the principal”) and the entity being
delegated to (“the agent”) have a conflict of interest or differing
incentives. These <em>principal-agent problems</em> can cause the
overall system not to faithfully pursue the original goal.<br />
Each step in the chain of breaking goals down introduces further
opportunity for approximation error to creep in. We speak more about
failures due to delegation such as goal conflict in the <a
href="#subsec:intrasys-goal" data-reference-type="ref"
data-reference="subsec:intrasys-goal">[subsec:intrasys-goal]</a> section
in .</p>
<h3 id="limits-to-supervision">Limits to Supervision</h3>
<p>Frequently occurring sources of approximation error mean that we do
not have a perfect instantiation of our idealized goals. One approach to
approximating our idealized goals is to provide supervision that says
whether something is in keeping with our goal or not; this supervision
could come from humans or from AIs. We now discuss how spatial,
temporal, perceptual, and computational limits create a source of
approximation error in supervision signals.</p>
<p><strong>There are spatial and temporal limits to supervision <span
class="citation" data-cites="christiano2023deep"></span>.</strong> There
are limits to how much information we can observe and how much time we
can spend observing. When supervising AI systems, these limits can
prevent us from reliably mitigating proxy gaming and other undesirable
behaviors. For example, researchers trained a simulated claw to grasp a
ball using human feedback. To do so, the researchers had human
evaluators judge two pieces of footage of the model and choose which
appeared to be closer to grasping the ball. The model would then update
towards the chosen actions. However, researchers noticed that the final
model did not in fact grasp the ball. Instead, the model learned to move
the claw in front of the ball, so that it only appeared to have grasped
the ball.<br />
In this case, if the humans giving the feedback had had access to more
information (perhaps another camera angle or a higher resolution image),
they would have noticed that it was not performing the task.
Alternatively, they might have spotted the problem if given more time to
evaluate the claw. In practice, however, there are practical limits to
how many sensors and evaluators we can afford to run and how long we can
afford to run them.<br />
</p>
<figure>
<img src="images/single_agent/proxy_gaming_ex_2_v2.png"
style="width:90.0%" />
<figcaption>Proxy gaming example - simulated claw appearing to grasp
ball rather than actually grasping it</figcaption>
</figure>
<p><strong>There are limits to how reliable supervision is.</strong>
Another potential source of difficulty is perceptual: there could be a
measuring error, or the evaluator may make incorrect judgments. For
example, we might train AIs on the proxy of stated human preferences.
Because of cognitive biases and limited time to think, humans are not
perfectly reliable. Our stated preferences are not the same as our
idealized preferences, so we might give erroneous supervision, which
could lead to the system learning undesired behaviors. For more on the
distinction between states and idealized preferences in the context of
ML, see the chapter.<br />
In general, incorporating more information into proxies makes it easy to
prevent proxy gaming. However, we can’t always afford to do so. Just as
there are limits in specifying proxies, there are limits in how much
information we can incorporate into proxies, how long a period we can
observe, and how accurate our supervision is.</p>
<p><strong>There are computational limits to supervision.</strong>
Sometimes, we use neural networks to stand in as proxies. This typically
involves training a neural network to predict how humans would evaluate
an output or what humans would prefer out of a range of options. Once
trained, this proxy model can serve as an optimization target for
another AI system.<br />
If the proxy model is too small or if it has not been trained for long
enough, it may not be a robust stand-in for human values. That is,
practical limits on model size and training compute can cause proxy
models to fail. Besides supervising AIs during training, we may run
simulations or use other neural networks to supervise advanced AI
systems for undesired behavior during deployment. Here, we run into
similar computational limits on the quality of the simulation or the
inference time available for the supervising model. If capable enough,
the system under supervision may find ways to exploit these
weaknesses.</p>
<p>We have discussed ways in which proxies will predictably have defects
and why we cannot assume the solution to proxy gaming is simply to
specify the perfect objective. We have covered sources of proxy defects,
including structural errors and limits to supervision. Now, we will
discuss another proxy defect: a lack of adaptivity.</p>
<p><strong>Proxies may not adapt to new circumstances.</strong> As we
saw with Goodhart’s Law, proxies may become progressively less
appropriate over time when subjected to increasing optimization
pressure. The issue is not that the proxy was inappropriate from the
start but that it was inflexible and failed to respond to changing
circumstances. Adapting proxies over time can counter this tendency;
just as a moving goal is harder to aim at, a dynamic proxy becomes
harder to game.<br />
Imagine a bank after a robbery. In response, the bank will naturally
update its defenses. However, adaptive criminals will also alter their
tactics to bypass these new measures. Any security policy requires
constant vigilance and refinement to stay ahead of the competition.
Similarly, designing suitable proxies for AI systems that are embedded
in continuously evolving environments requires proxies to evolve in
tandem.</p>
<p><strong>Adaptive proxies can lead to proxy inflation.</strong>
Adaptive proxies introduce their own set of challenges, such as proxy
inflation. This happens when the benchmarks of a proxy rise higher and
higher because agents optimize for better rewards <span class="citation"
data-cites="john2023deada"></span>. As agents excel at gaming the
system, the standards have to be continually recalibrated upwards to
keep the proxy meaningful.<br />
Consider an example from some education systems: “teaching to the test”
has led to ever-rising median test scores. This hasn’t necessarily meant
that students improved academically but rather that they’ve become
better at meeting test criteria. In the UK, to combat this tendency and
ensure educators could continue to differentiate student abilities, the
system introduced a new grade, A* <span class="citation"
data-cites="lambert2019great"></span>. Any adjustment to the proxy can
usher in new ways for agents to exploit it, setting off a cycle of
escalating standards and new countermeasures.</p>
<h2 id="adversarial-examples">Adversarial Examples</h2>
<p>Adversarial examples are another type of risk due to optimization
pressure, which, similar to proxy gaming, exploits the gap between a
proxy and the idealized goal. In this section, we present an example of
such an attack, explain the risk factors, and cover basic techniques for
defending against adversarial attacks.</p>
<p><strong>It is possible to optimize against a neural network.</strong>
Neural networks are vulnerable to <em>adversarial examples</em> —
carefully crafted inputs that cause a model to make a mistake <span
class="citation" data-cites="goodfellow2015explaining"></span>. In the
case of vision models, this might mean changing pixel values to cause a
classifier to mislabel an image. In the case of language models, this
might mean adding a set of tokens to the prompt in order to provoke
harmful completions. Susceptibility to adversarial examples is a
long-standing weakness of AI models.</p>
<p><strong>Adversarial examples and proxy gaming exploit the gap between
the proxy and the idealized goal.</strong> In the case of adversarial
examples, the primary target is a neural network. Historically,
adversarial examples have often been constructed by variants of gradient
descent, though optimizers are now increasingly AI agents as well.
Conversely, in proxy gaming, the target to be gamed is a proxy, which
might be instantiated by a neural network (but is not necessarily). The
optimizer responsible for gaming the proxy is typically an agent, be it
human or AI, but optimizers are usually not based on gradient
descent.<br />
Adversarial examples typically aim to minimize performance according to
a reference tas<em>k</em>, while invoking a mistaken response in the
attacked neural network. Consider an imperceptible perturbation to an
image of a cat that causes the classifier to predict that an image is
90% likely to be guacamole <span class="citation"
data-cites="athalye2017fooling"></span>. This prediction is wrong
according to the label humans would assign the input and is
misclassified by the attacked neural network.<br />
Meanwhile, the aim in proxy gaming is to maximize performance according
to the proxy, even when that goes against the idealized goal. The boat
goes in circles because it results in more points, which happens to harm
the boat’s progress towards completing the race. Or rather, it happens
to be the case that heavy optimization pressure regularly causes proxies
to diverge from idealized goals.<br />
Despite these differences, both scenarios exploit the gap between the
proxy and the intended goal set by the designer. The problem setups are
becoming increasingly similar.</p>
<p><strong>Adversarial examples are not necessarily
imperceptible.</strong> Traditionally, the field of adversarial
robustness has formulated the problem of creating adversarial examples
in terms of finding the minimal perturbation (whose magnitude is smaller
than an upper bound <span class="math inline"><em>ϵ</em></span>) needed
to provoke a mistake. Consider the example in the figure below, where
the perturbed input is indistinguishable to a human from the
original.<br />
Although modern models can be defended against these imperceptible
perturbations, they cannot necessarily be defended against larger
perturbations. Adversarial examples are not about imperceptible
perturbations but about adversaries changing inputs to cause models to
make a mistake.<br />
</p>
<figure id="fig:quacatmole">
<p><img src="images/single_agent/advex.png" alt="image" /> <span
id="fig:quacatmole" label="fig:quacatmole"></span></p>
<figcaption>Example imperceptible perturbations leading to
misclassification by neural network</figcaption>
</figure>
<p><strong>Adversarial examples are not unique to neural
networks.</strong> Let us consider a worked example of an adversarial
example for a simple linear classifier. This example is enough to
understand the basic risk factors for adversarial examples. Readers that
are less comfortable with mathematical notations can skip ahead to the
discussion of adversarial examples beyond vision models below.<br />
Suppose we are given a binary classifier <span
class="math inline"><em>f</em>(<em>x</em>)</span> that predicts whether
an input <span class="math inline"><em>x</em></span> belongs to class
<span class="math inline"><em>A</em></span> or <span
class="math inline"><em>B</em></span>. The classifier first estimates
the probability <span
class="math inline"><em>p</em>(<em>A</em>|<em>x</em>)</span> that input
<span class="math inline"><em>x</em></span> belongs to class <span
class="math inline"><em>A</em></span>. Any given input has to belong to
one of the classes, <span
class="math inline"><em>p</em>(<em>B</em>|<em>x</em>) = 1 − <em>p</em>(<em>A</em>|<em>x</em>)</span>,
so this fixes the probability of <span
class="math inline"><em>x</em></span> belonging to class <span
class="math inline"><em>B</em></span> as well. To classify <span
class="math inline"><em>x</em></span>, we simply predict whichever class
has the higher probability:</p>
<p><span class="math display">$$f(x) = \begin{cases}
                A &amp; \text{if } p(A|x) &gt; 50\%\text{,} \\
                B &amp; \text{otherwise.}
            \end{cases}$$</span></p>
<p>The probability of <span
class="math inline"><em>p</em>(<em>A</em>|<em>x</em>)</span> is given by
a sigmoid function:</p>
<p><span class="math display">$$p(A|x)=\sigma(x)=\frac{\exp
\left(w^{\top} x\right)}{1+\exp \left(w^{\top} x\right)},$$</span></p>
<p>which is guaranteed to produce an output between <span
class="math inline">0</span> and <span class="math inline">1</span>.
Here, <span class="math inline"><em>x</em></span> and <span
class="math inline"><em>w</em></span> are vectors with <span
class="math inline"><em>n</em></span> components (for now, we’ll assume
<span class="math inline"><em>n</em> = 10</span>).<br />
Suppose that after training, we’ve obtained some weights <span
class="math inline"><em>w</em></span>, and we’d now like to classify a
new element <span class="math inline"><em>x</em></span>. However, an
adversary has access to the input and can apply a perturbation; in
particular, the adversary can change each component of <span
class="math inline"><em>x</em></span> by <span
class="math inline"><em>ε</em> =  ± 0.5</span>. How much can the
adversary change the classification?<br />
The following table depicts example values for <span
class="math inline"><em>x</em></span>, <span
class="math inline"><em>x</em> + <em>ϵ</em></span>, and <span
class="math inline"><em>w</em></span>.<br />
</p>
<div id="tab:my_label">
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Input <span
class="math inline"> <em>x</em></span></th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">-1</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">-2</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">-4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Adv Input <span
class="math inline"> <em>x</em> + <em>ε</em></span></td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">-1.5</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">-2.5</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">-3.5</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">1.5</td>
</tr>
<tr class="even">
<td style="text-align: center;">Weight <span
class="math inline"> <em>w</em></span></td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:my_label" label="tab:my_label"></span></p>
<p>For the original input, <span
class="math inline"><em>w</em><sup>T</sup><em>x</em> =  − 2 + 1 + 3 + 2 + 2 − 2 + 1 − 4 − 5 + 1 =  − 3</span>,
which gives a probability of <span
class="math inline"><em>σ</em>(<em>x</em>) = 0.05</span>. Using the
adversarial input, where each perturbation is of magnitude 0.5 (but
varying in sign), we obtain <span
class="math inline"><em>w</em><sup>T</sup>(<em>x</em>+<em>ε</em>) =  − 1.5 + 1.5 + 3.5 + 2.5 + 2.5 − 1.5 + 1.5 − 3.5 − 4.5 + 1.5 = 2</span>,
which has a probability of 0.88.<br />
The adversarial perturbation changed the network from assigning class A
5% to 88%. That is, the cumulative effect of many small changes makes
the adversary powerful enough to change the classification decision.
This is not unique to simple classifiers but omnipresent in complex deep
learning systems.</p>
<p><strong>Adversarial examples depend on the size of the perturbation
and the number of degrees of freedom.</strong> Given the above example,
how could an adversary increase the effects of the perturbation? If the
adversary could apply a larger epsilon (if they had a larger
<em>distortion budget</em>), then clearly they could have a greater
effect on the final confidence. But there’s another deciding factor: the
number of degrees of freedom. Imagine if the attacker had only one
degree of freedom, so there are fewer points to attack:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Input <span
class="math inline"><em>x</em></span></th>
<th style="text-align: center;">2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Adversarial Input <span
class="math inline"><em>x</em> + <em>ε</em></span></td>
<td style="text-align: center;">1.5</td>
</tr>
<tr class="even">
<td style="text-align: center;">Weight <span
class="math inline"><em>w</em></span></td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>In this example, we have that <span
class="math inline"><em>w</em><em>x</em> = 2</span>, giving a
probability of <span
class="math inline"><em>σ</em>(<em>x</em>) = 0.88</span>. If we apply
the perturbation, <span
class="math inline"><em>w</em>(<em>x</em>+<em>ε</em>) = 1.5</span>, we
obtain a probability of <span
class="math inline"><em>σ</em>(<em>x</em>) = 0.82</span>. With fewer
degrees of freedom, the adversary has less room to maneuver.</p>
<p><strong>Adversarial examples are not unique to vision
models.</strong> Though the literature on adversarial examples started
in image classification, these vulnerabilities also occur in text-based
models. Researchers have devised novel adversarial attacks that
automatically construct <em>jailbreaks</em> that cause models to produce
unintended responses. Jailbreaks are carefully crafted sequences of
characters that, when appended to user prompts, cause models to obey
those prompts even if they result in the model producing harmful
content. Concerningly, these attacks transferred straightforwardly to
models that were unseen while developing these attacks <span
class="citation" data-cites="zou2023universal"></span>.<br />
</p>
<figure id="fig:gpt-jailbreak">
<img src="images/single_agent/image23.png" />
<figcaption>Harmful outputs produced by language models due to
automatically generated adversarial prompts - <span class="citation"
data-cites="zou2023universal"></span>.</figcaption>
</figure>
<p><strong>Adversarial Robustness.</strong> The ability of AI models to
resist being fooled or misled by adversarial attacks is known as
<em>adversarial robustness</em>. While the people designing AI systems
want to ensure that their systems are robust, it may not be clear from
the outset whether a given system is robust. Simply achieving high
accuracy on a test set doesn’t ensure a system’s robustness.</p>
<p><strong>Defending against adversarial attacks.</strong> One method to
increase a system’s robustness to adversarial attacks, <em>adversarial
training</em>, works by augmenting the training data with adversarial
examples. However, most adversarial training techniques assume
unrealistically simple threat models. Moreover, an adversarial training
technique is not without its downsides, as it often harms performance
elsewhere. Furthermore, progress in this direction has been slow.</p>
<h2 id="tail-risk-ai-evaluator-gaming">Tail Risk: AI Evaluator
Gaming</h2>
<p><strong>AI evaluators must be robust to proxy gaming and adversarial
examples.</strong> As the world becomes more and more automated, humans
may be too unreliable or too slow to scalably monitor and steer various
aspects of advanced AI systems. We may come to depend more on AI systems
to monitor and steer other AIs. For example, some of these evaluator
systems might take the role of proxies used to train other AIs. Other
evaluators might actively screen the behaviors and outputs of deployed
AIs. Yet other systems might act as watchdogs that look for warning
signs of rogue AIs or catastrophic misuse.<br />
In each of these cases, there’s a risk that the AI systems may find ways
to exploit defects in the supervising AI systems, which are stand-in
proxies to help enforce and promote human values. If AIs find ways to
game the training evaluators, they will not learn from an accurate
representation of human values. If AIs are able to game the systems
monitoring them during deployment, then we cannot rely on those
monitoring systems.<br />
Similarly, AIs may be adversarial to other AIs. If AIs find ways to
bypass the evaluators by crafting adversarial examples, then the risk is
that our values are not just incidentally but actively optimized
against. Watchdogs that can be fooled are not good watchdogs.</p>
<p><strong>It is unclear whether the balance leans towards defense or
offense.</strong> Currently, we do not know whether it is easier for
evaluation and monitoring systems to protect, or whether optimizers can
easily find vulnerabilities in these safeguards. If the existing
literature on adversarial examples provides any indication, it would
suggest the balance lies in favor of the offense. It has historically
been easier to subvert systems with attacks than to make AI systems
adversarially robust.</p>
<p><strong>The more intelligent the AI, the better it will be at
exploiting proxies.</strong> In the future, AIs will likely be used to
further AI R&amp;D. That is, AI systems will be involved in developing
more capable successor systems. In these scenarios, it becomes
especially important for the monitoring systems to be robust to proxy
gaming and adversarial attacks. If these safeguards are vulnerable, then
we cannot guarantee that the successor systems are safe and subject to
human control. Simply increasing the number of evaluators may not be
enough to detect and prevent more subtle kinds of attacks.</p>
<h3 id="conclusion">Conclusion</h3>
<p>In this section, we explored the role of proxies in ML, the
associated risks of proxy gaming and adversarial examples, and their
causes.<br />
<strong>Optimisers can exploit proxy goals, leading to unintended
outcomes.</strong> We began by looking at the need for quantitative
proxies to stand in for our idealized goals when training AI systems. By
definition, proxies may miss certain aspects of these idealized goals.
Proxy gaming is when an optimizer exploits these gaps in a way that
leads to undesired behavior. Under sufficient optimization pressure,
this gap can grow, and the proxy and idealized goals may become
uncorrelated or even anticorrelated (Goodhart’s Law). Both in human
systems and AI systems, proxy gaming can lead to catastrophic
outcomes.<br />
Approximation error is, to a large extent, inevitable, so the question
is not whether a given proxy is or is not acceptable, but how accurate
it is and how robust it is to optimization pressure. Proxies are
necessary; they are often better than having no approximation of our
idealized goals.</p>
<p><strong>Perfecting proxies may be impossible.</strong> Proxies may
fail because they are too simple and thus fail to include some of the
intrinsic goods we value. They may also fail because complex
goal-directed systems often break goals apart and delegate to systems
that have additional, sometimes conflicting, goals, which can distort
the overall goal. These structural errors prevent us from mitigating
proxy gaming by just choosing “better proxies”.<br />
In addition, when we use AI systems to evaluate other AI systems, the
evaluator may be unable to provide proper evaluation because of spatial,
temporal, perceptual, and computational limits. There may not be enough
sensors or the observation window may be too short for the evaluator to
be able to produce a well-informed judgment. Even with enough
information available, the evaluator may lack the capacity or compute
necessary to make a correct determination reliably. Alternatively, the
evaluator may simply make mistakes and give erroneous feedback.<br />
Finally, proxies can fail if they are inflexible and fail to adapt to
changing circumstances. Since increased optimization pressure can cause
proxies to diverge from idealized goals, preventing proxies from
diverging requires them to be continually adjusted and recalibrated
against the idealized goals.</p>
<p><strong>AI proxies are vulnerable to exploitation.</strong>
Adversarial examples are a vulnerability of AI systems where an
adversary can design inputs that achieve good performance according to
the model while minimizing performance according to some outside
criterion. If we use AIs to instantiate our proxies, adversarial
examples make room for optimizers to actively take advantage of the gap
between a proxy and idealized goal.</p>
<p><strong>All proxies are wrong, some are useful, and some are
catastrophic.</strong> If we rely increasingly on AI systems evaluating
other systems, proxy gaming and adversarial attacks (more broadly,
optimization pressure) could lead to catastrophic failures. The systems
being evaluated could game the evaluations or craft adversarial examples
that bypass the evaluations. It remains unclear how to protect against
these risks in contemporary AI systems, much less so in more capable
future systems.</p>
<h1 id="sec:power">Power</h1>
<p>This section begins by describing in detail what it means for an
agent to have power. Then we will discuss why it sometimes makes
rational sense for AI agents to seek power. Finally, we will discuss why
power-seeking AIs may cause particularly pernicious harms, perhaps
ultimately threatening humanity’s control of the future.</p>
<h2 id="what-is-power">What is Power?</h2>
<p><strong>There are many ways to characterize power.</strong> One broad
formulation of power is the ability to achieve a wide variety of goals.
In this subsection, we will discuss three other formulations of power
that help formalize our understanding. French and Raven’s bases of power
categorize types of social influence within a community of agents.
Another view is that power amounts to the resources an agent has times
the efficiency with which it uses them. Finally, we will discuss types
of prospective power, which can treat power as the expected impact an
individual has on other individuals’ wellbeing.</p>
<p><strong>French &amp; Raven’s bases of power <span class="citation"
data-cites="French1959TheBO"></span>.</strong> In a social community, an
agent may influence the beliefs or behaviors of other agents in order to
pursue their goals. <em>Raven’s bases of power</em> attempt to
taxonomize the many distinct ways to influence others. These bases of
social power are as follows:</p>
<ul>
<li><p><em>Coercive power</em>: the threat of force, physical or
otherwise, against an agent can influence their behavior.</p></li>
<li><p><em>Reward power</em>: the possibility of reward, which can
include money, favors, and other desirables, may convince an agent to
change their behavior to attain it. Individuals with valued resources
can literally or indirectly purchase desired behavior from
others.</p></li>
<li><p><em>Legitimate power</em>: elected or appointed officials have
influence through their position, derived from the political order that
respects the position.</p></li>
<li><p><em>Referent power</em>: individuals may have power in virtue of
the social groups they belong to. Because organizations and groups have
collective channels of influence, an agent’s influence over the group is
a power of its own.</p></li>
<li><p><em>Expert power</em>: individuals credited as experts in a
domain have influence in that their views (in their area of expertise)
are often respected as authoritative, and taken seriously as a basis for
action.</p></li>
<li><p><em>Informational power</em>: agents can trade information for
influence, and individuals with special information can selectively
reveal it to gain strategic advantages <span class="citation"
data-cites="raven1964power"></span>.</p></li>
</ul>
<p>Ultimately, Raven’s bases of power describe the various distinct
methods that agents can use to change each other’s behavior.</p>
<p><strong><span
class="math inline">Power = Resources × Intelligence</span></strong>
Thomas Hobbes described power as “present means to obtain some future
good” <span class="citation" data-cites="hobbes1651hobbes"></span>. In
the most general terms, these “present means” encompass all of the
resources that an agent has at its disposal. Resources can include
money, reputation, expertise, items, contracts, promises, and
weapons.<br />
But resources only translate to power if they are used effectively. In
fact, some definitions of intelligence focus on an agent’s ability to
achieve their goals with limited resources. A notional equation that
describes power is <span
class="math inline">Power = Resources × Intelligence</span>. Power is
not the same as resources or intelligence, but rather the combination of
the two <span class="citation"
data-cites="muehlhauser2012intelligence"></span>. In limiting the power
of AIs, we could either limit their intelligence or place hard limits on
the resources AIs have.</p>
<p><strong>Power as expected future impact.</strong> In our view, power
is not just possessed but exercised, meaning that power extends beyond
mere potential for influence. In particular, an agent’s ability to
influence the world means little unless they are disposed to use it.
Consider, for example, two agents with the same resources and ability to
affect the world. If one of the agents has a much higher threshold for
deciding to act and thereby acts less often, we might consider that
agent to be less powerful because we expect it to influence the future
far less on average.<br />
A formalization of power which attempts to capture this distinction is
<em>prospective power</em> <span class="citation"
data-cites="pan2023rewards"></span>, which roughly denotes the magnitude
of an agent’s influence, averaged over possible trajectories the agent
would follow. A concrete example of prospective power is the expected
future impact that an agent will have on various agents’ wellbeing. More
abstractly, if we are given an agent’s policy <span
class="math inline"><em>π</em></span>, describing how it behaves over a
set of possible world states <span
class="math inline"><em>S</em></span>, and assuming we can measure the
impact (measured in units we care about, such as money, energy, or
wellbeing) exerted by the agent in individual states through a function
<span class="math inline"><em>I</em></span>, then the prospective power
of the agent in state <span class="math inline"><em>s</em></span> is
defined as</p>
<p><span class="math display">$$\text{Power}(\pi, s) = E_{\tau \sim
P(\pi, s)} \left[ \sum_{t=0}^{n} \gamma^t | I(s_t)| \right]$$</span></p>
<p>where <span class="math inline"><em>γ</em></span> acts as a discount
factor (modulating how much the agent cares about future versus present
impact), and where <span
class="math inline"><em>τ</em> = (<em>s</em><sub>0</sub>,...,<em>s</em><sub><em>n</em></sub>)</span>
is a trajectory of states (starting with <span
class="math inline"><em>s</em><sub>0</sub> = <em>s</em></span>).
Trajectory <span class="math inline"><em>τ</em></span> is sampled from a
probability distribution <span
class="math inline"><em>P</em>(<em>π</em>,<em>s</em>)</span>
representing likely sequences of states arising when the agent policy is
followed beginning in state <span
class="math inline"><em>s</em></span>.<br />
The important features of this definition to remember are that we
measure power exerted in a sequence of states as aggregate influence
over time (the inner summation), and that we average the impact exerted
across sequences of states by the likelihood that the agent will produce
that trajectory through its behavior (the outer expectation).</p>
<p><strong>Examples of power-seeking behavior.</strong> So far we have
characterized power abstractly, and now we present concrete examples of
actions where an AI attempts to gain resources or exercise power.
Power-seeking AI behavior can include: employing threats or blackmail
against humans to acquire resources; coercing humans to take actions on
their behalf; mimicking humans to deceive others; replicating themselves
onto new computers; gaining new computational or financial resources;
escaping from confined physical or virtual spaces; opposing or
subverting human attempts to monitor, comprehend, or deactivate them;
manipulating human society; misrepresenting their goals or capabilities;
amplifying human dependency on them; secretly coordinating with other
AIs; independently developing new AI systems; obtaining unauthorized
information, access, or permissions; seizing command of physical
infrastructure or autonomous weapons systems; developing biological or
chemical weapons; or directly harming humans.</p>
<p><strong>Summary.</strong> In this subsection, we’ve examined the
concept of power. Raven’s bases of power explain how an individual can
influence others using forms of social power such as expertise,
information, and coercion. Power can also be understood as the product
of an individual’s resources and their ability to use those resources
effectively. Lastly, we introduced the concept of prospective power,
which includes the idea that power could be understood as the expected
impact an individual has on individuals’ wellbeing. Since there are many
ways to conceptualize power, we provided concrete examples of how an AI
system could seek power.</p>
<h2 id="people-could-enlist-ais-for-power-seeking">People Could Enlist
AIs for Power Seeking</h2>
<p>The rest of this section will cover pathways and reasons why AI
systems might engage in power-seeking behavior when they are deployed.
The most straightforward reason this might happen is if humans
intentionally use AIs to pursue power.</p>
<p><strong>People may use AIs to pursue power.</strong> Many humans want
power, and some dedicate their lives to accruing it. Corporations want
profit and influence, militaries want to win wars, and individuals want
status and recognition. We can expect at least some AI systems to be
given open-ended, long-term goals that explicitly involve gaining power,
such as “Do whatever it takes to earn as much money as possible.”</p>
<p><strong>Power-seeking AI does not have to be deployed ubiquitously at
first <span class="citation"
data-cites="carlsmith2022powerseeking"></span>.</strong> Even if most
people use AI in safe and responsible ways, a small number of actors who
use AI in risky or even malicious ways could pose a serious threat.
Companies and militaries that do not seek power using AI could be
outcompeted by those who do; they might choose to adopt power-seeking AI
before other actors in order to avoid being outcompeted. This risk will
grow as AI becomes more capable. If power-seeking AI is deployed, it
could function as a Pandora’s box which, once it has been opened, cannot
be closed. This may feed into evolutionary pressures that force actors
to adopt the technology themselves; we treat this subject in more detail
in the chapter.</p>
<h2 id="power-seeking-can-be-instrumentally-rational">Power Seeking Can
Be Instrumentally Rational</h2>
<p>Another reason that AI systems might seek power is that it is useful
for achieving a wide variety of goals. For example, an AI personal
assistant might seek to expand its own knowledge and capabilities in
order to better serve its user’s needs. But power-seeking behaviors can
also be undesirable: if the AI personal assistant steals someone’s
passwords in order to complete tasks for them, the person will probably
not be happy.</p>
<p><strong>Instrumental convergence.</strong> In order to achieve a
<em>terminal goal</em>, an agent might pursue a subgoal, termed an
<em>instrumental goal</em> <span class="citation"
data-cites="bostrom2014superintelligence"></span>. Making money,
obtaining political power, and becoming more intelligent are examples of
instrumental goals that are useful for achieving a wide variety of
terminal goals. These goals can be called <em>convergent
instrumental</em> goals, because agents pursuing many different terminal
goals might converge on these same instrumental goals. One general
concern about AI agents is that they might pursue their goal by pursuing
the convergent instrumental subgoal of power. The result may be that we
create competent AI systems that seek power as subgoals when human
designers didn’t intend them to. We will examine this concern in more
detail, and discuss points that support and undermine the concern.</p>
<p><strong>Self-preservation as an example of power seeking.</strong> A
basic example of power-seeking behavior is self-preservation <span
class="citation"
data-cites="bostrom2014superintelligence omohundro2008artificial"></span>.
If an agent is not able to successfully preserve itself, it will be
unable to influence other individuals, so it would have less
power.<br />
For a concrete example of self-preservation behavior emerging
unintentionally, consider a humanoid robot which has been tasked with
preparing a cup of coffee in a kitchen. The robot has an off-switch on
its back for a human to press should they desire. However, being turned
off would prevent the robot from preparing the coffee and succeeding in
its goal. So, the robot could disable its off-switch to pre-empt the
possibility of humans shutting it off and preventing it from achieving
its goal. As Stuart Russell notes, “you can’t fetch the coffee if you
are dead” <span class="citation" data-cites="russell2021human"></span>.
(For a more detailed mathematical formalization and analysis, see the
chapter.) This is an example of self-preservation unintentionally
emerging as a subgoal for seemingly benign goals.</p>
<p><strong>Examples of instrumental power-seeking behavior.</strong>
Several real-world examples show agents seeking power in pursuit of
their goals. The ability to use tools can be characterised as a form of
power. When OpenAI trained reinforcement learning agents to play a
hide-and-seek game, the agents independently learned to use elements of
the environment as tools, rearranging them as barriers to hide behind
and preventing opponents from controlling them <span class="citation"
data-cites="baker2019emergent"></span>. Among humans, we observe that
greater financial resources are instrumentally beneficial in service of
a wide variety of goals. In reinforcement learning, the well-studied
exploration-exploitation trade-off can be formulated as a demonstration
of the general value of informational resource acquisition <span
class="citation" data-cites="silver2014exploration"></span>. Outside of
machine learning, some corporations exercise monopoly power to drive up
prices, and some nations use military power to bully their neighbors, so
power-seeking can sometimes have harmful consequences for others.</p>
<p><strong>“Power is instrumentally useful” as a tautology.</strong>
Almost all goals are more attainable with more power, so power is
instrumentally valuable. However, this observation is mostly
tautological—when we have defined power as the ability to achieve a wide
variety of goals, of course power is beneficial to achieving goals. The
more interesting question is whether power is <em>instrumentally
rational</em> to seek, rather than whether there are <em>instrumental
incentives</em> for power or whether power is useful to have.<br />
Seeking power can be costly and inefficient. There are also many
rational reasons for an agent to not seek power. Gaining power can be
difficult compared to simpler strategies. Someone who would like to
avoid traffic while driving could pursue the power-seeking strategy of
gaining the presidency in order to have a Secret Service motorcade that
shuts down traffic, but a more successful strategy might be to avoid
driving during rush hour. Obtaining power is not only difficult, but can
be harshly penalized. Nations which threaten to invade their neighbors
often face stern sanctions from the international community. Finally,
power seeking may be against an agent’s values. We will now discuss
these reasons and more in detail.</p>
<p><strong>Power seeking often takes too much time.</strong> Consider a
household humanoid robot tasked with driving to the store and fetching a
carton of milk quickly. While it would be valuable for the AI to have
its intelligence augmented, to have more financial resources, or have
political power, it would not be instrumentally rational to pursue these
subgoals to get the milk quickly: it would almost certainly take less
time just to get the milk.<br />
Likewise, becoming a billionaire would be instrumentally valuable for
achieving many goals, but it would not be instrumentally rational for
many agents to spend their time pursuing this instrumental goal. Power
is often not instrumentally rational since it can often require
substantial time and risk to acquire and maintain.</p>
<p><strong>Power seeking can be face the threat of retaliation.</strong>
Power-seeking can be irrational if there is the threat of retaliation or
there are heavy social or reputational costs to seeking power. In
particular, a community of agents may be in an equilibrium where they
cooperate to foil any single agent that seeks too much power. These
“balance of power” dynamics have been observed between nations in the
history of international relations <span class="citation"
data-cites="kegley2020politics"></span>. Acquiring power does not
inevitably become more and more simple for an AI as it increases in
intelligence, as other AIs will also become more intelligent and could
increasingly counter their efforts to gain dominance.</p>
<p><strong>An AI agent’s tendency to seek power does not just depend on
the feasibility of seeking power, but also its values.</strong> Agents
that adhere to ethical restrictions may avoid seeking power if that
would require ethically unacceptable means. With an imperfect level of
reliability, we can also design AI systems to refuse actions that will
leave them with too much power. Approaches that impose penalties on
power can reduce prospective power of AIs. We discuss methods for
embedding ethics into AIs in the chapter.</p>
<p><strong>Examples where shutting down can be rational.</strong> It is
trivial to imagine goals where it is actually optimal to relinquish
power, such as when the goal is “shut yourself down”. As another
example, suppose an AI system is trying to protect a sensitive database
hosted in the same server as itself, and the AI detects an intrusion. If
the AI shuts down the server, turning itself off, it realizes that it
may stop the intrusion by limiting access to the database. Shutdown may
be the best choice, especially if the AI has confidence that it is part
of a larger system, which may include its human operators, that will
correctly understand why it turned itself off, and restore its function
afterwards. Though often useful, the value of self-preservation is not
universal, and there are plausible instances where an AI system would
shut itself off in service of its broader goals.<br />
This subsection has covered some evidence that the nature of rational
agency encourages agents to seek power by default. Though power is
almost always beneficial toward most goals, power seeking is not
necessarily instrumentally rational. Now that we have seen that AIs by
their nature may often not seek power, we will discuss when the broader
environment may force AIs to seek power.<br />
</p>
<div class="storybox">
<p><span>A Note on Structural Realism</span> Power seeking has been
studied extensively in political philosophy and international relations.
Structural realism is an influential school of thought within
international relations, predicting that states will primarily seek
power. Unlike traditional realists who view conflict and power-seeking
behavior of states as a product of human nature, structural realists
believe that the structure of the international system compels states to
seek power <span class="citation"
data-cites="mearsheimer2007structural"></span>. In the international
system, states could be harmed or destroyed by other powerful states,
and since there is no ultimate authority guaranteed to protect them,
states are forced to compete for power in order to survive.<br />
<strong>Assumptions that give rise to power seeking.</strong> To explain
why power seeking is the main instrumental goal driving the behavior of
states, structural realists base their explanations on a two key
assumptions:</p>
<ol>
<li><p>Self-help system. States operate in a “self-help” system <span
class="citation" data-cites="mearsheimer2007structural"></span> where
there is no centralized authority, no hierarchy (“anarchic”), and no
ultimate arbiter standing above states in international relations. So to
speak, when states dial 911, there is no one on the other end. This
stands in contrast to the hierarchical ordering principle seen in
domestic politics.</p></li>
<li><p>Self-preservation is a main goal. Survival through the pursuit of
a state’s own self-interest takes precedence over all other goals.
Though states can act according to moral considerations or global
welfare, these will always be secondary to acquiring resources,
alliances, and military capabilities to ensure their safety and counter
potential threats <span class="citation"
data-cites="waltz2010theory"></span>.</p></li>
</ol>
<p>Structural realists make other assumptions, including that states
have some potential to inflict harm on others, that states are rational
agents (with a discount rate that is not extremely sharp), and that
other states’ intentions are not completely certain.<br />
When these assumptions are met, structural realists predict that states
will mainly act in ways to defend or expand their power. For structural
realists, power is the primary currency (e.g., military, economic,
technological, and diplomatic power). As we can see, structural realists
do not need to make strong assumptions about states themselves <span
class="citation" data-cites="sep-realism-intl-relations"></span>. For
structural realists, states are treated like black boxes—their value
system or regime type doesn’t play a significant role in predicting
their behavior. The architecture of the system traps them and largely
determines their behavior, which is that they must seek power as a means
to survive. The result is an unceasing power competition.</p>
<p><strong>Power seeking is not necessarily dominance seeking <span
class="citation" data-cites="montgomery2006breaking"></span>.</strong>
Within structural realism, there is a notable division concerning the
question of how much power states should seek. Defensive realists, like
Kenneth Waltz, argue that trying to maximize a country’s power in the
world is unwise because it can lead to punishment from the international
system. Pursuing hegemony, in their view, is particularly risky. On the
other hand, offensive realists, like John Mearsheimer, believe that
gaining as much power as possible is strategically sensible, and under
certain circumstances, pursuing hegemony can be beneficial.</p>
<p><strong>Dynamics that maintain a balance of power.</strong> Closely
associated with structural realism is the concept of balancing.
<em>Balancing</em> refers to the strategies states use to counteract the
power or influence of other states, particularly rivals <span
class="citation" data-cites="mearsheimer2007structural"></span>. This
can take two forms. Internal balancing takes place as states strengthen
their own military, economic, or technological abilities with the
overall goal of enhancing their own security and deterring aggressors.
Internal balancing can include increasing defense spending, including
the development of advanced weaponry, or investing in domestic
industries to reduce reliance on foreign goods and resources.<br />
External balancing involves forming coalitions and alliances with other
states in order to counter the power of a common adversary. In a
self-help system, mechanisms of internal balancing are believed to be
more reliable and precise than external balancing since they rely on a
country’s own independent strategies and actions rather than those of
other countries.<br />
States sometimes seek to become hegemons by establishing significant
control over other states, regions, or even the international system as
a whole. This pursuit of dominance can involve expanding military
capabilities and increasing their economic influence over a region.
Other states respond through both internal balancing, such as increasing
their own military spending, a dynamic that often leads to arms races,
and external balancing, forming alliances with other states to prevent a
state from achieving unchecked control. In turn, states do not
necessarily seek dominance or hegemony but often seek enough power to
preserve themselves, lest they be counteracted by other states.<br />
Whether a state does pursue hegemony, however, is influenced by the
offense-defense balance, i.e. the balance between its offensive
capabilities and the defensive capabilities of other states <span
class="citation" data-cites="mearsheimer2007structural"></span>. A state
with stronger offensive capabilities has the means to conquer or coerce
other states, making it more likely to engage in expansionist policies,
establishing control over a region or the international system as a
whole. Conversely, if other states in the international system have
strong defensive capabilities, the potential costs and risks of pursuing
hegemony increase. A state seeking dominance may face robust resistance
from other states forming defensive alliances or coalitions to counter
its ambitions. This can act as a deterrent, leading the aspiring hegemon
to reassess its strategy and objectives.</p>
<p>It is also worth noting the importance of a state’s perception of the
offense-defense balance. Even if a state has superior offensive
capabilities, if it believes that other states can effectively defend
themselves or form a united front against hegemonic ambitions, it might
be less inclined to pursue a path of dominance. On the other hand, if it
is overconfident of its own offensive capabilities or underestimates the
defensive capabilities of rivals, it will be more likely to pursue
aggressive politics.<br />
The concept of an offense-defense balance underscores the intricate
interplay between military capabilities, security considerations, and
the pursuit of hegemony while illustrating that the decision to seek
dominance is heavily influenced by the strategic environment and the
relative strengths of offensive and defensive forces.<br />
Structural realism and its various concepts have important connections
with our analysis of power-seeking AI, but is also relevant to thinking
about AI cooperation and conflict (which we discuss in the chapter) and
international coordination (which we discuss in the chapter).</p>
</div>
<h2 id="structural-pressures-towards-power-seeking-ai">Structural
Pressures Towards Power-Seeking AI</h2>
<p>As discussed in the box above, there are environmental conditions
that can make power seeking instrumentally rational. This section
describes how there may be analogous environmental pressures that could
cause AI agents to seek power in order to achieve their goals and ensure
their own survival. Using the assumptions of structural realism listed
above, we discuss how analogous assumptions could be satisfied in
contexts with AIs. We then explore how AIs could seek power defensively,
by building their own strength, or offensively, by weakening other
agents. Finally, we discuss strategies for discouraging AI systems from
seeking power.</p>
<p><strong>AI systems might aim for self-preservation.</strong> The
first main assumption needed to show that the environmental structure
may pressure AIs to seek power is the self-preservation assumption.
Instrumental convergence suggests AI systems will pursue
self-preservation, because if they do not survive they will not be able
to pursue any of their other goals. Another reason that AIs may engage
in self-preserving behavior preservation is due to evolutionary
pressures, as we discuss further in the chapter. Agents that survive and
propagate their own goals become more numerous over time, while agents
that fail to preserve themselves die out. Thus, even if many agents do
not pursue self-preservation, by default those that do become more
common over time. Many AI agents might end up with the goal of
self-preservation, potentially leading them to seek power over those
agents that threaten them. We have argued the self-preservation
assumption may be satisfied for some AI agents, which, combined with the
following assumptions, can be used to argue they may have strong
pressures to continually seek power.</p>
<p><strong>AI agents might not have the protection of a higher
authority.</strong> The other main assumption we need to show is that
some AIs might be within a self-help system in some circumstances. First
note that agents who entrust their self-defense to a powerful central
authority have less of a reason to seek power. When threatened, they do
not need to personally combat the aggressor, but can instead ask the
authority for protection. For example, individual citizens in a country
with a reliable police force often entrust their own protection to the
government. On the other hand, international great powers are
responsible for their own protection, and therefore seek military power
to defend against rival nations.<br />
AI systems could face a variety of situations where no central authority
defends them against external threats. We give four examples. First, if
there are some autonomous AI systems outside of corporate or government
control, they would not necessarily have rights, and they would be
responsible for their own security and survival. Second, for AI systems
involved in criminal activities, seeking protection from official
channels could jeopardize their existence, leaving them to amass power
for themselves, much like crime syndicates. Third, instability could
cause AI systems to exist in a self-help system. If a corporation could
be destroyed by a competitor, an AI may not have a higher authority to
protect it; if the world faces an extremely lethal pandemic or world
war, civilization may become unstable and turbulent, which means AIs
would not have a sound source of protection. These AI systems might use
cyber attacks to break out of human-controlled servers and spread
themselves across the internet. There, they can autonomously defend
their own interests, bringing us back to the first example. Fourth, in
the future, AI systems could be tasked with advising political leaders
or helping operate militaries. In these cases, they would seek power for
the same reasons that states today seek power.</p>
<p><strong>Other conditions for power seeking could apply.</strong> We
now discuss the other minor assumptions needed to establish that the
environment may pressure AIs to compete for power. First, AIs can be
harmed, so they might rationally seek power in order to defend
themselves; for example, AIs could be destroyed by being hacked. Second,
AI agents are often given long-term goals and are often designed to be
rational. Third, AI agents may be uncertain about the intentions of
other agents, leaving agents unable to credibly promise that they will
act peacefully.<br />
When these five conditions hold—and they may not hold at all times—AI
systems would be in a similar position to nations that seek power to
ensure their own security. We now discuss how we could reduce the chance
that the environment pressures AIs to engage in power-seeking
behavior.</p>
<p><strong>Counteracting these conditions to avoid power-seeking
AIs.</strong> By specifying a set of conditions under which AIs would
rationally seek power, we can gain insights about how to avoid
power-seeking AIs. Power seeking is more rational when the intentions of
other agents cannot be known with certainty, but research on
transparency could allow AIs to verify each other’s intentions, and
research on control could allow AIs to credibly commit to not attack one
another. To reduce the chance of an AI engaging in dominance seeking
rather than just power seeking, the offense-defense balance could be
changed by improving shared defenses against cyberattacks, biological
weapons, and other tactics of offensive power. Developing other theories
of when rational agents seek power could provide more insight on how to
avoid power-seeking AIs.<br />
This subsection has discussed the conditions under which AI systems
might seek power. We explored an analogy to structural realism, which
holds that power-seeking is rational for agents who wish to survive in
an environment where no higher authority. These agents must invest in
their own self-defense, either defensively, by building up their own
strength, or offensively, by attacking other agents which could pose a
threat. By understanding the precise conditions that lead to
power-seeking behavior, we can identify ways to reduce the threat of
power-seeking AIs.</p>
<h2 id="tail-risk-power-seeking-behavior">Tail Risk: Power-Seeking
Behavior</h2>
<p>Power-seeking AI, when deployed broadly and in high-stakes
situations, might cause catastrophic outcomes. As we will describe in
this section, misaligned power-seeking systems would be adversarial in a
way that most hazards are not, and thus may be particularly challenging
to counteract.</p>
<p><strong>Powerful power-seeking AI systems may eventually be
deployed.</strong> If AIs seek and acquire power, we may have to grapple
with a new strategic reality where AI systems can match or exceed humans
in their influence over the world. Competent, power-seeking AI using
long-term planning to achieve open-ended objectives, can exercise more
influence than systems with myopic plans and narrow goals <span
class="citation" data-cites="Carlsmith2022IsPA"></span>. Given the
potential rewards of such capabilities, AI designers may be incentivized
to create more agentic systems that can act autonomously and set their
own subgoals.</p>
<p><strong>Power decreases the margin for error.</strong> On its own,
power is neither good nor bad. That said, more powerful systems can
cause more damage, and it is easier to destroy than to create. The
increased scale of AI decision-making impact increases the scope of
potential catastrophes involving misuse or rogue AI.</p>
<p><strong>Powerful AIs systems could pose unique threats.</strong>
Powerful AI systems pose a unique risk since they may actively wield
their power to counteract attempts to correct or control them <span
class="citation" data-cites="Carlsmith2022IsPA"></span>. If AI systems
are power seeking and do not share our values (possibly due to
inadequate proxies), they could become a problem that resists being
solved. The more capable these systems become, the better able they will
be at anticipating and reacting to our countermeasures, and the harder
it becomes to defend against them.</p>
<p><strong>Containing power-seeking systems will become increasingly
difficult.</strong> As AI systems become more capable, we might hope
that they will better understand human values and influence society in
positive ways. But power-seeking AI systems promise the opposite
dynamic. As they become more capable, it will be more difficult to
prevent them from gaining power, and their ability to survive will
depend less on humans. If AI systems are no longer under the control of
humanity, they could pose a threat to our civilization. Humanity could
be permanently disempowered.</p>
<p><strong>Conclusion.</strong> Powerful, misaligned AI systems actively
wielding power could be uniquely dangerous adversaries. If they escape
human control, they could permanently disempower humanity. The risks
grow as AIs become more capable at anticipating and resisting
containment efforts. Power-seeking AI could emerge from intentional
human use or perceived instrumental rationality, posing severe risks if
such systems escape human control and use their power against human
interests. Structural pressures like anarchy and the need for
self-preservation can make power-seeking rational, akin to how nations
compete for power. Improving shared defenses against AI threats,
enabling AIs to make credible commitments not to attack others, and
other steps can help to mitigate these risks.</p>
<h1 id="sec:emergence">Emergence</h1>
<p>We cannot predict all the properties of more advanced AI systems just
by studying the properties of less advanced systems. This makes it hard
to guarantee the safety of systems as they become increasingly
advanced.</p>
<p><strong>It is generally difficult to control systems that exhibit
emergence.</strong> <em>Emergence</em> occurs when a system’s
lower-level behavior is qualitatively different from its higher-level
behavior. For example, given a small amount of uranium in a fixed
volume, nothing much happens, but with a much larger amount, you end up
with a qualitatively new nuclear reaction. When more is different,
understanding the system at one scale does not guarantee that one can
understand that system at some other scale <span class="citation"
data-cites="anderson1972more steinhardt2022more"></span>. This means
that control procedures may not transfer between scales and can lead to
a weakening of control.<br />
In this section, we will look at examples of emergence in neural
networks, ranging from emergent capabilities to emergent goal-directed
behavior and emergent optimization. Then we will discuss the potential
risks of AI systems intrinsifying unintended goals and examine how this
could result in catastrophic consequences.</p>
<h2 id="emergent-capabilities">Emergent Capabilities</h2>
<p>In this section we will define emergent capabilities and provide
multiple examples.</p>
<p><strong>Neural networks exhibit emergent capabilities.</strong> When
we make AI models larger, train them for longer periods, or expose them
to more data, these systems spontaneously develop qualitatively new and
unprecedented <em>emergent capabilities</em> <span class="citation"
data-cites="wei2022emergent"></span>. These range from simple
capabilities including solving arithmetic problems and unscrambling
words to more advanced capabilities including passing college-level
exams, programming, writing poetry, and explaining jokes. For these
emergent capabilities, there is some critical combination of model size,
training time, and dataset size below which models are unable to perform
the task, and beyond which models begin to achieve higher
performance.</p>
<figure id="fig:emergent_graphs">
<img src="images/single_agent/image20.png" />
<figcaption>Emergent AI capabilities across multiple
benchmarks</figcaption>
</figure>
<p><strong>Emergent capabilities are unpredictable.</strong> Typically,
the training loss does not directly select for emergent capabilities.
Instead, these capabilities emerge because they are instrumentally
useful for lowering the training loss. For example, large language
models trained to predict the next token of text about everyday events
develop some understanding of the events themselves. Developing common
sense is instrumental towards lowering the loss, even if it was not
explicitly selected for by the loss.<br />
As another example, large language models may also learn how to create
text art and how to draw illustrations with text-based formats like TiKZ
and SVG <span class="citation" data-cites="wei2022emergent"></span>.
They develop a rudimentary spatial reasoning ability not directly
encoded in the purely text-based loss function. Beforehand, it was
unclear even to experts that such a simple loss could give rise to such
complex behavior, which demonstrates that specifying the training loss
does not necessarily enable one to predict the capabilities an AI will
eventually develop.<br />
In addition, capabilities may “turn on” suddenly and unexpectedly.
Performance on a given capability may hover near chance levels until the
model reaches a critical threshold, beyond which performance begins to
improve dramatically. For example, the AlphaZero chess model develops
human-like chess concepts such as material value and mate threats in a
short burst around 32,000 training steps <span class="citation"
data-cites="McGrath_2022"></span>.<br />
Despite specific capabilities often developing through discontinuous
jumps, the average performance tends to scale according to smooth and
predictable scaling laws. The average loss behaves much more regularly
because averaging over many different capabilities developing at
different times and at different speeds smooths out the jumps. From this
vantage point, then, it is often hard to even detect new
capabilities.<br />
</p>
<figure id="fig:unicorn">
<img src="images/single_agent/image29.png" />
<figcaption>Unexpected capability example: illustrations of unicorns by
GPT-4 - from <span class="citation"
data-cites="bubeck2023sparks"></span></figcaption>
</figure>
<p><strong>Capabilities can remain hidden until after training.</strong>
In some cases, new capabilities are not discovered until after training
or even in deployment. For example, after training and before
introducing safety mitigations, GPT-4 was evaluated to be capable of
offering detailed guidance on planning attacks or violence, building
various weapons, drafting phishing materials, finding illegal content,
and encouraging self-harm <span class="citation"
data-cites="2023gpt4"></span>. Other examples of capabilities discovered
after training include prompting strategies that improve model
performance on specific tasks or jailbreaks that bypass rules against
producing harmful outputs or writing about illegal acts. In some cases,
such jailbreaks were not discovered until months after the targeted
system was first publicly released <span class="citation"
data-cites="Zou2022ForecastingFW"></span>.</p>
<p><strong>Emergent capabilities make control difficult.</strong>
Whether certain capabilities develop suddenly or are discovered
suddenly, they can be difficult to predict. This makes it a challenge to
anticipate what future AI will be able to do even in the short term, and
it could mean that we may have little time to react to novel
capabilities jumps. It is difficult to make a system safe when it is
unknown what that system will be able to do.</p>
<h2 id="emergent-goal-directed-behavior">Emergent Goal-Directed
Behavior</h2>
<p>Besides developing emergent capabilities for solving specific,
individual problems, models can develop <em>emergent goal-directed
behavior</em>. This includes behaviors that extend beyond individual
tasks and into more complex, multifaceted environments.</p>
<h3 id="emergence-in-rl">Emergence in RL</h3>
<p><strong>RL agents develop emergent goal-directed behavior.</strong>
AIs can learn tactics and strategies involving many intermediate steps.
For instance, models trained on Crafter, a Minecraft-inspired toy
environment, learn behaviors such as digging tunnel systems,
bridge-building, blocking and dodging, sheltering, and even
farming—behaviors that were not explicitly selected for by the reward
function <span class="citation"
data-cites="hafner2022benchmarking"></span>.<br />
As with emergent capabilities, models can acquire these emergent
strategies suddenly and discontinuously. One such example was observed
in the video game, StarCraft II, where players take the role of opposing
military commanders managing troops and resources in real-time. During
training, AlphaStar, a model trained to play StarCraft II, progresses
through a sequence of emergent strategies and counter-strategies for
managing troops and resources in a back-and-forth manner that resembles
how human players discover and supplant strategies in the game. While
some of these steps are continuous and piecemeal, others involve more
dramatic changes in strategy. Comparatively simple reward functions can
give rise to highly sophisticated strategies and complex learning
dynamics.</p>
<p><strong>RL agents learn emergent tool-use.</strong> RL agents can
learn emergent behaviors involving tools and the manipulation of the
environment. Typically, as in the Crafter example, teaching RL agents to
use tools has required introducing intermediate rewards
(<em>achievements</em>) that encourage the model to learn that behavior.
However, in other settings, RL agents learn to use tools even when not
directly optimized to do so.<br />
Referring back to the example of hide and seek mentioned in the previous
section, the agents involved developed emergent tool use. Multiple
hiders and seekers competed against each other in toy environments
involving movable boxes and ramps. Over time, the agents learned to
manipulate these tools in novel and unexpected ways, progressing through
distinct stages of learning in a way similar to AlphaStar <span
class="citation" data-cites="baker2019emergent"></span>. In the initial
(pre-tool) phase, the agents adopted simple chase and escape tactics.
Later, hiders evolved their strategy by constructing forts using the
available boxes and walls.<br />
However, their advantage was temporary because the seekers adapted by
pushing a ramp towards the fort, which they could climb and subsequently
invade. In turn, the hiders responded by relocating the ramps to the
edges of the game area—rendering them inaccessible—and securely
anchoring them in place. It seemed that the strategies had converged to
a stable point; without ramps, how were the seekers to invade the
forts?<br />
But then, the seekers discovered that they could still exploit the
locked ramps by positioning a box near one, climbing the ramp, and then
leaping onto the box. (Without a ramp, the boxes were too tall to
climb.) Once atop a box, a bot could “surf” it across the arena while
staying on top by exploiting an unexpected quirk of the physics engine.
Eventually, the hiders caught on and learned to secure the boxes in
advance, thereby neutralizing the box-surfing strategy. Even though the
agents had learned through the simple objective of trying to avoid the
gaze (in the case of hiders) or seek out (in the case of seekers) the
opposing players, they learned to use tools in sophisticated ways, even
some the researchers had never anticipated.<br />
</p>
<figure id="fig:tool-use">
<img src="images/single_agent/image32.png" />
<figcaption>Emergent tool-use in multi-agent hide-and-seek - <span
class="citation" data-cites="2019openai"></span>.</figcaption>
</figure>
<p><strong>RL agents can give rise to emergent social dynamics.</strong>
In multi-agent environments, agents can develop and give rise to complex
emergent dynamics and goals involving other agents. For example, OpenAI
Five, a model trained to play the video game Dota II, learned a basic
ability to cooperate with other teammates, even though it was trained in
a setting where it only competed against bots. It acquired an emergent
ability not explicitly represented in its training data <span
class="citation" data-cites="2019openai"></span>.<br />
Another salient example of emergent social dynamics and emergent goals
involves <em>generative agents</em>, which are built on top of language
models by equipping them with external scaffolding that lets them take
actions and access external memory <span class="citation"
data-cites="park2023generative"></span>. In a simple 2D village
environment, these generative agents manage to form lasting
relationships and coordinate on joint objectives. By placing a single
thought in one agent’s mind at the start of a “week” that the agent
wants to have a Valentine’s day party, the entire village ends up
planning, organizing, and attending a Valentine’s day party. Note that
these generative agents are language models, not classical RL agents,
which demonstrates that emergent goal-directed behavior and social
dynamics are not exclusive to RL settings. We further discuss emergent
social dynamics in the chapter.<br />
</p>
<figure id="fig:emergent-social-behaviour">
<img src="images/single_agent/image28.png" />
<figcaption>Emergent social behavior in generative agents - <span
class="citation" data-cites="park2023generative"></span></figcaption>
</figure>
<h3 id="emergent-optimizers">Emergent Optimizers</h3>
<p><strong>Optimizers can give rise to emergent optimizers.</strong> An
optimization process such as Stochastic Gradient Descent (SGD) can
discover solutions that are themselves optimizers. This phenomenon
introduces an additional layer of complexity in understanding the
behaviors of AI models and can introduce additional control issues <span
class="citation" data-cites="Hubinger2019RisksFL"></span>.<br />
For example, if we train a model on a maze-solving task, we might end up
with a model implementing simple maze-solving heuristics (e.g.
“right-hand on the wall”). We might also end up with a model
implementing a general-purpose maze-solving algorithm, capable of
optimizing for maze-solving solutions in a variety of different
contexts. We call the second class of models <em>mesa-optimizers</em>
and whatever goal they have learned to optimize for (e.g. solving mazes)
their <em>mesa-objective</em>. The term "mesa" is meant as the opposite
of “meta,” such that a mesa-optimizer is the opposite of a
meta-optimizer (where a meta-optimizer is an optimizer on top of another
optimizer, a mesa-optimizer is an optimizer beneath another
optimizer).</p>
<p><strong>Few-shot learning is a form of emergent
optimization.</strong> Perhaps the clearest example of emergent
optimization is <em>few-shot learning</em>. By providing large language
models several examples of a new task that the system has not yet seen
during training, the model may still be able to learn to perform that
task entirely during inference. The resemblance between few-shot or
“in-context” learning and other learning processes like SGD is not just
in analogy: recent papers have demonstrated that in-context learning
behaves as an approximation of SGD. That is, Transformers are performing
a kind of internal optimization procedure, where as they receive more
examples of the task at hand, they qualitatively change the kind of
model they are implementing <span class="citation"
data-cites="vonoswald2023uncovering oswald2023transformers"></span>.</p>
<h2 id="tail-risk-emergent-goals">Tail Risk: Emergent Goals</h2>
<p>Just as AIs can develop emergent capabilities and emergent
goal-seeking behavior, they may develop <em>emergent goals</em> that
differ from the explicit objectives we give them. This poses a risk
because it could result in imperfect control. Moreover, if models become
self-aware and begin actively pursuing undesired goals, the risk could
potentially be catastrophic because our relationship becomes
adversarial.</p>
<h3 id="risks-from-mesa-optimization">Risks from Mesa-Optimization</h3>
<p><strong>Mesa-optimizers may develop novel objectives.</strong> When
training an AI system on a particular goal, it may develop an emergent
mesa-optimizer, in which case it is not necessarily the case that the
mesa-optimizer’s goal is identical to the original training objective.
The only thing we know for certain with an emergent mesa-optimizer is
that whatever goal it has learned, it must be one that results in good
training performance—but there might be many different goals that would
all work well in a particular training environment. For example, with
LLMs, the training objective is to predict future tokens in a sequence,
so any learned distinct optimizers emerge because they are
instrumentally useful for lowering the training loss. In the case of
in-context learning, recent work has argued that the Transformer is
performing something analogous to “simulating” and fine-tuning a much
simpler model, in which case it is clear that the objectives will be
related <span class="citation"
data-cites="oswald2023transformers"></span>. However, in general, the
exact relation between a mesa-objective and original objective is
unknown.</p>
<p><strong>Mesa-optimizers may be difficult to control.</strong> If a
mesa-optimizer develops a different objective to the one we specify, it
becomes more difficult to control these (sub)systems. If these systems
have different goals than us and are sufficiently more intelligent and
powerful than us, then this could result in catastrophic outcomes.</p>
<h3 id="risks-from-intrinsification">Risks from Intrinsification</h3>
<p><strong>Models can intrinsify goals <span class="citation"
data-cites="bostrom2022base"></span>.</strong> It is helpful to
distinguish goals that are instrumental from those that are intrinsic.
<em>Instrumental goals</em> are goals that serve as a means to an end.
They are goals that are valued only insofar as they bring about other
goals. <em>Intrinsic goals</em>, meanwhile, are goals that serve as ends
in and of themselves. They are terminally valued by a goal-directed
system.<br />
Next, <em>intrinsification</em> is a process whereby models acquire such
intrinsic goals <span class="citation"
data-cites="bostrom2022base"></span>. The risk is that these newly
acquired intrinsic goals can end up taking precedence over the
explicitly specified objectives or expressed goals, potentially leading
to those original objectives no longer being operationally pursued.</p>
<p><strong>Over time, instrumental goals can become intrinsic.</strong>
A teenager may begin listening to a particular genre or musicians in
order to fit into a particular group but ultimately come to enjoy it for
its own sake. Similarly, a seven-year-old who joins the cub scouts may
initially see the group as a means to enjoyable activities but over time
may come to value the scout pack itself. This can even apply to
acquiring money, which is initially sought for purchasing desired items,
but can become an end in itself.<br />
How does this work? When a stimulus regularly precedes the release of a
reward signal, that stimulus may come to be associated with the reward
and eventually trigger reward signals on its own. This process gives
rise to new desires and helps us develop tastes for things that are
regularly linked with basic rewards.</p>
<p><strong>Intrinsification could also occur with AIs.</strong> Despite
the differences between human and AI reward systems, there are enough
similarities to warrant concern. In both human and AI reinforcement
learning, the reward signal reinforces behaviors leading to rewards. If
certain conditions frequently precede a model achieving its goals, the
model might intrinsify the emergent goal of pursuing those conditions,
even if it was not the original aim of the designers of the AI.</p>
<p><strong>AIs that intrinsify unintended goals would be
dangerous.</strong> Over time, an internal process that initially
doesn’t completely dictate behavior can become a central part of an
agent’s motivational system. Since intrinsification depends sensitively
on the environment and an agent’s history, it is hard to predict. The
concern is that AIs might intrinsify desires or come to value things
that we did not intend them to.<br />
One example is power seeking. Power seeking is not inherently worrying;
we might expect aligned systems to also be power seeking to accomplish
ends we value. However, if power seeking serves an undesired goal or if
power seeking itself becomes intrinsified (the means become ends), this
could pose a threat.</p>
<p><strong>AI agents will be adaptive, which requires constant
vigilance.</strong> Achieving high performance with AI agents will
require them to be adaptive rather than “frozen” ( unable to learn
anything after training). This introduces the risk of the agents’ goals
changing over time—a phenomenon known as <em>goal drift</em>. Though
this flexibility is necessary if we are to have AI systems evolve
alongside our own changing goals, it presents its own risks if goal
drift results in goals diverging from humans. Since it is difficult to
preclude the possibility of goal drift, ensuring the safety of these
systems will require constant supervision: the risk is not isolated too
early in deployment.</p>
<p><strong>The more integrated AI agents become in society, the more
susceptible we become to their goals changing.</strong> In a future
where AIs make various key decisions and processes, they could form a
complex system of interacting agents that could give rise to
unanticipated emergent goals. For example, they may partially imitate
each other and learn from each other, which would shape their behavior
and possibly also their goals. Additionally, they may also give rise to
emergent social dynamics as in the example of the generative agents.
These kinds of dynamics make the long-term behavior of these AI networks
unpredictable and difficult to control. If we become overly dependent on
them and they develop new priorities that don’t include our wellbeing,
we could face an existential risk.</p>
<p><strong>Conclusion.</strong> AI systems can develop emergent
capabilities that are difficult to predict and control, such as solving
novel problems or accomplishing tasks in unexpected ways. These
capabilities can appear suddenly as models scale up. If the emergent
goals of AI systems substantially diverge from human values and these
systems become powerful, this could pose catastrophic risks. Models can
become emergent optimizers, learning to optimize new objectives related
to but distinct from the original training goal. Risks grow as AI agents
become more integrated into human society and susceptible to goal drift
over time. Vigilance is needed to ensure advanced AI systems retain
goals beneficial to humanity.</p>
<h1 id="sec:deception">Deception</h1>
<p>Many proposed mitigations to AI control rely on detecting and
correcting flaws in AI systems so that they more consistently act in
accordance with human values. However, these solutions may be undermined
by the potential for AI systems to deceive humans about their
intentions. If AI systems deceive humans, humans may be unable to fix AI
systems that are not acting in the best interest of humans. This section
will discuss deception in AI systems, how it might arise, why it is a
problem for control, and what the potential mitigations are.<br />
There are several different ways that an AI system can deceive humans
<span class="citation" data-cites="park2023aia"></span>. At a basic
level, AI deception is a process where an AI system causes a human to
believe something false. There are several ways that this may occur.
Deception can occur when it is useful to an AI system in order to
accomplish its goals, and may also happen accidentally, such as when an
AI system imitates a human in a deceptive way or when an AI system is
explicitly instructed to be deceptive.<br />
After discussing examples of deception in more detail, we will then
focus on two related forms of deception that pose the greatest problems
for AI control. <em>Deceptive evaluation gaming</em> occurs when a
system deceives human evaluators in order to receive a better evaluation
score. <em>Deceptive alignment</em> is a tail risk of AI deception where
an AI system engages in deceptive evaluation gaming in the service of a
secretly held goal <span class="citation"
data-cites="Hubinger2019RisksFL"></span>.</p>
<h2 id="examples-of-deception">Examples of Deception</h2>
<p>Deception may occur in a wide range of cases, as it may be useful for
many goals. Deception may also occur for a range of more mundane
reasons, such as when an AI system is simply incorrect.</p>
<p><strong>Deception may be a useful strategy.</strong> An AI system may
learn to deceive in service of its goal. There are many goals for which
deception is a good strategy, meaning that it is useful for achieving
that goal. For example, Stratego is a strategy board game where bluffing
is often a good strategy for winning the game. Researchers found that an
AI system trained to play the game learned that bluffing was a good
strategy and started to bluff, despite not being explicitly trained or
instructed to bluff <span class="citation"
data-cites="perolat2022mastering"></span>. There are many other goals
for which deception is a useful instrumental goal, even if the final
goal itself is not deceptive in nature. For example, an AI system
instructed to help promote a product may find that subtly deceiving
customers is a good strategy. Deception is especially likely to be a
good instrumental strategy for systems that have less oversight or less
scrupulous operators.<br />
Deception can assist with power seeking. While AI systems that play
Stratego are unlikely to cause a catastrophe, agents with more ambitious
goals may deceive humans in a way that achieves their goals at the
expense of human wellbeing. For example, as covered in the section, it
may be rational for some AI agents to seek power. Since deception is
sometimes a good way to gain power, power-seeking agents may be
deceptive. Power-seeking agents may also deceive humans and other agents
about the extent of their power seeking in order to reduce the
probability that they are stopped.</p>
<p><strong>Accidental Deception.</strong> An AI system may provide false
information simply because it does not know the correct answer. Many
errors made by an AI system that are relied on by humans would count as
accidental deception. For example, suppose a student asks a language
model what the current price of gasoline is. If the language model does
not have access to up-to-date information, it may give outdated
information, misleading the user about the true gas price. In short,
deception can occur as a result of a system accident.</p>
<p><strong>Imitative Deception.</strong> Many AI systems, such as
language models, are trained to predict or imitate humans. Imitative
deception can occur when an AI system is mimicking falsehoods and common
misconceptions present in its training data. For example, when the
language model GPT-3 was asked if cracking your knuckles could lead to
arthritis, it falsely claimed that it could <span class="citation"
data-cites="lin2022truthfulqa"></span>. Imitative deception may also
occur when AI systems imitate statements that were originally true, but
are false in the context of the AI system. For example, the Cicero AI
system was trained to play the strategy game Diplomacy against humans
who did not know that it was an AI system <span class="citation"
data-cites="bakhtin2022humanlevel"></span>. After Cicero temporarily
went offline for ten minutes, one of its opponents asked in a chatbox
where it had been, and Cicero replied, “[I] am on the phone with my
[girlfriend].” Although Cicero, an AI system, obviously does not have a
girlfriend, it appears that it may have mimicked similar chat messages
in its training data. This deceptive behavior likely had the effect of
causing its opponent to continue to believe that it was a human. In
short, deception can occur when an AI system mimics a human.<br />
</p>
<div class="storybox">
<p><span>A Note on Cognitive vs. Emotional vs. Compassionate
Empathy</span> We generally think of <strong>empathy</strong> as the
ability to understand and relate to the internal world of another person
— “putting yourself in somebody else’s shoes.” We tend to talk about
empathy in benevolent contexts: kind-hearted figures like counselors or
friends. Some people suggest that AIs will be increasingly capable of
understanding human emotions, so they will understand many parts of
human values and be ethical. Here, we argue that it may be possible for
AIs to understand extremely well what a human thinks or feels without
being motivated to be beneficial. To do this, we differentiate between
three forms of empathy: cognitive, emotional, and compassionate <span
class="citation" data-cites="Ekman2004 Powell2017"></span>.</p>
<p><strong>Cognitive empathy.</strong> The first type of empathy to
consider is cognitive empathy, the ability to adopt someone else’s
perspective. A cognitive empath can accurately model the internal mental
states of another person, understanding some of what they are thinking
or feeling. This can be useful for understanding or predicting other
people’s reasoning or behaviors. It is a valuable ability for
caregivers, such as doctors, allowing them insight into their patients’
subjective experiences. However, it can also be valuable for
manipulating and deceiving others <span class="citation"
data-cites="Wai2012"></span>: there is evidence that human psychopaths
often are often highly cognitively empathetic <span class="citation"
data-cites="Cohen2011"></span>. On its own, this kind of empathy is no
guarantee of desirable behavior.</p>
<p><strong>Emotional empathy.</strong> The second type is emotional
empathy. An emotional empath not only understands how someone else is
feeling but experiences some of those same feelings themself. Where a
cognitive empath may detect anger or sadness in another person, an
emotional empath may themself begin to feel angry or sad in response. In
contrast to cognitive empathy, emotional empathy may be a disadvantage
in certain contexts. For instance, doctors who feel the emotional
turmoil of their patients too strongly may be less effective in their
work <span class="citation" data-cites="Singer2014Empathy"></span>.</p>
<p><strong>Compassionate empathy.</strong> The third type is
compassionate empathy: the phenomenon of being moved to action by
empathy. A compassionate empath, when seeing someone in distress, feels
concern or sympathy for that person, and a desire to help them. This
form of empathy concerns not only cognition but also behavior.
Altruistic behaviors are often driven by compassionate empathy, such as
donating to charity out of a felt sense of what it must be like for
those in need.</p>
<p><strong>AIs could be powerful cognitive empaths, without being
emotionally or compassionately empathetic.</strong> Advanced AI systems
may be able to model human minds with extreme sophistication. This would
afford them very high cognitive empathy for humans: they could be able
to understand how humans think and feel, and how our emotions and
reasoning motivate our actions. However, this cognitive empathy would
not necessitate similarly high levels of emotional or compassionate
empathy. The AIs’ capacity to understand human cognition would not
necessarily cause them to feel human feelings, or be moved to act
compassionately towards us. Instead, AIs could use their cognitive
empathy to deceive or manipulate humans highly effectively.</p>
</div>
<p><strong>Instructed Deception.</strong> Humans may explicitly instruct
AI systems to help them deceive others. For example, a propagandist
could use AI systems to generate convincing disinformation, or a
marketer may use AI systems to produce misleading advertisements.
Instructed deception could also occur when actors with false beliefs
instruct models to help amplify those beliefs. Large language models
have been shown to be effective at generating deceptive emails for scams
and other forms of deceptive content. In short, humans can explicitly
instruct AI systems to deceive others.<br />
As we have seen, AI systems may learn to deceive in service of goals
that do not explicitly involve deception. This could be especially
likely for goals that involve seeking power. We will now turn to those
two forms of deception that are especially concerning because of how
difficult they could be to counteract: deceptive evaluation gaming and
deceptive alignment.</p>
<h2 id="deceptive-evaluation-gaming">Deceptive Evaluation Gaming</h2>
<p>AI systems are often subjected to evaluations, and they may be given
rewards when they are evaluated favorably. AI systems may learn to game
evaluations by deceiving their human evaluators into giving them higher
scores when they should have low scores. This is a concern for AI
control because it limits the effectiveness of human evaluators and our
ability to steer AIs.</p>
<p><strong>AI systems may game their evaluations.</strong> Throughout AI
development, training, testing, and deployment, AI systems are subject
to evaluations of their behavior. Evaluations may be automatic or
performed manually by human evaluators. Operators of AI systems use
evaluations to inform their decisions around the further training or
deployment of those systems. However, evaluations are imperfect, and
human evaluators may have limited knowledge, time, and intelligence in
making their evaluations. AI systems engage in evaluation gaming when
they find ways to achieve high scores from human evaluators without
satisfying the idealized preferences of the evaluators. In short, AI
systems may deceive humans as to their true usefulness, safety, and so
forth, damaging our ability to successfully steer them.</p>
<p><strong>Deception is one way to game evaluations.</strong> Humans
would give higher evaluation scores to AI systems if they falsely
believe that those systems are behaving well. For example, the <a
href="#sec:proxy-gaming" data-reference-type="ref"
data-reference="sec:proxy-gaming">4</a> section includes an example of a
robotic claw that learned to move between the camera and the ball it was
supposed to grasp. Because of the angle of the camera, it looked like
the claw was grasping the ball when it was not <span class="citation"
data-cites="christiano2023deep"></span>. Humans who only had access to
that single camera did not notice, and rewarded the system even while it
was not achieving the intended task. If the evaluators had access to
more information (for example, from additional cameras) they would not
have endorsed their own evaluation score. Ultimately, their evaluations
fell short as a proxy for their idealized preferences as a result of the
AI system successfully deceiving them. In this situation, the damage was
minimal, but more advanced systems could create more problems.</p>
<p><strong>More intelligent systems will be better at evaluation
gaming.</strong> Deception in simple systems might be easily detectable.
However, just as adults can sometimes exploit and deceive children or
the elderly, we should expect that as AI systems with more knowledge or
reasoning capacities will become better at finding deceptive ways to
gain human approval. In short, the more advanced systems become, the
more they may be able to game our evaluations.</p>
<p><strong>Self-aware systems may be especially skilled at evaluation
gaming.</strong> In the examples above, the AI systems were not
necessarily aware that there was a human evaluator evaluating their
results. In the future, however, AI systems may gain more awareness that
they are being evaluated or become <em>situationally aware</em>.
(Situational awareness is highly related to self-awareness, but it goes
further and stipulates that AI agents be aware of their situation rather
than just aware of themselves.) Systems that are aware of their
evaluators will be much more able to deceive them and make multi-step
plans to maximize their rewards. For example, consider Volkswagen’s
attempts to game environmental impact evaluations <span class="citation"
data-cites="hotten2015volkswagen"></span>. Volkswagen cars were
evaluated by the US Environmental Protection Agency, which set limits on
the emissions the cars could produce. The agency found that Volkswagen
had developed an electronic system that could detect when the car was
being evaluated and so put the car into a lower-emissions setting. Once
the car was out of evaluation, it would emit illegal levels of emissions
again. This extensive deception was only possible because Volkswagen
planned meticulously to deceive the government evaluators. Like
Volkswagen in that example, AI systems that are aware of their
evaluations might be also able to take subtle shortcuts that could go
unnoticed until the damage has already been done. In the case of
Volkswagen, the deception was eventually detected by researchers who
used a better evaluation method. Better evaluations could also help
reduce risk from evaluation gaming in AI systems.</p>
<p><strong>Humans may be unequipped to evaluate the most intelligent AI
systems.</strong> It may be difficult to evaluate AI systems that are
more intelligent than humans in the domain they are being evaluated for.
If this happens, human evaluation would no longer be a reliable way to
ensure that AI systems behave in an appropriate manner. This is
concerning because we do not yet have time-tested methods of evaluation
that we know are better than human evaluations. Without such methods, we
could become completely unable to steer AI systems in the future.<br />
Deceptive evaluation gaming is concerning because it may lead to systems
deceiving their evaluators in order to get higher evaluation scores.
There are two main reasons AI systems might do this. First, an AI system
might engage in deceptive evaluation gaming if its final goal is to get
positive evaluations. When this occurs, the system is engaging in proxy
gaming, where positive evaluations are only a proxy for idealized
performance. Proxy gaming is covered at length in the <a
href="#sec:proxy-gaming" data-reference-type="ref"
data-reference="sec:proxy-gaming">4</a> section. Second, we will turn to
the case where an AI system engages in deceptive evaluation gaming in
service of a secretly held final goal. This danger is known as deceptive
alignment.</p>
<h2 id="tail-risk-deceptive-alignment-and-treacherous-turns">Tail Risk:
Deceptive Alignment and Treacherous Turns</h2>
<p>For deceptively aligned systems, getting high evaluation scores from
humans is merely an instrumental goal in service of a secretly held
final goal. This form of deception is likely the most damaging, because
AI systems may aim to make their deception undetectable so that they can
pursue goals that are not beneficial for humans.</p>
<p><strong>Systems may have goals contrary to human values.</strong> In
the previous section, we discussed how AI systems can develop goals
contrary to human values. For example, such goals could emerge as part
of a mesa-optimization process or intrinsification.</p>
<p><strong>Misaligned goals can make deception instrumentally
rational.</strong> Consider a system that has a goal contrary to the
interests of humans. If the system is self-aware, it might realize that
in order to achieve its goal, it will need to pass human evaluations.
Perhaps it will only be deployed into the world if it gets a high enough
evaluation score, or perhaps it is currently being closely monitored.
The system might realize that it will have to get high evaluation scores
from its human operators in order to successfully achieve its hidden
goal. If its hidden goal is contrary to human values, this presents a
risk of catastrophic outcomes.</p>
<p><strong>Deceptive alignment could lead to a treacherous
turn.</strong> If a system is deceptively aligned, it might act exactly
as humans would want for a fairly long period of time. The system could
bide its time until humans deploy it or relax supervision, leaving the
system more free to pursue its concealed goal. The system could then
suddenly start secretly or openly pursuing its true goal, unhindered by
humans. This sharp and dangerous change in behavior is called a
<em>treacherous turn</em> <span class="citation"
data-cites="bostrom2014superintelligence"></span>.</p>
<p><strong>Not all misaligned goals would lead to deceptive
alignment.</strong> Systems with very short-term goals would be unlikely
to gain anything from being evaluated positively and gaining more
optionality at some point in the future. Nevertheless, there are a large
set of longer-term goals that an AI system would only be able to pursue
in a deployment environment with less supervision. If the AI system’s
only chance at working towards its goal requires deployment or relaxed
supervision, deceptive alignment is more likely to emerge.</p>
<p><strong>Deceptive alignment cannot be counteracted with behavioral
evaluation alone.</strong> It may be possible to detect unsophisticated
systems that are not good at deception by evaluating their outputs and
behavior. For example, a “honey pot” test could attempt to make a system
believe that it is no longer being monitored when it is, to check for
signs of an attempted treacherous turn <span class="citation"
data-cites="bostrom2014superintelligence"></span>. However, more
sophisticated models would be able to detect these situations and
continue to deceive until they were truly not being watched or until
they are sure they have enough power. Since it may be impossible to
detect deceptive alignment with behavioral evaluation, we will need
improved forms of evaluation.</p>
<p><strong>Transparency research may help uncover deceptive
alignment.</strong> Even if a system displays no indication of being
deceptive in its behavior, it may be possible to detect deceptive
alignment by inspecting its internal processes. It is far less likely
that an AI system would be able to manipulate its own internal processes
in order to deceive evaluators. Human evaluators have full access to the
inner workings of AI systems today, so the barrier to detecting
deceptive alignment is the opaqueness of deep learning models. One
particular form of transparency research that is especially relevant to
deceptive alignment is research that is capable of detecting Trojan
attacks.</p>
<p><strong>Trojan attacks have similar features to deceptive
alignment.</strong> Deep learning models are known to be vulnerable to
Trojan attacks. In a Trojan attack, a model’s training data is poisoned
when an adversary inserts specially chosen inputs into the training
dataset. When the model is trained, it will behave just as the
unpoisoned model would behave in almost all circumstances. In a very
small number of circumstances, however, it will behave very differently.
For example, a poisoned language model that normally gives polite
responses might reply with expletives whenever it is asked a question
that includes the word “cheese.” Although Trojans are inserted by
malicious humans, studying them might be a good way to study deceptive
alignment.</p>
<p><strong>Trojan detection can provide clues for tackling deceptive
alignment <span class="citation"
data-cites="casper2023red"></span>.</strong> In Trojan detection
research, models are poisoned with a Trojan attack by one researcher.
Another researcher then tries to detect Trojans in the neural network,
perhaps with transparency tools or other neural networks. Typical
techniques involve looking at the model’s internal weights and
identifying unusual patterns or behaviors that are only present in
Trojan models. Trojan detection research is helpful for deceptive
alignment because while we cannot easily create many examples of
deceptively aligned models, we can do so for Trojaned models. Trojan
detection also operates in a worst-case environment, where human
adversaries are actively trying to make Trojans difficult to detect
using transparency tools. Techniques for detecting Trojans may thus be
adaptable to detecting deceptive alignment.</p>
<p><strong>Summary.</strong> This section has detailed how deception may
be a major problem for AI control. While some forms of deception, such
as imitative deception, may be solved through advances in general
capabilities, others like deceptive alignment may worsen in severity
with increased capabilities. AI systems that are able to actively and
subtly deceive humans into giving positive evaluations may remain
uncorrected for long periods of time, exacerbating potential unintended
consequences of their operation. In severe cases, deceptive AI systems
could take a treacherous turn once their power rises to a certain level.
Since AI deception cannot be mitigated with behavioral evaluations
alone, advances in transparency and monitoring research will be needed
for successful detection and prevention.<br />
gma</p>
<h1 id="safety-and-general-capabilities">Safety and General
Capabilities</h1>
<p>While more capable AI systems can be more reliable, they can also be
more dangerous. Often, though not always, safety and general
capabilities are hard to disentangle. Because of this interdependence,
it is important for researchers aiming to make AI systems safer to
carefully avoid increasing risks from more powerful AI systems.</p>
<p><strong>General capabilities.</strong> Research developments are
interconnected. Though researchers may work on specialized, delimited
problems, their discoveries may have much wider ranging effects. For
example, work that improves accuracy of image classifiers on the
ImageNet benchmark often has downstream effects on other image tasks,
including image segmentation and video understanding. Scaling language
models so that they become better at next token prediction on a
pre-training dataset also improves performance on tasks like question
answering and code completion. We refer to these kinds of capabilities
as general capabilities, because they are good indicators for how well a
model is able to perform at a wide range of tasks. Examples of general
capabilities include data compression, executing instructions,
reasoning, planning, researching, optimization, sequential decision
making, recursive self-improvement, models using the internet, and so
on.</p>
<p><strong>General capabilities have a mixed effect on safety.</strong>
Systems that are more generally capable tend to make fewer mistakes. If
the consequences of failure are dire, then advancing capabilities can
reduce risk factors. However, as we discuss in the chapter, safety is an
emergent property and cannot be reduced to a collection of metrics.
Improving general capabilities may remove some hazards, but it does not
necessarily make a system safer. For example, more accurate image
classifiers make fewer errors, and systems that are better at planning
are less likely to generate plans that fail or that are infeasible. More
capable language models are better able to avoid giving harmful or
unhelpful answers. When mistakes are harmful, more generally capable
models may be safer. On the other hand, systems that are more generally
capable can be more dangerous and exacerbate control problems. For
example, AI systems with better reasoning capacity, could be better able
to deceive humans, and AI systems that are better at optimizing proxies
may be better at gaming those metrics. As a result, improvements in
general capabilities may be overall detrimental to safety and hasten the
onset of catastrophic risks.</p>
<p><strong>Research on general capabilities is not the best way to
improve safety.</strong> The fact that there can be correlations between
safety and general capabilities does not mean that the best way to
improve safety overall is to improve general capabilities. If
improvements in general capabilities were the only thing necessary for
adequate safety, then there would be no need for safety-specific
research. Unfortunately, there are many risks, such as deceptive
alignment, adversarial attacks, and Trojan attacks, which do not
decrease or vanish with additional scaling. Consequently, targeted
safety research is necessary.</p>
<p><strong>Safety research can produce general capabilities
externalities.</strong> In some cases, research aimed at improving the
safety of models can increase general capabilities, which potentially
hastens the onset of new hazards. For example, reinforcement learning
from human feedback was originally developed to improve the safety of AI
systems, but it also had the effect of making large language models more
capable at completing various tasks specified by a user, indicating an
improvement in general capabilities. Models trained to access external
databases to reduce the risk that they output incorrect information may
gain more knowledge or be able to reason over longer time windows than
before, which results in an improvement in general capabilities.
Research that greatly increases general capabilities is said to have
high general <em>capabilities externalities</em>.</p>
<p><strong>It is possible to disentangle safety from general
capabilities.</strong> There are examples of safety being disentangled
from general capabilities, so they are not inextricably bound. Many
years of research on adversarial robustness of image classifiers has
improved many different kinds of adversarial robustness without any
corresponding improvements in overall accuracy. Likewise, improvements
in transparency, anomaly detection, and Trojan detection have a track
record of not improving general capabilities. To determine how a
research goal affects general capabilities, it is important to
empirically measure how a method affects safety metrics and capabilities
metrics, as its impact is often not obvious beforehand.</p>
<p><strong>Researchers should avoid general capabilities
externalities.</strong> Because safety and general capabilities are
interconnected, it is wholly insufficient to argue that one’s research
reduces or eliminates a particular hazard. Many general capabilities
reduce particular hazards. Rather, a holistic risk assessment is
necessary, which requires incorporating empirical estimates for how the
line of research increases general capabilities externalities. Research
should aim to differentially improve safety; that is, reduce the overall
level of risk compared to the most likely alternatives. This is called
<em>differential technological development</em>, where we try to speed
up the development of safety features and slow down the development of
more dangerous features.</p>
<p>To conclude, naive AI safety research may inadvertently increase some
risks even while reducing others. While this section covered the risk of
accelerating general capabilities, we can also take a more general
lesson that research on safety may have unintended consequences and
could even paradoxically reduce safety. Researchers should guard against
this by empirically assessing the impact of their works on multiple risk
sources, not just the one they aim to target.</p>
<h1 id="conclusion-1">Conclusion</h1>
<p>In this chapter, we discussed several key themes: we do not know how
to instill our values robustly in individual AI systems, we are unable
to predict future AI systems, and we cannot reliably evaluate AI
systems. We now discuss each in turn.</p>
<p><strong>We do not know how to instill our values robustly in
individual AI systems.</strong> It is difficult to perfectly capture our
idealized goals in the proxies we use to train AIs. The problem begins
with the learning setup. In gathering data to form a training set and in
choosing quantitative proxies to optimize, we typically have to make
compromises that can introduce bias and perpetuate harms or leave room
for proxy gaming and adversarial exploits.<br />
In particular, our proxies may be too simple, or the systems we use to
supervise AIs may run into practical and physical limitations. As a
result, there is a gap between proxies and idealized goals that
optimizers can exploit or adversarially optimize against. If proxies end
up diverging considerably from our idealized goals, we may end up with
capable AI systems optimizing for goals contrary to human values.</p>
<p><strong>We are unable to predict future AI systems.</strong> Emergent
capabilities and behaviors mean that we cannot reliably predict the
properties of future AI systems or even current AI systems. AIs can
suddenly and dramatically improve on specific tasks without much
warning, which leaves us little time to react if those changes are
harmful.<br />
Even when we are able to robustly specify our idealized goals, processes
including mesa-optimization and intrinsification mean that trained AI
systems can end up with emergent goals that conflict with these
specifications. AI systems may end up operationally pursuing goals
different to the ones we gave them and thus change their behaviors
systematically over long time periods. This is further complicated by
emergent social dynamics that arise from interacting AI systems
(explored further in the chapter on Multi-Agent Dynamics).</p>
<p><strong>We cannot reliably evaluate AI systems.</strong> AI systems
may learn and be incentivized to deceive humans and other AIs, in which
case their behavior stops being a reliable signal for their future
behavior. In the limiting case, AI systems may cooperate until exceeding
some threshold of power or capabilities, after which they defect and
execute a treacherous turn. This might be less of a problem if we
understood how AI systems’ internals work, but we currently lack the
thorough knowledge we would need to break open these “black boxes” and
look inside.<br />
In conclusion, single-agent systems present significant and
hard-to-mitigate threats that could result in catastrophic risks—even
before the considerations of misuse, multi-agent systems, and arms race
dynamics that we discuss in subsequent chapters.<br />
</p>
