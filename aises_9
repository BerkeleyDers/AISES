<h1 id="introduction">Introduction</h1>
<p><strong>Safe artificial intelligence requires careful
governance.</strong> The development and deployment of advanced AI
systems involves many organizations and individuals with distinct goals
and incentives. These organizations and people can interact in
complicated ways, creating a complex sociotechnical system that we need
to govern effectively.<br />
There are a wide variety of issues that governance is required to manage
across different stages of AI development (from data collection through
various stages of training to deployment) and involving various actors
such as AI developers, businesses and consumers using AI systems, and
national governments, among others. Thoughtful governance provides the
constraints, incentives and institutions to steer AI progress in a
direction that benefits humanity.</p>
<p><strong>Governance refers to the rules and processes that coordinate
behavior.</strong> Governance is not just what governments do. Instead,
it can be defined more broadly as the process through which some
activity is organized, coordinated, steered, and managed. It includes
the norms, policies, and institutions that influence stakeholders’
actions to achieve socially desirable outcomes. In healthcare, for
instance, governance aimed at ensuring doctors avoid harming patients
for profit includes norms around patient care, professional ethical
standards, and licensing organizations’ ethics boards. AI regulators
should aim to encourage safety and responsibility among developers and
users of transformative technologies to ensure we avoid the risks of AIs
and instead reap its benefits.</p>
<p><strong>Governance takes many forms across sectors and
levels.</strong> Governance institutions include governmental bodies
that create laws, companies that shape internal rules, and collaborative
initiatives involving both public and private groups: legislatures pass
regulations, corporations adopt ethical guidelines, and public-private
forums establish AI safety best practices. Governance operates at
multiple levels as well, such as organizational policies, national laws,
and global agreements. Effective AI governance likely requires a
combination of approaches across sectors and levels.</p>
<p><strong>Overview.</strong> We begin by examining the ecosystem in
which governance takes place, including the relevant actors, such as
companies, governments, and individuals, and the tools available to
govern them. Then, we explore different scenarios of AI takeoff and
their implications. Next, we consider issues around the distribution of
power among AIs, access to AIs, and the distribution of costs, benefits
and risks.<br />
We then survey policy tools for AI governance at the corporate, national
and international levels. For corporations, we explore legal structures,
ownership models, and assurance mechanisms that impact AI safety. At the
national level, we consider regulations, liability frameworks,
resilience, and competitiveness. For international governance, we
examine tools ranging from standards and certification for civilian AIs
to non-proliferation agreements, verification schemes, and monopolies
for military systems. By highlighting levers across multiple levels,
this chapter provides an introduction to a range of governance
approaches aimed at ensuring AI is developed safely.</p>
<h2 id="the-landscape">The Landscape</h2>
<p>To govern AI, we must understand the system in which AIs are being
developed and deployed. Two crucial aspects for governance are the list
of actors and the tools available to govern them.</p>
<h3 id="actors">Actors</h3>
<p>AI governance involves many diverse groups across sectors that have
different goals and can do different things to accomplish them. Key
actors include companies (research or product), nonprofits, governments,
and individuals.</p>
<p><strong>Companies develop and deploy AIs, typically for
profit.</strong> Major firms such as OpenAI and Google DeepMind shape
the AI landscape through huge investments in research and development,
creating powerful models that advance AI capabilities. Startups explore
new applications of AIs and are often funded by venture capitalists. The
section looks at policies, incentives, and structures such as ownership
models of organizations that impact the development and deployment of AI
systems.</p>
<p><strong>Nonprofits play a variety of roles aimed at improving
society.</strong> Some nonprofits aim to develop safe AIs; OpenAI was
initially founded as a nonprofit. Others engage in advocacy and
coordination by bringing together companies, governments, and research
labs to collaborate on development and regulation. Academic labs such as
MILA research AI capabilities, and some researchers there aim to make AI
more beneficial. Some nonprofits perform a mix of these functions.</p>
<p><strong>National governments make laws, regulations, and
standards.</strong> National governments can directly shape AI
development and use within their jurisdictions through legislative and
regulatory powers such as market rules, public investment, and
government procurement policies; for instance, governments might require
licensing to develop large models, provide funding for AI research and
development, and require government contractors meet certain safety
standards. Governments of states such as the United States and United
Kingdom directly oversee technological development and commercialization
locally through their democratic processes and administrative
procedures. We will explore the role of governments in the section.</p>
<p><strong>International organizations facilitate cooperation between
countries.</strong> Organizations such as the United Nations, European
Union, and OECD have influence across borders by setting policies,
principles, and ethics standards that countries often implement locally.
International governance institutions allow countries to coordinate on
issues such as human rights or non-proliferation of dangerous
technologies across national borders. While international governance
mechanisms such as treaties are less enforceable than domestic tools
such as laws, they can exert soft power through financing, expertise,
norm-setting, and bringing countries together for dialogue and
consensus-building. We explore these tools in the section.</p>
<p><strong>Individuals use and are deeply impacted by AI
systems.</strong> AIs ultimately have profound impacts on individuals.
Externalities from AIs, such as invasions of privacy, concentration of
economic power, and catastrophic risks, are experienced by individuals,
as are many of the potential benefits such as rapid economic growth. As
consumers, individuals buy and use AI products and services, which means
they can apply social pressure to force development along specific
routes. As citizens, they can exert pressure through voting and other
forms of democratic representation. Their needs and perspectives should
be a central consideration.</p>
<h3 id="tools">Tools</h3>
<p>The AI governance landscape includes the sets of tools or mechanisms
by which actors interact and influence one another. Key tools for AI
governance fall into four main categories: information dissemination,
financial incentives, standards and regulations, and rights.</p>
<p><strong>Information dissemination changes how stakeholders think and
act.</strong> Education and training transmit technical skills and shape
mindsets of researchers, developers, and policymakers. Sharing data,
empirical analyses, policy recommendations, and envisioning positive
futures informs discussions by highlighting opportunities, risks, and
policy impacts. Facts are a prerequisite for the creation and
implementation of effective policy. Increasing access to information for
individuals and organizations can change their evaluations of what’s
best to do.</p>
<p><strong>Financial incentives influence behavior by changing payoffs
and motivations.</strong> Incentives such as funding sources, market
forces, potential taxes or fees, and regulatory actions shape the
priorities and cost-benefit calculations of companies, researchers, and
other stakeholders. Access to funding and well-regulated markets (such
as those with IP and competition protections) encourages technology
development and commercialization, while potential taxes or regulatory
penalties impose financial and reputational risks that promote caution
and consideration of governance goals. By shaping incentives,
governments can increase the degree to which private companies or other
actors’ opportunities and risks are aligned with those of society as a
whole.</p>
<p><strong>Standards and regulations set expectations and boundaries for
behavior.</strong> There is a spectrum of rules—-from flexible
guidelines to rigid laws—that encompasses many different governance
tools. At one end, the flexible guidelines look like standard operating
procedures and industry norms, which codify preferred practices within
and across institutions. At the other end, governments use formal and
enforceable tools including regulations and legislation, which carry
penalties for violations and aim to both address specific risks and
shape industries broadly. Well-designed rules establish which actions
are allowed or prohibited for organizations and individuals.</p>
<p><strong>Rights grant freedoms, entitlements, and claims over
assets.</strong> Human rights and civil liberties, such as privacy, free
speech, and due process, shape relationships between individuals and
organizations by defining unacceptable behaviors. Property rights and
intellectual property regimes determine ownership and control over
assets such as data, algorithms, trained models, and compute. Outlining
rights clearly can establish strong governance regimes that allow
litigation against any infringements.<br />
The diverse actors in AI development and deployment along with the
varied governance tools at our disposal form an intricate ecosystem.
Companies, researchers, governments, and individuals influence progress
based on their capabilities and incentives. Financial incentives,
established rules, delineated rights, and information sharing steer
beliefs and behaviors. Next, we will consider an issue central to
defining the landscape in which governance will take place: at what rate
should we expect AIs to achieve superhuman capabilities?</p>
<h1 id="takeoff">Takeoff</h1>
<p>In this section, we will explore the various ways in which AIs might
see “takeoff” and how this would affect the world. Takeoff refers to the
process of AIs going from being able to perform well on many tasks at a
human or subhuman level to being super-intelligent, surpassing human
capabilities. There is serious uncertainty about how AI development will
play out in the future. First, we will discuss how the development of
AIs can speed up the economy. Then, we will discuss why we cannot rule
out that advanced AIs might arrive soon. Lastly, we will discuss how AI
takeoff could be continuous or discontinuous and fast or slow.
Navigating these uncertainties is essential for formulating AI safety
policies that are robust to the various routes AI development might take
in the future.</p>
<h2 id="economic-growth">Economic Growth</h2>
<p>Economic growth has been limited by the growth rate of the human
population. By enabling cheap and scalable automation, AIs that perform
tasks at a human level might artificially augment the effective economic
population, significantly speeding up economic growth beyond anything we
have seen before.</p>
<p><strong>AI automation could create explosive growth.</strong> If we
add AIs to standard models of economic growth (such as the Solow model)
developed using economic theory and past data, we find that AIs could
trigger a dramatic surge in economic growth. AIs that can substitute for
human labor can augment a labor force by providing inexpensive and
highly scalable automation. Some studies suggest that such AIs could
spark unprecedented growth, causing the world economy to grow at rates
exceeding ten times the current growth rate <span class="citation"
data-cites="Davidson-growth"></span>. This could result in an
unprecedented acceleration of scientific and technological advancement,
reshaping our economy and the trajectory of human history over a period
of perhaps only a few years. Growth of this magnitude would be unlike
anything in human history: for the past 10,000 years after the
agricultural revolution, total world output grew at 0.1% per year,
steadily increasing over the last 1000 years to single-digit percentage
points.</p>
<p><strong>Population growth may drive economic growth.</strong> This
worldwide economic acceleration in recent centuries has been variously
attributed to the unique conditions of the industrial revolution, the
technologies developed in 18th and 19th century Europe, and the growth
in total population over time. Population growth is emphasized most by
the <em>semi-endogenous theory of economic growth</em>. It holds that
since economic growth causes population growth by reducing bottlenecks
on population growth and population growth causes economic growth by
providing a large labor force (including an increasing number of
researchers driving technological progress), there is a positive
feedback loop and so population growth is the key factor to consider
when looking at the increase in the economic output over time.<br />
According to this theory, human population growth was determined for
many thousands of years by the availability of food. As agricultural
technologies were developed, food became easier to produce, which
allowed for more population growth. Since larger populations have more
opportunities to innovate and develop better technology, this process
loops back into itself recursively, producing a faster-than-exponential
development curve over the long run.<br />
This acceleration ultimately slowed down in the mid-20th century. The
semi-endogenous theory explains this slowdown as a result of the
independent decline in the population growth rate. Demographic changes
such as falling birth rates uncoupled productivity and population
growth. This helps explain why economic growth did not explode in the
late 20th and early 21st centuries. If the population bottleneck were
lifted, the multi-thousand-year trend of accelerating growth could
continue again until we exhaust physical resources like energy and
space.</p>
<figure id="fig:homicide">
<embed src="images/governance/growth.pdf" />
<figcaption>Growth of economic output in two scenarios: growth slowdown,
and AI-driven growth explosion</figcaption>
</figure>
<p><strong>AIs could fuel effective population growth.</strong> If AIs
can automate the majority of important human tasks (including further AI
development), this would lift the bottleneck on labor that has prevented
explosive growth. There are strong reasons to think that AIs could boost
the economic growth rate by substituting for human labor. As easily
duplicable software, the AI population can grow at least as quickly as
we can manufacture hardware to run it on—-much faster than humans take
to reproduce and learn skills from adults. Additionally, the growth of
AI capabilities driven by design is much faster than the growth of human
capabilities driven by evolution: over the entire course of human
evolution from primates, human brains grew roughly 4 times in size,
whereas over the decade after AlexNet, the largest machine learning
models increased in size by the same amount roughly every 16 months.
Assuming we do not see a slowdown AI development or face other
bottlenecks like energy production, AIs may cause explosive growth by
lifting the population bottleneck.</p>
<h2 id="timelines">Timelines</h2>
<p>Advanced AIs could have significant impacts on our civilization such
as the dramatic and unprecedented acceleration of economic growth.
However, it is unclear when we will see such AIs being deployed. It is
comfortable to believe that we are nowhere close to creating advanced
AIs; however, given uncertainty among experts and current trends in
compute and algorithmic efficiency, we do not have strong reasons to
rule out the possibility that advanced AIs will exist in the near
future.</p>
<p><strong>There is high uncertainty about when advanced AIs will
exist.</strong> Opinions on “timelines”—–how difficult it will be to
create human-level AI–—are famously variable among experts, although
recent indications demonstrate rapid progress. In 2023, many
high-profile AI scientists such as Geoffrey Hinton (who was awarded the
Turing award in 2018 for his contributions to deep learning) argued that
AI development is moving quicker than expected. AI systems are now able
to perform a variety of tasks that were once treated as very hard, such
as providing explanations for jokes or producing artistic
creations.<br />
Nonetheless, it is worth being cautious about interpreting evidence of
rapid growth over a short period too narrowly. In the 1950s and 1960s,
many top AI scientists such as Marvin Minsky were overly optimistic
about what was achievable in the short term. In light of these
considerations, it seems wise to maintain uncertainty about how quickly
advanced AIs will arrive.</p>
<p><strong>The compute requirements of advanced AIs might not be
prohibitively expensive.</strong> An objection to short
timelines—–expecting that advanced AIs will arrive quickly—–is that they
may be very expensive to run, which may delay their deployment until the
price of compute falls to sufficiently affordable levels. However, this
seems unlikely: early LLMs, such as GPT-3.5, were relatively cheap,
requiring approximately one TFLOP per query, which cost less than
$0.00001 at the time. This is substantially less than human wages to
perform equivalent services such as answering questions.<br />
One estimate puts the human brain’s computational speed at around one
PFLOP (or 1000 TFLOPs) <span class="citation"
data-cites="cotra2020forecasting"></span>. Suppose we thought that
machine learning models required similar compute to achieve human-level
capabilities: AIs would then be able to work at a few dollars an hour at
2023 prices, and substantially less in the future (assuming prices
continue to fall as they have over the past several decades). While this
analysis is far from complete and highly uncertain, it gives us reason
to believe that advanced AIs would not necessarily be delayed because of
the costs of compute.</p>
<p><strong>Algorithms are quickly becoming more efficient.</strong>
Algorithmic progress refers to the process of making AIs more efficient
to train and run. Recent studies have determined that algorithmic
progress is roughly as important as compute for explaining progress in
machine learning, and that both have increased rapidly <span
class="citation" data-cites="erdil2023algorithmic"></span>. Not much is
known for sure about what drives algorithmic progress, but there are
several possibilities, such as theoretical work on refining existing and
finding new ideas, applying ideas to large-scale models, and engineers
experimenting with different algorithms over time. The truth is likely a
combination of these and other factors. If the last two possibilities
are true, then it would indicate that scale, and thus compute,
ultimately drives algorithmic progress. This would make compute by far
the most important contributor to AI progress overall.</p>
<p><strong>Advanced AIs may arrive soon.</strong> In short, we observe
that compute and algorithms are two key factors used for AI production
that are both advancing rapidly over time. While there might be
significant bottlenecks, such as constraints on the ability to acquire
or spend on compute, there are no decisive reasons why we should think
advanced AIs arriving soon is impossible. This doesn’t imply that
advanced AIs are right around the corner–—but it does mean that we
should plan for this possibility.</p>
<h2 id="types-of-ai-takeoff">Types of AI Takeoff</h2>
<p>AI takeoff could be <em>discontinuous</em>, involving emergent
abilities that break from trendlines, or <em>continuous</em>, involving
a smooth increase in capabilities over time. Relatedly, we are unsure
how fast these abilities will develop: one view is that early progress
in AI development will speed up later progress, creating exponential
growth that leads to a fast takeoff.</p>
<p><strong>Could important AI capabilities appear
discontinuously?</strong> General measures of AI performance have tended
to show fast but smooth increases in average performance on many tasks
as we increase model and dataset size. This is true over many benchmarks
and metrics. However, researchers have identified a number of tasks that
appear to break this trend, and exhibit sharp increases in performance
when increasing model size; these tasks are called emergent
capabilities, explored in the chapter.<br />
If important abilities (like possession of a mental model of the world)
were emergent in this sense, we should be concerned about the
unpredictable emergence of dangerous capabilities like hacking or
deception. This scenario is often referred to as a <em>discontinuous AI
takeoff</em> because of the abrupt break from past trends that it would
represent. It is necessary to do more research to find out how common
these emergent abilities are, and whether they can be predicted ahead of
time. The alternative is often called a <em>continuous</em> or
<em>gradual</em> <em>AI takeoff</em>, wherein abilities predictably
scale with size.</p>
<p><strong>How fast will AIs have a transformational effect on human
civilization?</strong> Those who believe a fast takeoff is likely expect
that we could see a leap from minimal AI automation to a world deeply
transformed by AI in mere weeks or months. They typically expect this to
be a result of recursive self-improvement, AIs continuously improving
their own abilities through research into development. If we possess an
AI that can carry out development research at a human level, then it
could keep refining itself, becoming more adept at its research
endeavors. This, in turn, would make it even more efficient at
self-enhancement, creating a positive feedback loop. Proponents of fast
takeoff argue that such an AI could rapidly progress from human-like
intelligence to vastly superior levels of cognition due to this
recursive, escalating process.<br />
Yet, this confidence might be misplaced. There are already instances of
AIs enhancing other AIs’ capabilities. For example, AI models adept at
tasks like programming, writing, and standardized testing have driven
down computing costs, facilitated data labeling, and augmented coding
skills. However, these developments have only slightly accelerated the
pace of AI advancements. This implies that even if AI models achieve
human-like capabilities across a wide array of tasks, their recursive
self-improvement might still be a gradual progression. Those who believe
in slow takeoff tend to think that this process will take years.<br />
Note that both camps in this debate think that AI will profoundly change
the world. The question is, how quickly will we find ourselves in a
radically altered society? Generally speaking, a slow takeoff would
likely be safer, giving the world more time to understand and react to
the situation with safety measures. While the current evidence points
towards a slow takeoff, there is currently no consensus on which
scenario is more likely.</p>
<h3 id="conclusions-about-takeoff">Conclusions About Takeoff</h3>
<p>We have considered the different routes AI development might take to
create superintelligent AIs. First, we explored the effects AIs will
have on the economy, examining how they might supercharge economic
growth through automation, revolutionizing our world. Then, we
considered timelines: whether advanced AIs are likely to arrive soon.
Considering the vast uncertainty in experts’ best guesses and the lack
of fundamental restrictions on factors like compute and algorithms that
are necessary for AI production, we cannot assume that advanced AIs are
a thing of the distant future.<br />
Lastly, we considered uncertainties about how AIs would go from
performing at a human level to being superintelligent. We briefly
examined evidence about trends in the development of AI, discovering
that it is mixed, suggesting that most AI capabilities usually increase
smoothly with scale but some are emergent. We also discussed the
phenomenon of recursive self-improvement, which might lead to rapid
takeoff speeds in the future. Understanding how takeoff might play out
could greatly inform our approach to the regulation of AI. Next, we will
consider matters of distribution: of power, access, and costs and
benefits.</p>
<h1 id="distribution">Distribution</h1>
<p>In this section we discuss three main dimensions of how aspects of AI
systems are distributed:</p>
<ol>
<li><p>Power: whether there will be a few highly sophisticated AIs with
vastly more power than the rest, or many highly capable AIs.</p></li>
<li><p>Access: whether powerful AIs will be kept in the hands of a small
group of people, or whether they will be widely accessible to the
general public.</p></li>
<li><p>Benefits and costs: whether the benefits and costs of AIs will be
evenly or unevenly shared.</p></li>
</ol>
<p>We will see that both high <em>centralization</em> and
<em>decentralization</em> of AIs present different risks. Centralization
of power and access would allow dangerous actors to pursue their goals
with no opposition. Decentralized power and access, on the other hand,
may lead to dangerous multi-agent dynamics. We will see that by default
on current trends it is plausible that the benefits of AIs would accrue
to only a few powerful companies, while the costs would be shared by all
of society. Governance solutions may help redistribute AI’s benefits
fairly, and protect society from its costs.</p>
<h2 id="distribution-of-power-among-ais">Distribution of Power Among
AIs</h2>
<p>The first dimension we will consider is how power might be
distributed among AIs themselves. We will contrast two scenarios: one in
which a single AI has enduring decisive power over all other AIs and
humans, and one in which there are many different powerful AIs. We will
look at the factors that could make each situation more likely to
emerge, the risks we are most likely to face in each case, and the kinds
of policies that might be appropriate to mitigate them.</p>
<h3 id="ai-singleton">AI singleton</h3>
<p>One possible future scenario is the emergence of an AI singleton—–an
AI with vastly greater power than all others, to the extent that it can
permanently secure its power over the others <span class="citation"
data-cites="bostrom2006singleton"></span>.</p>
<p><strong>A monopoly on AI could make a single powerful AI more
likely.</strong> One factor affecting the number of powerful AIs that
emerge is the number of actors that can independently develop AIs of
similar capabilities. If a single organization, whether a government or
a corporation, were to achieve a monopoly on the development of highly
sophisticated AI, this would increase the likelihood of a single AI
emerging with decisive and lasting power over all individuals.</p>
<p><strong>A fast takeoff could make a single powerful AI more
likely.</strong> If an AI were to undergo a fast takeoff, where its
capabilities suddenly grew to surpass other intelligences, then it could
prevent other existing AIs from going through the same process. Such an
AI might be motivated to destroy any potential threats to its power and
secure permanent control, ensuring it could pursue its goals unimpeded.
On the other hand, if intelligence were to progress more gradually, then
there would not be a window of time where any single AI was sufficiently
more powerful than the others to halt their further development. Note,
however, that a fast takeoff does not necessitate one AI becoming a
permanent singleton. That is because AIs may still be vulnerable even if
they are extremely powerful. Simple structures can take down more
complex structures; just as humans are vulnerable to pathogens and
chemical weapons, simpler AIs (or humans) might be able to counteract
more capable AIs.</p>
<p><strong>An AI singleton could reduce competitive pressures and solve
collective action problems.</strong> If an AI singleton were to emerge,
the actor in control of it would not face any meaningful competition
from other organizations. In the absence of competitive pressures, they
would have no need to try to gain an advantage over rivals by rushing
the development and deployment of the technology. This scenario could
also reduce the risk of collective action problems in general. Since one
organization would have complete control, there would be less potential
for dynamics where different entities chose not to cooperate with one
another leading to a negative overall outcome.</p>
<p><strong>An AI singleton increases the risk of single points of
failure.</strong> In a future scenario with only one superintelligent
AI, a failure in that AI could be enough to cause a catastrophe. If, for
instance, it were to start pursuing a dangerous goal, then it might be
more likely to achieve it than if there were other similarly powerful
AIs that could counteract it. Similarly, if a human controlling an AI
singleton would like to lock in their values, they might be able to do
so unopposed. Therefore, an AI singleton could represent a single point
of failure.</p>
<p><strong>An AI singleton could increase the risk of human
disempowerment.</strong> If there were just one superintelligent AI and
it sought to capture global power, it would not have to overpower other
superintelligent AIs in order to do so. If, instead, there were multiple
powerful AIs, humans might be able to cooperate with those that were
more willing to cooperate with humans. However, an AI singleton would
have little reason to cooperate with humans, as it would not face any
competition from other AIs. This scenario would therefore increase the
risk of single-agent control and the disempowerment of humanity.</p>
<h3 id="diverse-ecosystem-of-ais">Diverse Ecosystem of AIs</h3>
<p>An alternative possibility is the emergence of a diverse ecosystem of
similarly capable AIs, in which no single agent is significantly and
sustainably more powerful than all the others combined. An AI singleton
might also not occur if there is turnover amongst the most powerful AIs
due to the presence of vulnerabilities. Just as human empires rise and
fall, AIs may gain and lose power to others.</p>
<p><strong>Declining development costs could make multiple AIs more
likely.</strong> If the costs associated with developing AIs diminish
considerably over time, then more actors will be able to develop AIs
independently of one another. Also, if there aren’t increasing returns
from intelligence in many economic niches, then many businesses will
settle for the minimum necessary capable AIs. That is, an AI intended to
cook fast food may not benefit from knowing advanced physics. This
increases the probability of a future where multiple AIs coexist.</p>
<p><strong>A diverse ecosystem of AIs might be more stable than a single
superintelligence.</strong> There are reasons to believe that a diverse
ecosystem of AIs would be more likely to establish itself over the long
term than a single superintelligence. The general principle that
variation improves resilience has been observed in many systems. In
agriculture, planting multiple varieties of crops reduces the risk that
all of them will be lost to a single disease or pest. Similarly, in
finance, having a wide range of investments reduces the risk of large
financial losses. Essentially, a system comprising many entities is less
vulnerable to collapsing if a single entity within it fails.<br />
There are multiple additional advantages that a diverse ecosystem of AIs
could have over a single superintelligence. Variation within a
population means that individuals can specialize in different skills,
making the group as a whole better able to achieve complex goals that
involve multiple different tasks. Such a group might also be generally
more adaptable to different circumstances, since variation across
components could offer more flexibility in how the system interacts with
its environment. The “wisdom of the crowds” theory posits that groups
tend to make better decisions collectively than any individual member of
a group would make alone. This phenomenon would be true of groups of
AIs. For all these reasons, a future involving a diverse ecosystem of
AIs may be more likely than one where a single powerful AI gains
decisive power.</p>
<p><strong>Diverse AIs could remove single points of failure.</strong>
Having multiple diverse AIs could dilute the negative effects of any
individual AI failing to function as intended. If each AI were in charge
of a different process, then they would have less power to cause harm
than a single AI that was in control of everything. Additionally, if a
malicious AI started behaving in dangerous ways, then the best chance of
preventing harm might involve using similarly powerful AIs to counteract
it, such as through the use of “watchdog AIs” tasked with detecting such
threats. In contrast with a situation where everything relies on a
single AI, a diverse ecosystem reduces the risk of single points of
failure.</p>
<p><strong>Multi-agent dynamics could lead to selfish traits.</strong>
Having a group of diverse AIs, as opposed to just one, could create the
necessary conditions for a process of evolution by natural selection to
take effect (). This might cause AIs to evolve in ways that we would not
necessarily be able to predict or control. In many cases, evolutionary
pressures have been observed to favor selfish traits in biological
organisms. The same mechanism might promote AIs with undesirable
characteristics.</p>
<p><strong>Diverse AIs could increase the risk of unanticipated
failures.</strong> A group of AIs interacting with one another would
form a complex system, and could therefore exhibit collective emergent
properties that could not be predicted from understanding the behavior
of just one. A group of AIs might therefore increase the risk of black
swan events (Section <a href="#sec:black-swans"
data-reference-type="ref"
data-reference="sec:black-swans">[sec:black-swans]</a>). Additionally,
interactions between AIs could form feedback loops, increasing the
potential for rapid downward spirals that are difficult to intervene and
stop. A group of powerful AIs in control of multiple military processes
could, for example, present a risk of a flash war, resulting from a
feedback loop of adversarial reactions.</p>
<p><strong>Diverse AI ecosystems could exhibit failure modes of AI
singletons.</strong> If multiple AI systems collude with one another, or
if inequality amongst AIs is significant such that one or a few are much
more powerful than others, risks will mirror those of an AI singleton.
We will examine why collusion and inequality may occur, and the
implications.</p>
<p><strong>Multiple AIs may or may not collude.</strong> It has been
proposed that if there were multiple highly capable AIs, they would
collude with one another, essentially acting as a single powerful AI
<span class="citation" data-cites="Drexler2019"></span>. This is not
inevitable. The risk of collusion depends on the exact environmental
conditions.<br />
Some circumstances that make collusion more likely and more successful
include:</p>
<ol>
<li><p>A small number of actors being involved.</p></li>
<li><p>Collusion being possible even if some actors cease to
participate.</p></li>
<li><p>Colluding actors being similar, for example in terms of their
characteristics and goals.</p></li>
<li><p>Free communication between actors.</p></li>
<li><p>Iterated actions, where each actor can observe what another has
done and respond accordingly in their next decision.</p></li>
<li><p>All actors being aware of the above circumstances.</p></li>
</ol>
<p>Conversely, some circumstances that impede collusion include:</p>
<ol>
<li><p>A large number of actors being involved.</p></li>
<li><p>A requirement for every single actor to participate in order for
collusion to succeed.</p></li>
<li><p>Dissimilarity among colluders, perhaps having different histories
and conflicting goals.</p></li>
<li><p>Limited communication between actors.</p></li>
<li><p>Processes involving only one step where actors cannot observe
what other actors have done and respond in a future action.</p></li>
<li><p>Uncertainty about the above circumstances.</p></li>
</ol>
<p><strong>Power among AIs may be distributed unequally.</strong> AIs
may follow a long-tail distribution, like wealth among humans in the US.
It is therefore important to note that even if we have many diverse AIs
of similar capabilities, power may still end up being concentrated in
just a few that have a slight edge, and the impact of AI may be largely
determined by only a few. There are situations short of an AI singleton
where power is mainly concentrated in one or a few AIs.</p>
<h2 id="distribution-of-access-to-ais">Distribution of Access to
AIs</h2>
<p>We have discussed different possible distributions of power among
AIs, but this is not the only axis of distribution we need to consider;
there is also the question of how access to the most powerful AIs will
be distributed among humans. For example, even if there is an AI
singleton, we still might have a situation where everyone could use it.
Conversely, even if there were multiple similarly capable models, their
use might nevertheless be restricted to a small number of people.<br />
We will discuss wide access through open source, and narrow access
through restricted models, as well as striking a balance between the two
through structured access. We will examine the safety implications of
each level of access.</p>
<h3 id="levels-of-access">Levels of Access</h3>
<p><strong>Restricted AI models concentrate power.</strong> Restricted
AI models are those that can only be used by a small group of people.
They may, for example, be exclusively accessible to people working in
private companies, government agencies, or a small group of people with
national security clearances. Restricted models cannot be used by the
general public.<br />
While some AIs could be restricted, it is possible that a significant
number of highly capable AIs will be tightly restricted. If all AIs, or
at least the most powerful, are restricted models, then power would be
concentrated in the hands of a small number of people. This could
increase the risk of value lock-in, where the values of that small group
of people would be promoted and perpetuated, potentially irreversibly,
even if they did not represent the interests of the larger human
population.</p>
<p><strong>Open-source AI models increase the risks of malicious
use.</strong> Open-source AI models are those that are freely available
for anyone to use. There are no restrictions on what people can use them
for, or how users can modify them. These AI systems would proliferate
irreversibly. Since open-source models are, by definition, accessible to
anyone who can run them without restrictions on how they can be used,
there is an increased probability of malicious use.</p>
<p><strong>If information security is poor, AIs are effectively
open-source.</strong> Robust cybersecurity will be required to prevent
unintended users from accessing powerful AIs. Inadequate protections
will mean that AIs are implicitly open-source even if they are not
intended to be, because they will likely be leaked or stolen.</p>
<p><strong>Structured access.</strong> One possible option for striking
a balance between keeping AIs completely private and making them fully
open-source would be to adopt a <em>structured access approach</em>.
This is where the public can access an AI, but with restrictions on what
they can use it for and how they can modify it. There may also be
restrictions on who is given access, with “Know Your Customer” policies
for verifying users’ identities. In this scenario, the actor controlling
the AI has ultimate authority over who can access it, how they can
access it, what they can use it for, and if and how they can modify it.
They can also grant access selectively for other developers to integrate
the AI within their own applications, with consideration of these
developers’ safety standards.<br />
One practical way of implementing structured access is to have users
access an AI via an application programming interface (API). This
indirect usage facilitates controls on how the AI can be used and also
prevents users from modifying or building on it. The rollout of GPT-3 in
2020 is an example of this style of structured access: the large
language model was stored in the cloud and available for approved users
to access indirectly through a platform controlled by OpenAI.</p>
<h3 id="openness-norms">Openness Norms</h3>
<p>Traditionally, the norm in academia has been for research to be
openly shared. This allows for collaboration between researchers in a
community, enabling faster development. While openness may be a good
default position, there are certain areas where it may be appropriate to
restrict information sharing. We will now discuss the circumstances
under which these restrictions might be justified and their relevance to
AI development.</p>
<p><strong>There are historical precedents for restricting information
sharing in dual-use research.</strong> Dual-use technologies are those
that can be used for both beneficial and harmful purposes. It is not a
new idea that information about the development of such technologies
should not be widely shared. In the 1930s, publication of research on
the nuclear chain reaction, which could be used for both nuclear power
and nuclear weapons, prompted a Nazi program developing the latter. The
Manhattan Project was then conducted in secrecy to avoid enemy
intelligence learning of any breakthroughs. Biotechnology has also been
subject to debate about the appropriate level of openness, with concerns
around the publication of papers detailing potential methods for
creating bioweapons.<br />
Powerful AIs would be a dual-use technology and therefore deserves
serious consideration of who can be trusted with it. Absolute openness
means implicitly trusting anyone who has the necessary hardware to use
AIs responsibly. However, there could in future be many people with
sufficient means to deploy AIs, and it might only take one person with
malicious intent to cause a catastrophe.</p>
<p><strong>Technological progress may be too fast for regulations to
keep up.</strong> Another reason for restricting information sharing is
the pacing problem—–where technological progress happens too quickly for
policymakers to devise and implement robust controls on a technology’s
use. This means that we cannot rely on regulations and monitoring to
prevent misuse in an environment where information that could enable
misuse is being openly shared.</p>
<p><strong>It may be difficult to predict the type of research that is
potentially dangerous.</strong> Within AI research, there are different
kinds of information, such as the model weights themselves and the
methods of building the system. There have been cases where the former
has been restricted for safety reasons but the latter openly shared.
However, it seems feasible that information on how to build dangerous
AIs could also be used to cause harm.<br />
Moreover, it can be difficult to predict exactly how insights might be
used and whether they are potentially dangerous. For instance, the
nuclear reactor, which could help society create more sustainable
energy, was instrumental in developing a cheaper version of the atomic
bomb. It is possible that AIs designed for seemingly harmless tasks
could be used to propel the advancement of potentially dangerous AIs. We
may not be able to predict every way in which technologies that are
harmless in isolation might combine to become hazardous.</p>
<p><strong>Since there are costs to restrictions, it is worth
considering when they are warranted.</strong> Any interventions to
mitigate the risk of misuse of AIs are likely to come at a cost, which
may include users’ freedom and privacy, as well as the beneficial
research that could be accelerated by more open sharing. It is therefore
important to think carefully about which kind of restrictions are
justified, and in which scenarios.<br />
It might, for example, be worth comparing the number of potential
misuses and how severe they would be with the number of positive uses
and how beneficial they would be. Another factor that could be taken
into account is how narrowly targeted an intervention could be, namely
how accurately it could identify and mitigate misuses without
interfering with positive uses.<br />
Restrictions on the actual capabilities of an AI tend to be more general
and less precisely targeted than interventions implemented downstream.
The latter may include restrictions on how a user accessing an AI
indirectly can use it, as well as laws governing its use. However,
capability restrictions may be warranted under specific conditions. They
may be needed if interventions at later stages are insufficient, if the
dangers of a capability are particularly severe, or if a particular
capability lends itself much more to misuse than positive use.</p>
<h3 id="risks-from-open-vs-controlled-models">Risks From Open vs
Controlled Models</h3>
<p>Open models would enable dangerous members of the general public to
engage in harmful activities. Tightly controlled models exacerbate the
risk that their creators, or elites with special access, could misuse
them with impunity. We will examine each possibility.</p>
<p><strong>Powerful, open AIs lower the barrier to entry for many
harmful activities.</strong> There are multiple ways in which
sophisticated AIs could be harnessed to cause widespread harm. They
could, for example, lower the barrier to entry for creating biological
and chemical weapons, conducting cyberattacks like spear phishing on a
large scale, or carrying out severe physical attacks, using lethal
autonomous weapons. Individuals or non-state actors wishing to cause
harm might adapt powerful AIs to harmful objectives and unleash them, or
generate a deluge of convincing disinformation, to undermine trust and
create a more fractured society.</p>
<p><strong>More open AI models increase the risk of bottom-up
misuse.</strong> Although the majority of people do not seek to bring
about a catastrophe, there are some who do. It might only take one
person pursuing malicious intentions with sufficient means to cause a
catastrophe. The more people who have access to highly sophisticated
AIs, the more likely it is that one of them will try to use it to pursue
a negative outcome. This would be a case of <em>bottom-up misuse</em>,
where a member of the general public leverages technology to cause
harm.</p>
<p><strong>Some AI capabilities may be skewed in favor of offense over
defense.</strong> It could be argued that AIs can also be used to
improve defenses against these various threats. However, some misuse
capabilities may be skewed in favor of offense not defense. For example,
it may be much easier to create and release a deadly pathogen than to
control it or come up with cures or vaccines. Even if an AI were to
facilitate faster vaccine development, a bioweapon could still do a
great deal of harm even in a short timeframe, leading to many deaths
before the vaccine could be discovered and rolled out.</p>
<p><strong>Releasing highly capable AIs to the public may entail a risk
of black swans.</strong> Although numerous risks associated with AIs
have been identified, there may be more that we are unaware of. AIs
themselves might even discover more technologies or ways of causing harm
than humans have imagined. If this possibility were to result in a black
swan event (see Section <a href="#sec:black-swans"
data-reference-type="ref"
data-reference="sec:black-swans">[sec:black-swans]</a> for a deeper
discussion of black swans), it would likely favor offense over defense,
at least to begin with, as decision makers would not immediately
understand what was happening or how to counteract it.</p>
<p><strong>More tightly controlled models increase the risk of top-down
misuse.</strong> In contrast with bottom-up misuse by members of the
public, <em>top-down misuse</em> refers to actions taken by government
officials and elites to pursue negative outcomes. If kept in the hands
of a small group of people, powerful AIs could be used to lock in those
people’s values, without consideration of the interests of humanity more
broadly. Powerful AIs could also increase governments’ surveillance
capacity, potentially facilitating democratic backsliding or
totalitarianism. Furthermore, AIs that can quickly generate large
quantities of convincing content and facilitate large-scale censorship
could hand much greater control of media narratives to people in power.
In extreme cases, these kinds of misuse by governments and elites could
enable the establishment of a very long-lasting or permanent dystopian
civilization.</p>
<h2 id="distribution-of-costs-and-benefits-of-ai">Distribution of Costs
and Benefits of AI</h2>
<p>We considered distributions of power among AIs and people’s
distributions of access to AIs. However, another important consideration
is how the costs and benefits associated with AIs might be distributed.
The distribution of the costs and benefits of AIs will ultimately depend
on both market forces and governance decisions. It is possible that big
tech companies will receive most of the economic gains from AIs they
control, while in the long term AI could spell the end of economically
valuable work for many or all humans. By redistributing wealth, the
government can ensure that even if only a small number of people
directly gain wealth from AIs, wealth is eventually shared more broadly
among the population.<br />
First, we will focus on the potential costs of automation: unprecedented
levels of unemployment and inequality. Conversely, we will consider ways
that governance could help equitably distribute economic gains from
automation. Second, we will examine moral hazards, scenarios where
agents take risks because someone else will bear the consequences. We’ll
see that if AI developers internalize their risks, they will have an
incentive to reduce them, and therefore impose less risk onto
society.</p>
<h3 id="automation">Automation</h3>
<p>In the near future, AIs might create jobs by making the economy more
productive. Upon the advent of Human-Level AI (HLAI), however, the need
for human labor would disappear <span class="citation"
data-cites="brynjolfsson2022turing"></span>. Conventional policies to
address job losses from automation, like worker retraining programs,
would be meaningless in a world where there are no jobs to retrain for.
At that point, economic gains from automation would be likely to accrue
to a small handful of people or organizations that control AIs.<br />
Governments may seek to redistribute wealth from big tech companies to
the masses in order to address poverty and inequality. However, in an
autonomous economy, which operates without human labor, humans may
become enfeebled and dependent on AIs for all aspects of life. In this
section, we will consider impacts of automation before and after the
development of HLAI.</p>
<h3 id="benefits-of-automation">Benefits of automation</h3>
<p><strong>Automation has historically altered the workforce but led to
economic development.</strong> On the one hand, automation negatively
impacts employment and wages through the <em>displacement effect</em>,
as human labor is substituted with technology. On the other hand,
automation has a positive economic impact through the <em>productivity
effect</em>, as new tasks or industries require human labor <span
class="citation" data-cites="acemoglu2020robots"></span>. Historically,
the productivity effect has dominated and the general standard of living
has increased. Consider the Industrial Revolution: mechanized production
methods displaced artisanal craftspeople, but eventually led to new
types of employment opportunities in factories and industries that
hadn’t previously existed. Similarly, access to computers has automated
away many manual and clerical jobs like data entry and typing, but has
also spawned many more new professions. Technological shifts have
historically led to increases in employment opportunities and wages.</p>
<p><strong>In the short term, AI could boost employment.</strong> We may
soon see an explosion of job opportunities in AI research as well as
jobs that involve managing or checking the outputs of AI systems. Given
the likely penetration of AI into all sectors of the economy, this could
lead to a significant creation of new positions, not dissimilar to how
IT services are required across industries.<br />
Economic theory suggests that automation can improve economic efficiency
and promote economic growth, thus increasing total wealth (Section ). As
wealth increases, people may have more disposable income, potentially
spurring job growth in sectors like hospitality, recreation, and mental
health services. Automation can also lower the costs of goods and
services, increasing demand. For example, automation may expedite
tedious aspects of the legal profession, decreasing the cost of legal
services. Legal services might then become more affordable, increasing
demand and creating more work for legal professionals.<br />
Regardless of AI capabilities, there will likely continue to be some
professions where the presence of humans is valued. Although computers
are superior at playing chess than humans, chess remains a widely
popular game. Analytics have increased the competitiveness of chess and
other games and have also helped organizers identify ways to engage
audiences. However, for the vast majority of jobs, even jobs like those
in caretaking and education, the preference for humans may diminish as
AI substitutes become vastly cheaper and more efficient.</p>
<h3 id="costs-of-human-level-ai">Costs of Human-Level AI</h3>
<p><strong>Human-Level AI (HLAI) would cause mass unemployment.</strong>
Human-Level AI, by definition, is capable of doing every task that
humans can do, at least as cheaply. The implication of achieving HLAI is
that human labor would no longer be valuable or necessary.<br />
Historically, routine tasks have been the primary target for automation.
As AIs approach human-level intelligence, cognitive nonroutine jobs
<span class="citation" data-cites="dvorkin2017growing"></span>, which
require education and are typically high-paying, would also become
automated. Programmers, researchers, and artists are already augmenting
their productivity using large language models, which will likely
continue to become more capable. One way the future of work could play
out is that increasingly few high-skilled workers will excel at managing
or using AIs or will provide otherwise exceptionally unique skills,
while the vast majority of people become unemployable.<br />
Human innovation has historically created new jobs that would have
seemed inconceivable just decades earlier. However, HLAI would also
possess the ability to invent new economically valuable tasks, possibly
more quickly than humans can think and innovate. The idea that there are
always jobs for humans to do, while historically true, is not a law of
nature <span class="citation" data-cites="harari2016homo"></span>.</p>
<p><strong>An autonomous economy could operate without human
labor.</strong> The transition to a fully automated economy may not stem
from one defining moment, but from an accumulation of small economic
decisions. Imagine the following story: as AIs become increasingly
capable, humans delegate an increasing number of business tasks to them.
Initially, they handle emails and scheduling, but soon manage
organizational accounting and communications with clients and partners.
The AIs can be trained and deployed quickly and make fewer errors than
humans. Over time and due to competitive pressures, businesses feel the
need to automate more of their operations, and AIs begin handling
complex cognitive tasks like strategy, innovation, and leadership. Soon,
the economy is mostly made up of AIs communicating with other AIs,
operating at a pace and complexity that humans can’t keep up with,
further motivating the automation of all economic activity. Eventually,
the economy is able to operate without need for human contribution or
oversight. Humans on the other hand, have become utterly reliant on AIs
for basic needs, societal governance, and even social interactions.
Humans have little reason to learn, work, or aspire to create, and their
survival depends on the beneficence of AIs <span class="citation"
data-cites="hendrycks2023overview"></span>.</p>
<p><strong>Automation could lead to social decay.</strong> If the
majority of the population became unemployed, they would have lost their
sources of income, causing unprecedented levels of poverty and
inequality ( section). Automation may also reduce social wellbeing
through the loss of meaning or enfeeblement: the loss of abilities and
values that make human life worthwhile. Overcoming challenges through
effort is an essential way that humans find purpose and meaning, and
this often comes through work or learning. Absent the need to cultivate
skills for work, humans may shy away from challenges to pursue instant
gratification. Personal achievement and growth are deeper forms of
wellbeing than momentary pleasure. In an autonomous economy, areas of
life like personal expression, communication, and emotional resilience
may get delegated to the domain of AI assistants, and people may lose
the ability to, or never learn to, independently process their emotions
or connect with other humans.</p>
<h3 id="redistribution-and-social-spending">Redistribution and Social
Spending</h3>
<p>Governments may seek to redistribute this wealth to the masses,
addressing poverty by providing a guaranteed income to everyone. In an
ideal outcome, people feel liberated from the need to work and are free
to spend their time on what they value, their basic needs provided for
by the government. We will survey wealth redistribution policies, and
consider their impacts.</p>
<p><strong>Public ownership of AI organizations can distribute benefits
from AIs.</strong> Governments might seek to assume partial or full
ownership of AI companies, in order to control their operations safely
and guarantee equitable revenue distribution.<br />
Public utilities are often nationalized because they benefit from
economies of scale, where larger organizations are cheaper and more
efficient. For example, building power plants and transmission lines is
expensive, so governments are interested in maintaining one large,
well-regulated company. The French government owns the utility company
Électricité de France, whose nuclear plants power the majority of the
country’s electricity. AIs may be similar to public utilities if they
are ubiquitous through the economy, essential to everyday activity, and
require special safety considerations.<br />
A common argument against nationalization is that it destroys
competition. One potential concern about undermining competition is that
in the absence of competition, people and companies do not need to be
more productive than a competitor. This might not be an issue with
regards to AIs—they would be capable of maximum efficiency around the
clock.</p>
<p><strong>Taxation is the most straightforward redistribution
policy.</strong> Wealth redistribution or social spending is most often
funded by taxes. Progressive tax policies, adopted today by most but not
all nations, require that those who earn more money pay a greater
proportion of their earnings than those who earn less. Governments then
seek to redistribute wealth through a wide variety of programs, from
healthcare and education to direct checks to people. In light of massive
earnings by tech companies, economists and policymakers have already
proposed specialized taxes on robots, digital goods and services, and
AI. If AI enables big tech companies to make orders of magnitude more
money than any previous company, while much of the population is unable
to pay income tax, targeted taxes on AI may be necessary to maintain
government revenues.<br />
Seeking to encourage innovation, the US’s tax landscape currently favors
capital investment over labor investment. If a firm wants to hire a
worker, they have to pay payroll taxes and employees have to pay a
number of separate taxes. If a firm replaced their worker with an AI,
they would presently only pay corporate tax, which was incurred anyway
<span class="citation" data-cites="acemoglu2020does"></span>. As with
other redistributive policies surveyed in this chapter, there are
political barriers to high taxes, including the ability of companies to
lobby the government in favor of lower taxes, as well as long-standing
and contentious debates over the economic effects of taxation.</p>
<h3 id="distribution-of-risks">Distribution of Risks</h3>
<p>This textbook presents many risks from AI. It is important to discuss
how these risks are distributed, since impacts from these societal-scale
risks materialising would not fall only on those who develop them. Risks
might even fall disproportionately on those who play no part in the
development or control of AIs. We will first introduce the idea of moral
hazards and discuss moral hazards from AIs. Then we will consider ways
that risks can be reduced.</p>
<p><strong>Moral hazards occur when risks are externalized.</strong>
Moral hazards are situations where an entity is encouraged to take on
risks, knowing that any costs will be borne by another party. Insurance
policies are a classic example: people with damage insurance on their
phones might handle them less carefully, secure in the knowledge that
any repair costs will be absorbed by the insurance company, not
them.<br />
The bankruptcy system ensures that no matter how much a company damages
society, the biggest risk it faces is its own dissolution, provided it
violates no laws. Companies may rationally gamble to impose very large
risks on the rest of society, knowing that if those risks ever come back
to the company, the worst case is the company going under. The company
will never bear the full cost of damage caused to society due to its
risk taking. Sometimes, the government may step in even prior to
bankruptcy. For example, American big banks took on massive risks in the
lead up to the 2008 financial crisis, but many of them were considered
"too big to fail", leading to an expectation that the government would
bail them out in time of need <span class="citation"
data-cites="acharya2016end"></span>. These dynamics ultimately
contributed to the Great Recession.</p>
<p><strong>Developing advanced AIs is a moral hazard.</strong>
Throughout this textbook, we have outlined great risks from advanced
AIs. However, while the potential costs to society are immense, the
maximum financial downside to a tech company developing these AIs is
filing for bankruptcy.<br />
Consider the following, quite extreme, thought experiment. Suppose that
a company is on the cusp of inventing an AI system that would boost its
profits by a thousand-fold, making every employee a thousand times
richer. However, the company estimates that their invention comes with a
0.1% chance of human extinction. In the likely case, the average person
in the economy would see some benefits due to increased productivity in
the economy, and possibly from wealth redistribution. Still, most people
view this gamble as irrational, preferring not to risk extinction for
modest economic improvements. On the other hand, the company may see
this as a worthwhile gamble, as it would make each employee considerably
richer.</p>
<h3 id="governance-solutions-for-risk-management">Governance Solutions
for Risk Management</h3>
<p><strong>Risk internalization encourages safer behavior.</strong> In
the above examples of moral hazards, companies take risks that would
more greatly affect external parties than themselves. The converse of
this is risk internalization, where risks are primarily borne by the
party that takes them. Risk internalization compels the risk-taker to
exercise caution, knowing that they would directly suffer the
consequences of reckless behavior. If AI companies bear the risk of
their actions, they would be more incentivized to invest in safety
research, take measures to prevent malicious use, and be generally
disincentivized from creating potentially dangerous systems.</p>
<p><strong>Governance solutions for moral hazards.</strong> We will
survey four ideas for dealing with moral hazards: legal liability,
regulations, public ownership, and taxation. The first idea is to hold
companies legally liable for externalities that arise from their AIs. We
could try to ensure, for example, that companies are not able to avoid
responsibility for adverse outcomes by blaming users for misuse.
Companies would then feel compelled to build in and rigorously test
safety features, since they would suffer the consequences of failing to
do so.<br />
The second idea is to impose strict regulations. Although regulations do
not necessarily cause internalization of risks, they can help prevent
the materialization of risks by ensuring good practices. Regulations
have been the United States’s main policy tool in existing fields like
nuclear power and biological research, which entail societal-scale
risks.<br />
A third idea is to facilitate public ownership, which we previously
discussed. Public ownership means that the public receives both the
benefits and the costs (risks) of AIs, by definition eliminating moral
hazards. We previously outlined a thought experiment where a company
might feel compelled to take on an existential risk to humanity; the
public on the other hand is unlikely to gamble on its own
existence.<br />
A fourth idea, also discussed previously, is targeted taxation. Although
taxes do not directly force risk internalization, they can provide
government revenues that can be reserved for risk mitigation or disaster
relief efforts. For example, the Superfund is a US government program
that funds the cleanup of abandoned hazardous waste sites. It is funded
by excise taxes–—a special tax on some good, service, or activity–—on
chemicals. The excise tax ensures that the chemical manufacturing
industry pays for the handling of dangerous waste sites that it has
created. Special taxes on AIs could support government programs to
prevent risks or address disasters.</p>
<h3 id="conclusions-about-distribution">Conclusions About
Distribution</h3>
<p>In this section, we examined distribution concerns with AIs. First,
we considered that power among AIs could result in a singleton with
decisive and lasting power, a diverse ecosystem of AIs with varying
degrees of power, or something in between. Second, we examined varying
levels of access to AIs from open source to highly restricted private
models. Since AIs can be used in dangerous ways, traditional openness
norms in research must be reconsidered. Both centralization and
decentralization of AIs, in terms of power and access, carry different
risks.<br />
Third, we considered how costs and benefits of AIs might be distributed,
including societal-scale risks. In the short term, automation can lead
to economic growth, but the development of Human-Level AI would result
in the effective end of human labor. In the post-labor world, humans may
struggle for meaning and become dependent on AIs. Moral hazards present
other societal-scale risks. Governance can mitigate these risks by
holding companies accountable for the risks they take, redistributing
wealth from AIs, including the public in the ownership of AIs, or
imposing proactive regulations to prevent catastrophes.</p>
<h1 id="sec:compute-gov">Compute Governance</h1>
<p>A common shorthand for computational resources or computing power
used for AI is <em>compute</em>. In this section, we will discuss how
compute governance enables AI governance. First, we will examine how
since compute is indispensable for AI development, governing it would
help us govern AIs. Then, we will examine the key properties of compute
that make it governable—–physicality, excludability, and
quantifiability–—and consider why governing compute is more promising
than governing other factors used in AI production.<br />
</p>
<h2
id="compute-is-indispensable-for-ai-development-and-deployment">Compute
Is Indispensable for AI Development and Deployment</h2>
<p>Compute enables the development of more capable AIs. In addition,
compute is necessary to deploy AIs. If we restrict someone’s access to
compute, they cannot develop or deploy any AIs. As a result, we can use
compute to govern AIs, determining how and when they are deployed.</p>
<p><strong>Hardware is necessary for AI systems.</strong> Like the
uranium in a nuclear weapon, compute is fundamental to running AIs. In
its simplest form, we can think of compute as a select group of
high-performance chips like GPUs and TPUs that are designed to run
modern AIs. These are often the latest chips, tailor-made for AI tasks
and found in large data centers. As AI technology changes, so too will
the hardware, adapting to new tech developments, regulations, and the
evolving requirements of AI applications.</p>
<p><strong>The metric FLOP/s is common across forms of compute.</strong>
To measure compute, we often use the metric <em>FLOP/s</em>, which is
the number of floating-point operations (such as addition or
multiplication) a computer can do in a second. When we talk about
“increasing” compute, we’re referring to using more processors, using
better processors, or allowing these processors to run for extended
periods, effectively increasing the number of floating-point operations
done in total. An analogous escalation of this is improving a nuclear
arsenal by adding more weapons to it, developing more dangerous weapons
like H-bombs, or creating bigger bombs by using more uranium.</p>
<p><strong>More compute allows for the development of more AI
capabilities.</strong> Compute plays a pivotal role in the evolution of
AI capabilities. More compute means that AI systems can be built with
more parameters and effectively utilize larger datasets. In the chapter,
we looked at scaling laws, which show us that many AIs have their
performance increase reliably with an increase in model size and dataset
size. Richard Sutton’s “The Bitter Lesson” states that general methods
in AI that harness computation tend to be the most effective by a
significant margin <span class="citation"
data-cites="Sutton-bitter"></span>. Having more compute means training
AI systems that are more capable and advanced, which means that knowing
how much compute an AI uses lets us approximate its capabilities.<br />
Often, pushing the boundaries in AI development requires having vast
compute. AIs can require training on large supercomputers that cost
hundreds of millions or even billions of dollars. Moreover,
computational demands for these AI models are constantly intensifying,
with their compute requirements doubling roughly every six months. This
growth rate surpasses the 2.5-year doubling time we see for the
price-performance of AI chips <span class="citation"
data-cites="amodei2018ai"></span>. Given this trend, it’s likely that
future AI models will demand even greater investment in computational
resources and, in turn, possess greater capabilities.</p>
<p><strong>More compute enables better results.</strong> Compute is not
only essential in training AI-—it is also necessary to run powerful AI
models effectively. Just as we rely on our brains to think and make
decisions even after we’ve learned, AI models need compute to process
information and execute tasks even after training. If developers have
access to more compute, they can run bigger models. Since bigger models
usually yield better results, having more compute can enable better
results.</p>
<p><strong>Large-scale compute isn’t a strict requirement for all future
AI applications.</strong> AI efficiency research aims to reduce compute
requirements while preserving performance by improving other factors
like algorithmic efficiency. If algorithms become so refined that
high-capability systems can train on less powerful devices, compute’s
significance for governance might diminish. Some existing systems like
AI-powered drug discovery tools do not require much compute, but it has
been demonstrated that they be can repurposed to create chemical weapons
<span class="citation" data-cites="Urbina2022DualUO"></span>.<br />
Additionally, there’s continued interest towards creating efficient AI
models capable of running on everyday devices such as smartphones and
laptops. Though projections from current trends suggest it will be
decades before data center-bound systems like GPT-4 could train on a
basic GPU <span class="citation" data-cites="epoch2023mltrends"></span>,
the shift towards greater efficiency might speed up dramatically with
just a few breakthroughs. If AI models require less compute, especially
to the point that they become commonplace on consumer devices,
regulating AI systems based on compute access might not be the most
effective approach.</p>
<h2 id="compute-is-physical-excludable-and-quantifiable">Compute Is
Physical, Excludable, and Quantifiable</h2>
<p>To produce AI, developers need three primary factors: data,
algorithms, and compute. In this section, we will explore why governing
compute appears to be a more promising avenue than governing the other
factors. A resource is governable when the entity with legitimate claims
to control it—–such as a government–—has the ability to control and
direct it. Compute is governable because</p>
<ol>
<li><p>It can be determined who has access to compute and how they
utilize it.</p></li>
<li><p>It is possible to establish and enforce specific rules about
compute.</p></li>
</ol>
<p>These are true because compute is <em>physical</em>,
<em>excludable</em>, and <em>quantifiable</em>. These characteristics
allow us to govern compute, making it a potential point of control in
the broader domain of AI governance. We will now consider each of these
in turn.</p>
<h3 id="compute-is-physical">Compute Is Physical</h3>
<p>The first key characteristic that makes compute governable is its
physicality. Compute is physical, unlike datasets, which are virtual, or
algorithms, which are intangible ideas. This makes compute rivalrous and
enables tracking and monitoring, both of which are crucial to
governance.</p>
<p><strong>Since compute is physical, it is rivalrous.</strong> Compute
is rivalrous: it cannot be used by multiple entities simultaneously.
This is unlike other factors in AI production, such as algorithms which
can be used by anyone who knows them or data which can be acquired from
the same source or even stolen or copied and used simultaneously
(although this may be difficult due to information security and the size
of training datasets). Because compute cannot be simultaneously accessed
by multiple users or easily duplicated, regulators can be confident that
when it is being used by an approved entity, it is not also being used
by someone else. This makes it easier to regulate and control the use of
compute. GPUs can’t be downloaded but instead must be fabricated,
purchased, and shipped.</p>
<div class="wrapfigure">
<p><span>r</span><span>8.4cm</span> <img
src="images/governance/image1.png" style="width:8.4cm"
alt="image" /></p>
</div>
<p><strong>Since compute is physical, it is trackable.</strong> Compute
is trackable, from chip fabrication to its use in data centers. This is
because compute is tangible and often sizable: figure <a
href="#wrap-fig:governance" data-reference-type="ref"
data-reference="wrap-fig:governance">[wrap-fig:governance]</a> shows a
cutting-edge semiconductor tool used as compute that costs $200 million
and requires 40 freight containers, 20 trucks, and 3 cargo planes to
ship anywhere.<br />
Unlike uranium, which is difficult to procure but not impossible to
steal and transport small amounts of, acquiring large-scale compute
requires the investment of resources in a relatively complicated and
visible process. Stakeholders, whether semiconductor firms, regulatory
bodies, or other involved entities, can accurately evaluate and trace
the overall quantity of these assets. For instance, if we monitor the
sales and distribution of chips, we know who possesses which
computational capacities and their intended applications. The complete
supply chain, from the semiconductor origins to the extensive data
centers harboring vast AI computational power, can be monitored, which
means it can be governed. By contrast, data acquisition and algorithmic
improvements can be done discreetly: possession of these within a
computing infrastructure can be concealed more easily than the
possession of the infrastructure itself.</p>
<h3 id="compute-is-excludable">Compute Is Excludable</h3>
<p>The second key characteristic that makes compute governable is its
excludability. Something is excludable if it is feasible to stop others
from using it. Most privately produced goods like automobiles are
excludable whereas others, such as clean air or street lighting, are
difficult to exclude people from consuming even if a government or
company doesn’t want to let them use it. Compute is excludable because a
few entities, such as the US and the EU, can control crucial parts of
its supply chain. This means that these actors can monitor and prevent
others from using compute.</p>
<p><strong>The compute supply chain makes monitoring easier.</strong> In
2023, the vast majority of advanced AI chips globally are crafted by a
single firm, Taiwan Semiconductor Manufacturing Company (TSMC). These
chips are based on designs from a few major companies, such as Nvidia
and Google, and TSMC’s production processes rely on photolithography
machines from a similarly monopolistic industry led by ASML <span
class="citation" data-cites="arnold2022eto"></span>. Entities such as
the US and EU can, therefore, regulate these companies to control the
supply of compute—if the supply chain dynamics do not change
dramatically over time <span class="citation"
data-cites="khan2021semiconductor"></span>. This simplifies the tracking
of frontier AI chips and enforcing of regulatory guidelines; it’s what
made the US export ban of cutting-edge AI chips to China in 2022
feasible. This example illustrates that these chips can be governed. By
contrast, data can be purchased from anywhere or found online, and
algorithmic advances are not excludable either, especially given the
open science and collaborative norms in the AI community.</p>
<p><strong>Frequent chip replacements means governance is effective
quickly.</strong> The price performance of AI chips is increasing
exponentially. With new chips frequently making recent products
obsolete, compute becomes more excludable. Historical trends show that
GPUs double their price performance approximately every 2.5 years <span
class="citation" data-cites="epoch2023mltrends"></span>. In conjunction
with the rapidly increasing demand for more compute, data centers
frequently refresh their chips and purchase vast quantities of new
compute regularly to retain competitiveness. This frequent chip turnover
offers a significant window for governance since regulations on new
chips will be relevant quickly.</p>
<h3 id="compute-is-quantifiable">Compute Is Quantifiable</h3>
<p>The third key characteristic that makes compute governable is its
quantifiability. Quantifiability refers to the ability to measure and
compare both the quantity and quality of resources. Metrics such as
FLOP/s serve as a yardstick for comparing computational capabilities
across different entities. If a developer has more chips of the same
type, we can accurately deduce that they have access to more compute,
which means we can use compute to set thresholds and monitor
compliance.</p>
<p><strong>Quantifiability facilitates clear threshold setting.</strong>
While chips and other forms of compute differ in many ways, they can all
be quantified in FLOP/s. This allows regulators to determine how
important it is to regulate a model that is being developed: models that
use large amounts of compute are likely more important to regulate.
Suppose a regulator aims to regulate new models that are large and
highly capable. A simple way to do this is to set a FLOP/s threshold,
above which more regulations, permissions, and scrutiny take effect. By
contrast, setting a threshold on dataset size is less meaningful:
quality of data varies enough that 25 GB of data could contain all the
text in Wikipedia or one high-definition photo album. Even worse,
algorithms are difficult to quantify at all.</p>
<p><strong>Quantifiability is key to monitoring compliance.</strong>
Beyond the creation of thresholds, quantifiability also helps us monitor
compliance. Given the physical nature and finite capacity of compute, we
can tell whether an actor has sufficient computational power from the
type and quantity of chips they possess. A regulator might require
organizations with at least 1000 chips at least as good as A100s to
submit themselves for additional auditing processes. A higher number of
chips directly correlates to more substantial computational
capabilities, unlike with algorithms where there is no comparable metric
and data for which metrics are much less precise. In addition to compute
being physical and so traceable, this enables the enforcement of rules
and thresholds.</p>
<h3 id="conclusions-about-compute-governance">Conclusions About Compute
Governance</h3>
<p><strong>Compute governance is a promising route to AI
governance.</strong> Compute is necessary for the development and
deployment of AIs, as well as being well-suited to governance,
especially relative to the other factors used in AI production. In
general, we see that compute governance is a promising lever for
ensuring the development of safe and beneficial AIs, allowing
governments to control distribution. Relative to governing algorithms
and datasets, the other factors used to produce AIs, governing compute
is promising because it is physical, excludable, and quantifiable.<br />
During this discussion, we have explored a wide variety of ways that
governments can set, monitor, and enforce standards and regulations.
Additionally, we have seen that international cooperation on compute
governance can help use its excludability to govern it well, assuming
entities like the US and EU can cooperate on governance. If factors such
as the controllability of the supply chain or requirement of large-scale
compute for highly capable models remain the same, then we might be
optimistic about using compute to govern AIs.<br />
While talking about compute governance, we have touched upon various
ideas about how to use compute to ensure companies produce safe AIs,
governments create and enforce regulations, and international
cooperation can control the compute supply chain. In the rest of this
chapter, we are going to further discuss governance tools aimed at
corporate, national, and international governance.</p>
<h1 id="sec:corp-gov">Corporate Governance</h1>
<p><strong>Overview.</strong> AI companies need sound corporate
governance: it is essential that these firms are guided in directions
that enable the creation of safe and beneficial AIs. In this section, we
explain what corporate governance is, differentiating between
shareholder and stakeholder theory. Then, we give an overview of
different legal structures, ownership structures, organizational
structures, as well as internal and external assurance mechanisms.</p>
<h2 id="what-is-corporate-governance">What Is Corporate Governance?</h2>
<p>There are different views on what the purpose of corporate governance
is. These differences are related to different theories about
capitalism.</p>
<p><strong>Shareholder theory.</strong> According to one view, corporate
governance is about the relationship between a company and its
<em>shareholders</em>. This view is based on <em>shareholder
theory</em>, according to which companies have an obligation to maximize
returns for their shareholders. Using this theory, the purpose of
corporate governance is to ensure that actors within a company act in
the best interest of the company’s shareholders. The relationship
between shareholders and actors within a company can be conceptualized
as the following problem: shareholders delegate responsibilities to
managers and workers, but managers and workers might not act in the
shareholders’ interests <span class="citation"
data-cites="jensen2019theory"></span>. On this view, corporate
governance is ultimately a tool for maximizing shareholder value.</p>
<p><strong>Stakeholder theory.</strong> According to another view,
corporate governance is about the relationship between a company and its
<em>stakeholders</em>. The idea is that companies are responsible not
only to their shareholders but to many other stakeholders like
employees, business partners, and civil society <span class="citation"
data-cites="friedman2007social"></span>. Following this theory, the
purpose of corporate governance is to balance the interests of
shareholders with the interests of other stakeholders. It refers to all
the rules, practices, and processes by which a company is directed and
controlled.<br />
For the purposes of this book, we consider corporate governance that
aims to achieve stakeholder representation, not shareholder
representation. We are mainly interested in how AI companies are or
should be governed to best advance the public interest <span
class="citation" data-cites="cihon2021corporate"></span>. Next, we will
consider how a firm’s legal structure can help permit this.</p>
<h2 id="legal-structure">Legal Structure</h2>
<p>A company’s legal structure refers to its legal form and place of
incorporation, its statutes and bylaws.</p>
<p><strong>Legal form.</strong> AI companies can have different legal
forms. In the US, the most common form is a <em>C corporation</em> or
“C-corp” for short. Other forms include <em>public benefit corporations
(PBCs)</em>, <em>limited partnerships (LPs)</em>, and <em>limited
liability companies (LLCs)</em>. The choice of legal form has
significant influence on what a company is able and required to do. For
example, while C-corps must maximize shareholder value, PBCs can also
pursue public benefits. This can be important in situations where AI
companies may want to sacrifice profits in order to promote safety.
Google, Microsoft, and Meta are all C-corps, Anthropic is a PBC, and
OpenAI is an LP (which is partly owned by a nonprofit).</p>
<p><strong>Place of incorporation.</strong> Since there are significant
differences between jurisdictions, it matters where a company is
incorporated. An important distinction can be drawn between common law
countries like the US and UK, wherein judicial precedent is important,
and civil law countries like Germany and France, where recorded laws are
more comprehensive and less open to interpretation. But there are also
important differences within a given country. For example, in the US,
many AI companies such as OpenAI and Anthropic are incorporated in the
state of Delaware due to its relatively business-friendly regulations,
but they often have branches in other states like California.</p>
<p><strong>Statutes and bylaws.</strong> Although many governance
decisions are determined by the legal form and the place of
incorporation, companies have some room for customization. They can
customize their legal structure in their statutes and bylaws. Companies
are typically required to specify their mission statement in their
statutes. The mission of Google DeepMind is “to solve intelligence to
advance science and benefit humanity” and OpenAI’s mission is “to ensure
that AGI benefits all of humanity.” The statutes of PBCs also need to
contain a specific public benefits statement. But the statutes and
bylaws can also contain more specific rules; for example, an AI company
could give its board of directors specific powers, such as to veto the
deployment of a new model.<br />
Ensuring that a firm’s legal structures enable its employees to act in
the best interests of the firm’s stakeholders is important, and firms
have many ways to do this. Next, we will consider how ownership can help
solidify a firm’s safety-conscious goals.</p>
<h2 id="ownership-structure">Ownership Structure</h2>
<p>Companies are owned by shareholders, who elect the board of
directors, which appoints the senior executives who actually run the
company.</p>
<p><strong>Types of shareholders.</strong> AI companies can have
different types of shareholders. In their early stages, AI companies
typically get investments from wealthy individuals—so-called “angel
investors.” For example, Elon Musk was among the first investors in
OpenAI, while Dustin Moskovitz was part of the initial funding round of
Anthropic. As AI companies mature, professional investors like venture
capital (VC) firms and big tech companies start to invest—DeepMind was
acquired by Google in 2016, and Microsoft invested heavily in OpenAI. At
some point, many companies decide to “go public”, which means that they
are turned into publicly traded companies. At that point, institutional
investors like pension funds enter the stage. For example, Vanguard and
BlackRock are among the largest shareholders of Microsoft, Google, and
Meta, though the founders often retain a significant amount of shares.
It is also not uncommon to give early employees stock options, which
allow them to purchase stock at fixed prices.</p>
<p><strong>Powers of shareholders.</strong> Shareholders can influence
the company in several ways. They can voice concerns in the annual
general meeting and vote in shareholder resolutions. In the 2019 annual
general meeting of Alphabet, one shareholder called for better board
oversight of AI and suggested creating a Societal Risk Oversight
Committee. If the board of directors of a C-Corp does not act in the
shareholders’ interest, shareholders can theoretically replace them.
However, in practice this is very rare. A more common way to put
pressure on board members is to file lawsuits against them if they fail
to meet certain obligations. Such lawsuits are often settled in ways
that improve corporate governance. The attempt of shareholders to steer
the company in a certain direction is called <em>shareholder
activism</em>.</p>
<p><strong>Customized ownership structures.</strong> Depending on the
company’s legal form, it may be possible to customize the ownership
structure. A common way to do that is to issue two different classes of
shares: both classes give their holders ownership, but only one class
grants voting rights. This allows structures where the founders remain
in control of the company while other shareholders contribute capital
and receive returns on their investment. Another way to achieve certain
goals is to combine different legal entities. For example, the returns
of investors in the OpenAI LP are capped. Above a certain threshold,
returns are owned by the OpenAI nonprofit. They call this a
<em>capped-profit structure</em>. A related idea is to adopt a
governance structure that, under certain conditions, transfers control
to a committee representing society’s interests rather than the
shareholders’ interests.<br />
It is important who owns AI companies because these owners have strong
ways to influence operations. Next, we will put aside questions of
ownership and examine how these organizations are structured and
run.</p>
<h2 id="organizational-structure">Organizational Structure</h2>
<p>While shareholders own the company, it is governed by the board of
directors and managed by the chief executive officer (CEO) and other
senior executives.</p>
<p><strong>Board of directors.</strong> The board of directors is the
main governing body of the company. Board members have a legal
obligation to act in the best interests of the company, so-called
<em>fiduciary duties</em>. These duties can vary: board members of
Alphabet have the typical fiduciary duties of a C-Corp, while board
members of OpenAI’s nonprofit’s have a duty to “to ensure that AGI
benefits all of humanity,” not to maximize shareholder value. The board
has a number of powers they can use to steer the company in a more
prosocial direction. It sets the strategic priorities, is responsible
for risk oversight, and has significant influence over senior
management; for instance, it can replace the chief executive officer.
However, the board’s influence is often indirect. Many boards have
committees, some of which might be important from a safety perspective
such as a risk committee or audit committee.</p>
<p><strong>Senior executives.</strong> The company is managed by the
CEO, who appoints other senior executives such as a chief technology
officer (CTO), chief scientist, chief risk officer (CRO), and so on.
Since the behavior of executives is at least in part driven by financial
incentives, remuneration is often a valuable tool to ensure that they
act in the corporation’s interest. Before appointing new executives, the
board might also want to conduct background checks; in the case of AI
companies, a board might want to consider candidates’ views on risks
from AIs.</p>
<p><strong>Organizational units.</strong> Below the executive level, AI
companies have a number of teams, often in a hierarchical structure.
This will typically involve research and product development teams, but
also legal, risk management, finance, and many others. AI companies can
also have safety and governance teams. These teams should be well
resourced and have buy-in from senior management.<br />
In addition, some AI companies have other organizational structures,
such as an ethics board that advises the board of directors and senior
management, or an institutional review board (IRB) that checks if
publishing certain types of research might be harmful <span
class="citation" data-cites="cihon2021corporate"></span>. Google
DeepMind has a IRB-like review committee that played a key role in the
release of AlphaFold.<br />
Various aspects of the legal, ownership, and organizational structure of
AI companies can influence to what extent their outputs focus on
employees’ and managers’ best interests, creating shareholder value, or
achieving their mission and ensuring societal wellbeing. In the last
subsection, we will explore how we can ensure that these structures are
well chosen and smoothly functioning.</p>
<h2 id="assurance">Assurance</h2>
<p>Different stakeholders within and outside AI companies need to know
whether existing governance structures are adequate. AI companies
therefore take a number of measures to evaluate and communicate the
adequacy of their governance structures. These measures are typically
referred to as <em>assurance</em>. We can distinguish between internal
and external assurance measures.</p>
<p><strong>Internal assurance.</strong> AI companies need to ensure that
senior executives and board members get the information they need to
make good decisions. It is, therefore, essential to define clear
reporting lines. To ensure that key decision-makers get objective
information, AI companies may set up an internal audit team that is
organizationally independent from senior management <span
class="citation" data-cites="schuett2023agi"></span>. This team would
assess the adequacy and effectiveness of the company’s risk management
practices, controls, and governance processes, and report directly to
the board of directors.</p>
<p><strong>External assurance.</strong> Many companies are legally
required to publicly report certain aspects of their governance
structures, such as whether the board of directors has an audit
committee. Often, AI companies also publish information about their
released models and often in the form of model or system cards. Some
organizations disclose parts of their safety strategy and their
governance practices as well; for instance, how they plan to ensure
their powerful AI systems are safe, whether they have an ethics board,
or how they conduct pre-deployment risk assessments. These publications
allow external actors like researchers and civil society organizations
to evaluate and scrutinize the company’s practices. In addition, many AI
companies commission independent experts to scrutinize their models,
typically in the form of third-party audits, red teaming exercises to
adversarially test systems, or bug bounty programs that reward finding
errors and vulnerabilities.</p>
<h3 id="conclusions-about-corporate-governance">Conclusions About
Corporate Governance</h3>
<p>Corporate governance refers to all the rules, practices, and
processes by which a company is directed and controlled—–ranging from
its legal form and place of incorporation to its board committees and
remuneration policy. The purpose of corporate governance is to balance
the interests of a company’s shareholders with the interests of other
stakeholders, including society at large. To this end, AI companies are
advised to follow existing best practices in corporate governance.
However, in light of increasing societal risks from AI, they also need
to consider more innovative governance solutions, such as a
capped-profit structure or a long-term benefit committee.</p>
<h1 id="sec:nat-gov">National Governance</h1>
<p><strong>Overview.</strong> Government action may be crucial for AI
safety. Governments have the authority to enforce AI regulations, to
direct their own AI activities, and to influence other governments
through measures such as export regulations and security agreements.
Additionally, major governments can leverage their massive budgets,
diplomats, intelligence agencies, and leaders selected to serve the
public interest. More abstractly, as we saw in the chapter, institutions
can help agents avoid harmful coordination failures. For example,
penalties for unsafe AI development can counter incentives to cut
corners on safety.<br />
This section provides an overview of four ways governments may be able
to advance AI safety: safety standards and regulations, liability for AI
harms, improving societal resilience, and not falling behind while
focusing on developing safe AI.</p>
<h2 id="standards-and-regulations">Standards and Regulations</h2>
<p>To ensure that frontier AI development and deployment is safe, two
complementary approaches are formally establishing strong safety
measures as best practices for AI labs, and requiring the implementation
of strong safety measures. This can be done using standards and
regulations, respectively.</p>
<p><strong>Standards are specified best practices.</strong> Standards
are written specifications of the best practices for carrying out
certain activities. There are standards in many areas, from
telecommunications hardware to country codes to food safety. Standards
often aim to ensure quality, safety, and interoperability; for instance,
the International Food Standard requires the traceability of products,
raw materials and packaging materials.</p>
<p><strong>The substance of AI safety standards.</strong> In the context
of frontier AI, technical standards for AI safety could guide various
aspects of the AI model life cycle. Before training begins, training
plans could be assessed to determine if training is safe, based on
evidence about similar training runs and the proposed safety methods.
After training begins, models could be evaluated to determine if further
training or deployment is safe, based on whether models show dangerous
capabilities or propensities. During deployment, particularly powerful
models could be released through a monitored API. Standards could guide
all aspects of this process.</p>
<p><strong>Standards are developed in dedicated standard-setting
organizations.</strong> Many types of organizations, from government
agencies to industry groups to other nonprofits, develop standards. Two
examples of standard-setting organizations are the National Institute of
Standards and Technology (NIST) and the International Organization for
Standardization (ISO). In the US, standard setting is often a
consensus-based activity in which there is substantial deference to
industry expertise. However, this increases the risk that standards
over-represent industry interests.</p>
<p><strong>The impact of standards.</strong> Standards are not
automatically legally binding. Despite that, standards can advance
safety in various ways. First, standards can shape norms, because they
are descriptions of best practices, often published by authoritative
organizations. Second, governments can mandate compliance with certain
standards. Such “incorporation by reference” of an existing standard may
bind both the private sector and government agencies. Third, governments
can incentivize compliance with standards through non-regulatory means.
For example, government agencies can make compliance required for
government contracts and grants, and standards compliance can be a legal
defense against lawsuits.</p>
<p><strong>Regulations are legally binding.</strong> Regulations are
legal requirements established by governments. Some examples of
regulations are requirements for new foods and drugs to receive an
agency’s approval before being sold, restrictions on the pollution
emitted by cars, requirements for aircraft and pilots to have licenses,
and constraints on how companies may handle personal data.</p>
<p><strong>Regulations are often shaped by both legislatures and
agencies.</strong> In some governments, such as the US and UK,
regulations are often formed through the following process. First, the
legislature passes a law. This law creates high-level mandates, and it
gives a government agency the authority to decide the details of these
rules and enforce compliance. By delegating rulemaking authority,
legislatures let regulations be developed with the greater speed, focus,
and expertise of a specialized agency. As we discussed, agencies often
incorporate standards into regulations. Legislatures also often
influence regulation through their control over regulatory agencies’
existence, structure, mandates, and budgets.<br />
Regulatory agencies do not always regulate adequately. Regulatory
agencies can face steep challenges. They can be under-resourced: lacking
the budgets, staff, or authorities they need to do well at designing and
enforcing regulations. Regulators can also suffer from regulatory
capture—being influenced into prioritizing a small interest group
(especially the one they are supposed to be regulating) over the broader
public interest. Industries can capture regulators by politically
supporting sympathetic lawmakers, providing biased expert advice and
information, building personal relationships with regulators, offering
lucrative post-government jobs to lenient regulatory staff, and
influencing who is appointed to lead regulatory agencies.<br />
Standards and regulations give governments some ways to shape the
behavior of AI developers. Next, we will consider legal means to ensure
that the developers of AI have incentives in line with the rest of
society.</p>
<h2 id="liability-for-ai-harms">Liability for AI Harms</h2>
<p>In addition to standards and regulations, legal liability could
advance AI safety. When AI accidents or misuse cause harm, liability
rules determine who (if anyone) has to pay compensation. For example, an
AI company might be required to pay for damages if it leaks a dangerous
AI, or if its AI provides a user step-by-step instructions for building
or acquiring illegal weapons.</p>
<p><strong>Non-AI-specific liability.</strong> Legal systems including
those of the US and UK have forms of legal liability that would
plausibly apply to AI harms even in the absence of AI-specific
legislation. For example, in US and UK law, negligence is grounds for
liability. Additionally, in some circumstances, such as when damages
result from a company’s defective product, companies are subject to
strict liability. That means companies are liable even if they acted
without negligence or bad intentions. These broad conditions for
liability could apply to AI, but there are many ways judges might
interpret concepts like negligence and defective products in the case of
AI. Instead of leaving it to a judge’s interpretation, legislators can
specify liability rules for AI.</p>
<p><strong>There are advantages to holding AI developers liable for
damages.</strong> Legal liability helps AI developers internalize the
effects of their products on the rest of society by ensuring that they
pay when their products harm others. This improves developers’
incentives. Additionally, legal liability helps provide accountability
without relying on regulatory agencies. This avoids the problem that
government agencies may be too under-resourced or captured by industry
to mandate and enforce adequate safety measures.</p>
<p><strong>Legal liability is a limited tool.</strong> There are
practical limits to what AI companies can actually be held liable for.
For example, if an AI were used to create a pandemic that killed 1 in
100 people, the AI developer would likely be unable to pay beyond a
small portion of the damages owed (as these could easily be in the tens
of trillions). If the amount of harm that can be caused by AI companies
exceeds what they can pay, AI developers cannot fully internalize the
costs they impose on society. This problem can be eased by requiring
liability insurance (a common requirement in the context of driving
cars), but there are amounts of compensation that even insurers could
not afford. Moreover, sufficiently severe AI catastrophes may disrupt
the legal system itself. Separately, liability does little to deter AI
developers who do not expect their AI development to result in large
harms—even if their AI development really proves catastrophically
dangerous.<br />
Ensuring legal liability for harms that result from deployed AIs helps
align the interests of AI developers with broader social interests.
Next, we will consider how governments can mitigate harms when they do
occur.</p>
<h2 id="improving-resilience">Improving Resilience</h2>
<p>The government actions already discussed focus on preventing unsafe
AI development and deployment, but another useful intervention point may
be mitigating damages during deployment.</p>
<p><strong>Resilience may protect against extreme risks.</strong>
Governments may be able to improve societal resilience to AI accidents
or misuse through promoting cybersecurity, biosecurity, and AI
watchdogs. Measures for increasing resilience may raise the level of AI
capabilities needed to cause catastrophe. That would buy valuable time
to develop safety methods and further defensive measures—–ideally enough
time for safety and defense to always keep pace with offensive
capabilities. Sufficient resilience could lastingly reduce risk.</p>
<p><strong>Policy tools for resilience.</strong> To build resilience,
governments could use a variety of policy tools. For example, they could
provide R&amp;D funding to develop defensive technologies. Additionally,
they could initiate voluntary collaborations with the private sector to
assist with implementation. Governments could also use regulations to
require owners of relevant infrastructure and platforms to implement
practices that improve resilience.</p>
<p><strong>Tractability of resilience.</strong> If governments defend
narrowly against some attacks, rogue AIs or malicious users might just
find other ways to cause harm. Increasingly advanced AIs could pose
novel threats in many domains, so it may be hard to identify or
implement targeted defensive measures that make a real difference.
However, perhaps there are a few domains where societal vulnerabilities
are especially dire and tractable to improve (cybersecurity or
biosecurity, for example), while some defensive measures could provide
broader defenses (such as AI watchdogs).</p>
<p><em>Cybersecurity</em>. AIs could strengthen cybersecurity. AIs could
identify and patch code vulnerabilities (that is, they could fix faulty
programming that would let attackers get unauthorized access to a
computer). AIs could also help detect phishing attacks, malware and
other attempts to attack a computer network, enabling responses such as
blocking or quarantining malicious programs. These efforts could be
targeted to defend widely used software and critical infrastructure.
However, AIs that identify code vulnerabilities are dual-use; they can
be used to either fix or exploit vulnerabilities.<br />
<em>Biosecurity</em>. Dangerous pathogens can be detected or countered
through measures such as wastewater monitoring (which might be enhanced
by anomaly detection), far-range UV technology, improved personal
protective equipment, and DNA synthesis screening that is secure and
universal.<br />
<em>AI watchdogs</em>. AIs could monitor the activity of other AIs and
flag dangerous behavior. For example, AI companies can analyze the
outputs of their own chatbots and identify harmful outputs.
Additionally, AIs could identify patterns of dangerous activities in
digital or economic data. However, some implementations of this could
harm individual privacy.<br />
Defensive measures including cybersecurity, biosecurity, and AI
watchdogs may mitigate harms from the deployment of unsafe AI systems.
However, defensive measures, regulation, and liability may all be
insufficient for safety if the countries that implement them all fall
behind the frontier of AI development. Next, we will consider how
countries can remain competitive while ensuring safety in domestic AI
production.</p>
<h2 id="not-falling-behind">Not Falling Behind</h2>
<p>If some countries take a relatively slow and careful approach to AI
development, they may risk falling behind other countries that take less
cautious approaches. It would be risky for the global leaders in AI
development to be within countries that lack adequate guardrails on AI.
Various policy tools could allow states to avoid falling behind in AI
while they act to keep their own companies’ AIs safe.</p>
<p><strong>Risks of adversarial approaches.</strong> Adversarial
approaches to AI policy–—that is, policies focused on advancing one
country’s AI leadership at the expense of another’s—–have risks.
Adversarial policies could rely on wrong assumptions about which states
will adequately guardrail AI, and they could also motivate
counter-actions and increase international tensions (making cooperation
harder). Competitive mindsets can also encourage de-prioritizing safety
in the name of competing–—in the chapter, we consider this problem in
greater depth using formal models. Additionally, AI technologies might
proliferate quickly even with strong efforts to build national leads in
AI.<br />
International cooperation, as explored in the <a href="#sec:int-gov"
data-reference-type="ref" data-reference="sec:int-gov">7</a> section,
may enable states to keep their AIs safe, preserve national
competitiveness, and avoid the pitfalls of adversarial AI policy. Still,
as options for cases where cooperation fails, here we consider several
policy tools for preserving national competitiveness in AI.</p>
<p><strong>Export controls.</strong> Restrictions on the export of
AI-specialized hardware can limit states’ access to a key input into AI.
Due to the extreme complexity of advanced semiconductor manufacturing,
it is very difficult for states subject to these export controls to
manufacture the most advanced hardware on their own. Additionally, the
AI hardware supply chain is extremely concentrated, perhaps making
effective export controls possible without global agreement. We explore
this further in the section.</p>
<p><strong>Immigration policy.</strong> Immigration policy affects the
flow of another important input into AI development: talented AI
researchers and engineers. Immigration could be an asymmetric advantage
of certain countries; surveys suggest that the international AI
workforce tends to have much more interest in moving to the US than
China <span class="citation" data-cites="zwetsloot2021winning"></span>.
Immigrants may be more likely to spread AI capabilities internationally,
through international contacts or by returning to their native
countries, but many immigrants who are provided with the chance choose
to stay in the US.</p>
<p><strong>Information security.</strong> Information security measures
could slow the diffusion of AI insights and technologies to countries or
groups that lack adequate guardrails. For example, governments could
provide information security assistance to AI developers, and they could
incentivize or require developers’ compliance with information security
standards.</p>
<p><strong>Intelligence collection.</strong> Collecting and analyzing
intelligence on the state of AI development in other countries would
help governments avoid both unwarranted complacency and unwarranted
insecurity about their own AI industries.<br />
Governments can use a range of measures to remain internationally
competitive while maintaining the safety of domestic AI development.</p>
<h3 id="conclusions-about-national-governance">Conclusions About
National Governance</h3>
<p>National governments have many tools available for advancing AI
safety. Standards, regulations, and liability could stop dangerous AIs
from being deployed, while encouraging the development of safe AIs.
Improved resilience could mitigate the damage of dangerous deployments
when they do occur, giving us more time to create safe AIs and
mitigating some risk from dangerous ones. Measures such as strong
information security could allow governments to ensure domestic AI
production is both safe and competitive. Each of these approaches has
largely distinct limitations—for example, regulations may be held back
by regulatory capture, while liability might impose too few penalties
too late—so effective governance may require combining many of the
government actions discussed in this section.<br />
With robust AI safety standards and regulations, a well-functioning
legal framework for ensuring liability, strong resilience against
societal-scale risks from AIs, and measures for not being outpaced
internationally by unconstrained AI developers, there would be multiple
layers of defense to protect society from reckless or malicious AI
development.</p>
<h1 id="sec:int-gov">International Governance</h1>
<p>In this section, we will discuss the international governance of AI
systems. First, we will consider the problem of the international
governance of AI. Then, we will discuss the basics of international
governance, such as its stages and techniques. To determine what sort of
international cooperation is possible and necessary, we will discuss
four key questions that are important for understanding features of the
emerging technologies we wish to regulate. Lastly, we will discuss
possibilities for the international governance of AIs, separately
considering AIs made by civilians and AIs made by militaries.</p>
<p><strong>International regulation can promote global benefits and
manage shared risks.</strong> It is important to actively regulate AI
internationally. Firstly, it allows for the distribution of global
benefits that advanced AIs can provide, such as improved healthcare,
increased efficiency in transportation, and enhanced communication.
Countries can work together to ensure that AI technologies are developed
and deployed in a way that benefits humanity as a whole.<br />
Secondly, international governance of AI is necessary to manage
effectively the risks associated with its development. The risks from AI
systems are not confined to the country in which they are developed; for
instance, even if an AI system is developed in the US, it is likely to
be deployed around the world, and so its impacts will be felt worldwide.
Risks, such as the danger of progressively weaker national regulations
in the absence of international standards and the potential for arms
races in which actors cut corners on safety in order to compete, require
international cooperation to avoid <span class="citation"
data-cites="armstrong2016racing"></span>. From the point of view of
public safety, the risks of negative effects of systems developed across
borders is a simple and powerful argument for international
governance.</p>
<p><strong>International governance of powerful technologies is
challenging.</strong> In general, international cooperation takes place
through bilateral or multilateral negotiations between relevant
countries, or through international organizations like the UN and its
agencies. Even at the best of times and with the least contentious of
issues, international cooperation is slow, difficult, and often
ineffectual. Strained international relationships, such as frequent
tensions between the US and China, make successful international
standards even less likely.<br />
For the regulation of AI, we can draw analogies to the regulation of
other dangerous technologies such as nuclear, biological, and chemical
weapons. While these analogies are imperfect, they give us reasons to be
concerned about the prospects of international cooperation for AI. There
are few convincing examples of agreements between major powers to limit
the development of technologies for which there were no military
substitutes. However, we can learn from both the failures and successes
of past regulation: by asking questions about how AI is similar to past
technologies, we can understand what form successful and unsuccessful
governance might take.</p>
<h2 id="forms-of-international-governance">Forms of International
Governance</h2>
<p>Before we can consider how to govern AIs internationally, we must
understand the nature of international governance. We will first
consider the different stages of international governance, from
recognizing a problem exists to ensuring that its governance is
effective. Then, we will consider a range of techniques used by
international actors to ensure global governance.</p>
<h3 id="stages">Stages</h3>
<p>We can break international governance into four stages <span
class="citation" data-cites="avant2010governs"></span>. First, issues
must be recognized as requiring governance. Then, countries must come
together to agree how to govern the issue. After that, they must
actually do what was agreed. Lastly, countries’ actions must be
monitored for compliance to ensure that governance is effective into the
future.</p>
<p><strong>Setting agendas.</strong> The first stage of governance is
agenda-setting. This involves getting an issue recognized as a problem
needing collective action. Actors have to convince others that an issue
exists and matters. Powerful entities often want to maintain their power
in the status quo, and thus deny problems or ignore their
responsibilities for dealing with them. Global governance over an issue
can’t start until it gets placed on the international agenda. Non-state
actors like scientists and advocacy groups play a key role in
agenda-setting by drawing attention to issues neglected by states; for
example, scientists highlighted the emerging threat of ozone depletion,
building pressure that led to the Montreal Protocol.<br />
Agenda-setting makes an issue a priority for collective governance.
Without it, critical problems can be overlooked due to political
interests or inertia.</p>
<p><strong>Policymaking.</strong> Once an issue makes the global agenda,
collections of negotiating countries or international organizations
often take the lead in policymaking. Organizations like the UN
facilitate formal negotiations between states to develop policies, as
seen at major summits on climate change. In other cases, organizations’
own procedures shape policies over time; for instance, export control
lists in the US like the International Traffic in Arms Regulations are
expanded by regulators without Congressional action. Organizations
manage competing country interests to build consensus on vague or
detailed policies. For example, the International Civil Aviation
Organization brought countries together to agree on aircraft safety
standards. Effective policymaking by organizations converts identified
issues into guidelines for collective action. Without actors taking
responsibility for driving the policy process, implementation lacks
direction.</p>
<p><strong>Implementation and enforcement.</strong> After policies are
made, the next stage is implementing them. High-level policies are
sometimes vague, allowing flexibility to apply them; for example, the
Chemical Weapons Convention relies on countries to enforce bans on
chemical weapons domestically in the ways they find most effective.
Governance controls how these policies are enforced; for instance, the
International Atomic Energy Agency (IAEA) conducts inspections to verify
compliance with the Treaty on the Non-Proliferation of Nuclear Weapons
(NPT). Even if actors agree on policies, acting on them takes resources,
information, and coordination. Effective implementation and enforcement
through governance converts abstract rules into concrete actions.</p>
<p><strong>Evaluation, monitoring, and adjudication.</strong> The final
stage of governance is evaluating outcomes and monitoring compliance.
The organization implementing policies may perform these oversight tasks
itself. But often other actors play watchdog roles as third-party
evaluators. It may be formally established who assesses compliance, like
the Organization for the Prohibition of Chemical Weapons (OPCW). In
other cases, evaluation is informal, with self-appointed civil society
monitors. Both insiders and outsiders frequently evaluate progress
jointly. For example, the UN, OPCW, and NGOs all track progress on
chemical weapons disarmament. Clarifying who monitors and evaluates
policies is important to ensure accountability and transparency. Without
effective evaluation, it is difficult to learn from and improve on
governance efforts over time.</p>
<h3 id="techniques">Techniques</h3>
<p>There are many ways in which countries and other international actors
govern issues of global importance. Here, we consider six ways that past
international governance of potentially dangerous technologies has taken
place, ranging from individual actors making unilateral declarations to
wide-ranging, internationally binding treaties.</p>
<p><strong>Unilateral commitments.</strong> Unilateral commitments
involve single actors like countries or companies making pledges
regarding their own technology development and use. For example,
President Richard Nixon terminated the US biological weapons program and
dismissed its scientists in 1969, which was instrumental in creating the
far-reaching Biological Weapons Convention in 1972. Leaders within
governments and companies can make such announcements, either about what
they would do in response to others’ actions or to place constraints on
their own behavior. While not binding on others, unilateral commitments
can change others’ best responses to a situation, as well as build
confidence and set precedents to influence international norms. They
also give credibility in pushing for broader agreements. Unilateral
commitments lay the groundwork for wider collective action.</p>
<p><strong>Norms and standards.</strong> International norms and
technical standards steer behavior without formal enforcement. Norms are
shared expectations of proper conduct, such as the norm of “no first
use” for detonating nuclear weapons. Standards set technical best
practices, like guidelines for the safe handling and storage of
explosives. Often, norms emerge through public discourse and precedent
while standards arise via expert communities. Even if they have no
ability to coerce actors, strong norms and standards shape actions
nonetheless. Additionally, they make it easier to build agreements by
aligning expectations. Norms and standards are a collaborative way to
guide technology development.</p>
<p><strong>Bilateral and multilateral talks.</strong> Two or more
countries directly negotiate over a variety of issues through bilateral
or multilateral talks. These allow open discussion and
confidence-building between rivals, such as granting the US and USSR the
ability to negotiate over the size and composition of their nuclear
arsenals during the Cold War. Talks aim to establish understandings to
avoid technology risks like arms races. Regular talks build
relationships and identify mutual interests. While non-binding
themselves, ongoing dialogue can lay the basis for making deals. Talks
are essential for compromise and consensus between nations.</p>
<p><strong>Summits and forums.</strong> Summits and forums bring
together diverse stakeholders for debate and announcements. These raise
visibility on issues and build political will for action. Major powers
can make joint statements signaling priorities, like setting goals on
total carbon emissions to limit the effects of global warming. Companies
and civil society organizations can announce major initiatives. Summits
set milestones for progress and mobilize public pressure.</p>
<p><strong>Governance organizations.</strong> International
organizations develop governance initiatives with diverse experts and
resources. Organizations like the IAEA propose principles and governance
models, such as an inspection and verification paradigm for nuclear
technology. They provide neutral forums to build agreements. Their
technical expertise also assists implementation and capacity-building.
While largely voluntary, organizations lend authority to governance
efforts, often by virtue of each of their members delegating some
authority to them. Their continuity sustains attention between summits.
Organizations enable cooperation for long-term governance.</p>
<p><strong>Treaties.</strong> Treaties offer the strongest form of
governance between nations, creating obligations backed by potential
punishment. Treaties have played a large role in banning certain risky
military uses of technologies, such as the 1968 Treaty on the
Non-Proliferation of Nuclear Weapons. They often contain enforcement
mechanisms like inspections. However, compromising on enforceable rules
is difficult, especially between rivals. Verifying compliance with
treaties can be challenging. Still, the binding power of treaties makes
them valuable despite their potential limitations.<br />
In this section, we have considered the various stages of international
governance, moving from recognizing an issue to solving it, as well as a
wide variety of different ways that enable this. This is a large set of
tools, so we will next examine four questions that inform our
understanding of which tools are most effective for AI governance.<br />
</p>
<h2 id="four-questions-for-ai-regulation">Four Questions for AI
Regulation</h2>
<p>We will now consider four questions that are important for the
regulation of dangerous emerging technologies:</p>
<ol>
<li><p>Is the technology defense-dominant or offense-dominant?</p></li>
<li><p>Can compliance with international agreements be
verified?</p></li>
<li><p>Is it catastrophic for international agreements to fail?</p></li>
<li><p>Can the production of the technology be controlled?</p></li>
</ol>
<p>Each of these highlights important strategic variables that we are
uncertain about. They give us insights into the characteristics of the
technology we are dealing with. Consequently, they help us illustrate
what sorts of international cooperation may be possible and
necessary.</p>
<h3 id="is-the-technology-defense-dominant-or-offense-dominant">Is the
Technology Defense-Dominant or Offense-Dominant?</h3>
<p><strong>AI capabilities determine the need for international
governance.</strong> Suppose we could use some AIs to prevent other AIs
from doing bad things. There would be less need for an international
regime to govern AIs. Instead, AI development would prevent the harms of
AI development—such technology is called <em>defense-dominant</em>. By
contrast, if AI is an offense-dominant technology—–if AIs cannot manage
other AIs as the technological frontier advances–—then we will need
alternative solutions <span class="citation"
data-cites="garfinkel2021does"></span>. Unfortunately, we have reason to
believe that AIs will be offense-dominant: military technologies usually
are. It is difficult for AIs to protect against threats from other AIs;
an AI that can make the creation of engineered pandemics easy is much
more likely to exist than one that can comprehensively defend against
pandemics. It is usually easier to cause harm than prevent it.<br />
Nuclear weapons are a classic example of offense-dominant technologies:
when asked how to detect a nuclear weapon smuggled into New York in a
crate, Robert Oppenheimer replied “with a screwdriver” to open every
crate <span class="citation" data-cites="panofsky2008panofsky"></span>.
In other words, there was no feasible technological solution; a social
solution was required. When dealing with offense-dominant technologies,
we often need to develop external social measures to defend against
them. Similarly, if AIs prove to be offense-dominant, they will also
require social solutions to protect against their potential harmful
impacts. Since the scale of the technology is global, this will likely
require international governance.</p>
<h3 id="can-compliance-with-international-agreements-be-verified">Can
compliance with international agreements be verified?</h3>
<p>Verification of compliance means agreements can regulate technology.
After establishing rules about the development and use of AIs, we need
to verify whether signatories are following them. If it is easy to
evaluate this, then it is easier to implement such regimes
internationally. In the case of nuclear weapons, nuclear tests could be
verified by monitoring for large explosions. Using a nuclear weapon is
impossible to do discreetly, enabling the norm of no first use.
Unfortunately, verifying how and when AIs are being developed and used
may be difficult to verify–—certainly more so than observing a nuclear
explosion.<br />
Verification is a difficult technical challenge. We lack clear methods
for conducting verification, since we do not know what to test to ensure
that models are safe. Progress can be made with serious effort; for
instance, investing in the development of standard benchmarks and
evaluations that promote transparency and enable shared knowledge that
other developers are using responsible development practices (such as
mitigating any power-seeking tendencies in AI systems). This will allow
the creation of clear standards that can be verified with relative ease,
without compromising the confidentiality of privileged technical
details. However, this requires investment: we must make progress on our
ability to verify characteristics of AIs before we can implement
effective international regulation. Until then, establishing an
equilibrium to deter the dangerous use of AIs may not be possible.</p>
<h3 id="is-it-catastrophic-for-international-agreements-to-fail">Is It
Catastrophic for International Agreements to Fail?</h3>
<p><strong>Whether agreements failing is catastrophic changes how
permissive they can be.</strong> Consider an example of an international
treaty that sets limits on the amount of compute that organizations can
use to train their AIs. If an AI trained with more compute than the
specified threshold poses a significant risk of catastrophic
consequences—–just as even a single nuclear weapon can have devastating
effects–—then the treaty must focus on preventing this possibility
entirely. In such cases, deviations from the agreement cannot be
permitted. On the other hand, if most AIs trained with more compute than
the threshold amount pose little immediate danger, we can adopt a more
lenient approach. In this scenario, the agreement can prioritize
monitoring and disincentivizing deviations through punishments after the
fact. In general, if even one actor deviating from an agreement is
dangerous, then it must be much stricter.<br />
Of course, stricter agreements are more difficult to create. Many
international agreements present small punishments for deviation or
include escape clauses—–allowing occasional exemptions from
obligations—–to encourage states to sign the agreements. If agreements
about the development and use of AIs cannot contain such clauses, then
it will be more difficult to create widespread agreement on them.</p>
<h3 id="can-the-production-of-the-technology-be-controlled">Can the
Production of the Technology Be Controlled?</h3>
<p><strong>Controlling production changes which actors are needed in
order to succeed with international regulation.</strong> Suppose the US
could gain complete control of all the compute required to produce AIs.
Then, the US would be able to create and implement regulations on AI
development that apply globally, since they can withhold compute from
any non-compliant actors. More generally, if a small group of
safety-conscious countries can block actors from gaining control of the
factors of production, then they can create an international regime
themselves. This makes it much easier to achieve international
governance since it doesn’t require the cooperation of many foreign
actors with distinct interests.</p>
<p>By answering these questions, we can make important decisions about
international governance. First, we understand whether we need
international governance or whether AIs will be able to mitigate the
harmful effects of other AIs. Second, we determine whether international
agreements are possible, since we need to verify whether actors are
following the rules. Third, we can decide what features these agreements
might have; specifically, we can determine whether they must be
extremely restrictive to avoid catastrophes from a single deviation.
Lastly, we consider who must agree to govern AI by understanding whether
or not a few countries can impose regulations on the world. Even if some
of these answers imply that we live in high-risk worlds, they guide us
towards actions that help mitigate this risk. We can now consider what
these actions might be.</p>
<h2 id="what-can-be-included-in-international-agreements">What Can Be
Included in International Agreements?</h2>
<p>We will now consider the specific tools that might be useful for
international governance of AI. We separate this discussion into
regulating AIs produced by the private sector and AIs produced by
militaries, since these have different features and thus require
different controls. For civilian AIs, certification of compliance with
international standards is the key precedent to follow. For military
AIs, we can turn to non-proliferation agreements, verification schemes,
or the establishment of AI monopolies.</p>
<p><strong>Regulating the private sector is important and
tractable.</strong> Much of the development of advanced AIs is seemingly
taking place in the private sector. As a result, ensuring that private
actors do not develop or misuse harmful technologies is a priority.
Regulating civilian development and use is also likely to be more
feasible than regulating militaries, although this might be hindered by
overlaps between private and military applications of AIs (such as
civilian defense contracts) and countries’ reluctance to allow an
international organization access to their firms’ activities.</p>
<p><strong>Certification has proven to be an effective tool.</strong>
One proposal for civilian governance involves certifying jurisdictions
for having and enforcing appropriate regulation <span class="citation"
data-cites="trager2023international"></span>. Some international
organizations, such as the International Civilian Aviation Organization
(ICAO), the International Maritime Organization (IMO), and the Financial
Action Task Force (FATF), follow a similar approach. These organizations
leave enforcement in the hands of domestic regulators, but they check
that domestic regulators have appropriate procedures. States have
incentives to comply with the international organizations’ standards
because of the ecosystems in which they are embedded. The Federal
Aviation Administration, for instance, can deny access to US airspace to
states that violate ICAO standards. In the case of AI, states might deny
access to markets and factors of production to the firms of
jurisdictions that violate international standards.</p>
<p>Governing military AIs is different from governing civilian ones.
Most of the options for governing AI used by militaries can be described
as drawing from one of three regimes: <em>nonproliferation plus norms of
use</em>, <em>verification</em>, or <em>monopoly</em>.</p>
<p><strong>Option 1: Nonproliferation plus Norms of Use.</strong> The
<em>nonproliferation</em> regime, alongside norms against first use of
nuclear weapons–—along with a measure of luck—–enabled the world to
survive the Cold War. This regime was centered around the
Nonproliferation Treaty (NPT), an international agreement primarily
concerned with stopping the spread of nuclear weapons, and the
International Atomic Energy Agency (IAEA), an international organization
for the governing of nuclear energy. In addition, nonproliferation was a
pillar of super-power foreign policy during the Cold War <span
class="citation" data-cites="gavin2015strategies"></span>. Both the US
and USSR made many threats and promises, including guarantees of
assistance to third parties, to reduce the likelihood of nuclear weapons
being used anywhere. A similar international regime for AI could be
similarly helpful; for instance, it could enable countries to cooperate
to avoid AI races and encourage the development of safer AI by
establishing standards and guidelines.</p>
<p><strong>Nonproliferation may be insufficient.</strong> Suppose
investing in AI continues to reliably increase system
capabilities—–unlike with nuclear weapons, where the security gain from
additional weapons of mass destruction is low. Then, countries who
already have advanced AI will have strong incentives to continue to
compete with each other. As we have explored, increasing AI capabilities
is likely to increase AI risk. This means that nonproliferation plus
norms of use might be insufficient for controlling advanced, weaponized
AIs.</p>
<p><strong>Norms may be difficult to establish.</strong> With nuclear
weapons, the <em>norms</em> of “no first use” and “mutually assured
destruction” created an equilibrium that limited the use of nuclear
weapons. With AIs, this might be more difficult for a variety of
reasons: for instance, AIs have a much broader field of capabilities (as
opposed to a nuclear weapon detonating) and AIs are already being widely
used. Monitoring or restricting the development or use of new AI systems
requires deciding precisely which capabilities are prohibited. If we
cannot decide which capabilities are the most dangerous, it is difficult
to decide on a set of norms, which means we cannot rely on norms to
encourage the development of safe AIs.</p>
<p><strong>Option 2: Verification.</strong> Many actors might be happy
to limit their own development of military technology if they can be
certain their adversaries are doing the same. <em>Verification</em> of
this fact can enable countries to govern each other, thereby avoiding
arms races. The Chemical Weapons Convention, for instance, has
provisions for verifying member states’ compliance, including
inspections of declared sites.<br />
When it comes to critical military technologies, however, verification
regimes might need to be invasive; for instance, it might be necessary
to have the authority to inspect <em>any</em> site in a country. It is
unclear how they could function in the face of resistance from domestic
authorities. For these reasons, a system which relies on inspection and
similar police—like methods might be entirely infeasible—unless all
relevant actors agree to mutually verify and govern.</p>
<p><strong>Option 3: Monopoly.</strong> The final option is a
<em>monopoly</em> over the largest-scale computing processes, which
removes the incentive for risk-taking by removing adversarial
competition. Such monopolies might arise in several ways. Established AI
firms may benefit from market forces like economies of scale, such as
being able to attract the best talent and invest profits from previous
ventures into new R&amp;D. Additionally, they might have first-mover
advantages from retaining customers or exercising influence over
regulation. Alternatively, several actors might agree to create a
monopoly: there are proposals like “CERN for AI” which call for this
explicitly <span class="citation"
data-cites="coyle2023preempting"></span>. Such organizations must be
focused on the right mission, perhaps using tools from corporate
governance, which is a non-trivial task. If they are, however, then they
present a much easier route to safe AI than verification and
international norms and agreements.</p>
<h3 id="conclusions-about-international-governance">Conclusions About
International Governance</h3>
<p><strong>International governance is important and difficult.</strong>
International regulation of AI is crucial to distribute its global
benefits and manage associated risks. Since AI’s impacts are not
restricted to its country of development, an internationally coordinated
approach ensures that advantages, like improved healthcare, are
accessible worldwide, and risks, such as weak regulations or safety
shortcuts, are avoided. This approach will need to create awareness that
a problem exists, create policies that tackle it, oversee the
implementation of these policies, and ensure compliance with them
through verification. It can use a wide variety of tools, including
unilateral declarations, talks through meetings, forums, and
international organizations, and the creation of norms, standards, and
binding treaties. From experience with other dangerous technologies, we
know that this global cooperation is challenging to achieve.</p>
<p><strong>Understanding features of AI is required for effective
governance.</strong> We need to understand whether AI is an
offense-dominant technology which poses significant risks if even one
powerful system is imperfect. This tells us whether we need
international cooperation of AIs and, if we do, what sort of enforcement
features such agreements would require to be effective. Further, we need
to know whether we can verify whether AI development meets a
comprehensive list of regulations and standards and control its
production when it does not. If this is possible, we can conclude that
international regulation of AI is possible, and might be possible with
the cooperation of just a small group of safety-conscious countries.</p>
<p><strong>Understanding the landscape, we can govern civilian and
military AIs.</strong> Civilian AIs largely originate from the private
sector, making certification of compliance with international standards
vital. Existing organizations, like the International Civilian Aviation
Organization, effectively use certification; states could restrict
market access to firms that violate international AI standards. For
military AIs, options include nonproliferation agreements plus
established norms, verification of technology development between
nations, or creating monopolies on large-scale computing. If the risks
of advanced AI are as great as some fear, it is likely that the world
needs such regulation. Each proposal will require overcoming serious
challenges, both social and technical.</p>
<h1 id="conclusion">Conclusion</h1>
<p>In the introduction, we laid out the purpose of this chapter:
understanding the fundamentals of how to govern AI. In other words, we
wanted to understand how to organize, manage, and steer the development
and deployment of AI technologies using an array of tools including
norms, policies, and institutions. To set the scene, we considered a set
of actors and tools that governance needs to consider.</p>
<p><strong>Takeoff.</strong> We explored potential scenarios for
advanced AI takeoff. There is serious uncertainty regarding when these
transformative AIs will arise and whether progress will follow
continuous trends or exhibit abrupt, discontinuous jumps in
capabilities. However, once they emerge, advanced AIs are likely to
massively accelerate technological advancement and economic growth. This
acceleration could profoundly reshape human civilization, potentially
within a short timeframe. Careful governance, informed by answers to
questions about how AI development will progress, will be essential for
steering it in broadly positive directions that benefit humanity.</p>
<p><strong>Distribution.</strong> We then explored three key dimensions
around which the impacts of advanced AI systems may be distributed:
power among AIs, access to AIs, and costs/benefits of AIs. First, in
terms of distributing power among AIs, concentrating capabilities and
control in a single AI or small group of AIs poses risks like
permanently locking in certain values or goals. However, distributing
power more widely among a large, diverse ecosystem of AIs also has
downsides, like increasing the potential for misuse or making it more
difficult to correct AI systems that begin behaving undesirably.<br />
Second, regarding access to AIs, it is unclear whether availability
should be tightly restricted or widely open to the general public.
Limiting access risks misuse by powerful groups whereas open access
risks misuse by malicious actors. Third, equitable distribution of the
economic and social impacts of AI will be crucial, including dealing
with potential technologically-induced unemployment while ensuring that
productivity gains are shared broadly rather than accruing only to a
small group like AI developers and investors. Distributing these three
key aspects of advanced AI fairly will require thoughtful
governance.</p>
<p><strong>Compute Governance.</strong> Next, we explored how governing
access to and use of the computing resources that enable AI development
could provide an important lever for influencing the trajectory of AI
progress. Compute is an indispensable input to developing advanced AI
capabilities. It also has properties like physicality, excludability,
and quantifiability that make governing it more feasible than other
inputs like data and algorithms. Compute governance can allow control
over who is granted access to what levels of computational capabilities,
controlling who can create advanced AIs. It also facilitates setting and
enforcing safety standards for how compute can be used, enabling the
steering of AI development.</p>
<p><strong>Corporate Governance.</strong> At the organizational level,
we discussed how aspects of corporate structure and governance like
legal form, ownership models, policies, practices, and assurance
mechanisms can help steer technology companies away from solely
maximizing profits and shareholder value. Instead, they can guide
corporate AI work in directions that prioritize broader societal
interests like safety, fairness, privacy, and ethics. However, achieving
this through corporate governance alone may prove challenging, making
complementary approaches at other levels vital.</p>
<p><strong>National Governance.</strong> For governance at the national
level, we explored policy tools governments can use to align AI
development with public interests, both in the public and private
sectors. These included safety regulations, liability rules that make AI
developers internalize potential damages, investments to improve
societal resilience against AI risks, and measures for maintaining
national competitiveness in AI while still ensuring domestic safety.
Combinations of these policy mechanisms can help nations steer AI
progress in their jurisdictions towards beneficial ends.</p>
<p><strong>International Governance.</strong> At the international
level, governance of AI systems is made challenging by issues like
verifying adherence to agreements. However, international cooperation is
essential for managing risks from AI and distributing benefits globally.
Approaches like international safety standards for civilian AI
applications, agreements to limit military uses of AI, and proposals for
concentrating advanced AI development within select transnational
groups, may all help promote global flourishing. A lack of any
meaningful international governance could lead to a dangerous spiral of
competitive dynamics and erosion of safety standards.</p>
<p><strong>Conclusion.</strong> This chapter provides an overview of a
diverse selection of governance solutions spanning from policies within
technology firms to agreements between nations in global institutions.
The arrival of transformative AI systems will require thoughtful
governance at multiple levels in order to steer uncertain technological
trajectories in broadly beneficial directions aligned with humanity’s
overarching interests. The deliberate implementation of policies,
incentives and oversight will be essential to realizing the potential of
AI to improve human civilization rather than destroy it.</p>
