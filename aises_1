<h1 id="introduction">Introduction</h1>
<p><strong>To reduce risks from AI systems, we need to understand their
technical foundations.</strong> Like many other technologies, AI
presents benefits and dangers on both individual and societal scales. In
addition, AI poses unique risks, as it involves the creation of
autonomous systems that can intelligently pursue objectives without
human assistance. This represents a significant departure from existing
technologies, and we have yet to understand its full implications,
especially since the internal workings of AI systems are often opaque
and difficult to observe or interpret. Nevertheless, the field is
progressing at a remarkable speed, and AI technologies are being
increasingly integrated into everyday life. Understanding the technical
underpinnings of AI can inform our understanding of what risks it poses,
how they may arise, and how they can be prevented or controlled.</p>
<p><strong>Overview.</strong> This chapter mostly focuses on machine
learning (ML), the approach that powers most modern AI systems. We
provide an overview of the essential elements of ML and discuss some
specific techniques. While the term “AI” is most commonly used to refer
to these technologies and will be the default in most of this book, in
this chapter we distinguish between AI, ML, and their subfields.</p>
<p><strong>Artificial Intelligence.</strong> We will begin our
exploration by discussing AI: the overarching concept of creating
machines that perform tasks typically associated with human
intelligence. We will introduce its history, scope, and how it permeates
our daily lives, as well as its practical and conceptual origins and how
it has developed over time. Then, we will survey different “types” or
“levels” commonly used to describe AI systems, including narrow AI,
artificial general intelligence (AGI), human-level AI (HLAI),
transformative AI (TAI), and artificial superintelligence (ASI) <span
class="citation" data-cites="bostrom2014superintelligence"></span>.</p>
<p><strong>Machine Learning.</strong> Next, we will narrow our
discussion to machine learning (ML), the subfield of AI focused on
creating systems that learn from data, making predictions or decisions
without being explicitly programmed. We will present fundamental
vocabulary and concepts related to ML systems: what they are composed
of, how they are developed, and common tasks they are used to achieve.
We will survey various types of machine learning, including supervised,
unsupervised, reinforcement, and deep learning, discussing their
applications, nuances, and interrelations.</p>
<p><strong>Deep Learning.</strong> Then, we will delve into deep
learning (DL), a further subset of ML that uses neural networks with
many layers to model and understand complex patterns in datasets. We
will discuss the structure and function of deep learning models,
exploring key building blocks and principles of how they learn. We will
present a timeline of influential deep learning architectures and
highlight a few of the countless applications of these models.</p>
<p><strong>Scaling Laws.</strong> Having established a basic
understanding of AI, ML, and DL, we will then explore scaling laws.
These are equations that model the improvements in performance of DL
models when increasing their parameter count and dataset size. We will
examine how these are often power laws—equations in which one variable
increases in proportion to a power of another, such as the area of a
square—and examine a few empirically determined scaling laws in recent
AI systems.</p>
<p><strong>Reinforcement Learning.</strong> Lastly, we will explore
reinforcement learning (RL), a type of ML that enables an agent to learn
how to behave in an environment by performing actions and receiving
rewards. We will present the basic framework and terminology of RL and
how it differs from other approaches in ML. We will conclude by
discussing a few of its applications.<br />
Throughout the chapter, we focus on building intuition, breaking down
technical terms and complex ideas to provide straightforward
explanations of their core principles. Each section presents fundamental
principles, shows prominent algorithms and techniques, and provides
examples of real-world applications. We aim to demystify these fields,
empowering us to grasp the concepts that underpin AI systems. By the end
of this chapter, we should have a well-rounded understanding of machine
learning, ready to delve deeper into the complexities and challenges of
AI systems, the risks they pose, and how they interact with our society.
This will provide the technical foundation we need for the following
chapters, which will explore the risks and ethical considerations that
these technologies present from a wide array of perspectives.</p>
<h1 id="sec:AI-and-ML">Artificial Intelligence &amp; Machine
Learning</h1>
<p>Artificial intelligence (AI) is reshaping our society, from its small
effects on daily interactions to sweeping changes across many industries
and implications for the future of humanity. This section explains what
AI is, discusses what AI can and cannot do, and helps develop a critical
perspective on the potential benefits and risks of artificial
intelligence. Firstly, we will discuss what AI means, its different
types, and its history. Then, in the second part of this section, we
will analyze the field of machine learning (ML).</p>
<h2 id="artificial-intelligence">Artificial Intelligence</h2>
<p><strong>Defining Artificial Intelligence.</strong> In general, AI
systems are computer systems performing tasks typically associated with
intelligent beings (such as problem solving, making decisions, and
forecasting future events) <span class="citation"
data-cites="Russell2020"></span>. However, due to its fast-paced
evolution and the variety of technologies it encompasses, AI lacks a
universally accepted definition, leading to varying interpretations.
Moreover, the term is used to refer to different but related ideas.
Therefore, it is essential to understand the contexts in which people
use the term. For instance, AI can refer to a branch of computer
science, a type of machine, a tool, a component of business models, or a
philosophical idea. We might use the term to discuss physical objects
with human-like capabilities, like robots or smart speakers. We may also
use AI in a thought experiment that prompts questions about what it
means to be intelligent or human and encourages debates on the ethics of
learning and decision-making machines. This textbook primarily uses AI
to refer to an intelligent computer system.</p>
<p><strong>Different meanings of intelligence.</strong> While
intelligence is fundamental to AI, there is no widespread consensus on
its definition <span class="citation" data-cites="Legg2007"></span>.
Generally, we consider something intelligent if it can learn to achieve
goals in various environments. Therefore, one definition of intelligence
is the ability to learn, solve problems, and perform tasks to achieve
goals in various changing, hard-to-predict situations. Some theorists
see intelligence as not just one skill among others but the ultimate
skill that allows us to learn all other abilities. Ultimately, the line
between what is considered <em>intelligent</em> and what is not is often
unclear and contested.<br />
Just as we consider animals and other organisms intelligent to varying
degrees, AIs may be regarded as intelligent at many different levels of
capability. An artificial system does not need to surpass all (or even
any) human abilities for some people to call it intelligent. Some would
consider GPT intelligent, and some would not. Similarly, outperforming
humans at specific tasks does not automatically qualify a machine as
intelligent. Calculators are usually much better than humans at
performing rapid and accurate mathematical calculations, but this does
not mean they are intelligent in a more general sense.</p>
<p><strong>Continuum of intelligence.</strong> Rather than classifying
systems as “AI” or “not AI,” it is helpful to think of the capabilities
of AI systems on a continuum. Evaluating the intelligence of particular
AI systems by their capabilities is more helpful than categorizing each
AI using theoretical definitions of intelligence. Even if a system is
imperfect and does not understand everything as a human would, it could
still learn new skills and perform tasks in a helpful, meaningful way.
Furthermore, an AI system that is not considered human-level or highly
intelligent could pose serious risks; for example, weaponized AIs such
as autonomous drones are not generally intelligent but still dangerous.
We will dive into these distinctions in more detail when we discuss the
different types of AI. First, we will explore the rich history of AI and
see its progression from myth and imagination to competent,
world-changing technology.</p>
<h3 id="history">History</h3>
<p>We will now follow the journey of AI, tracing its path from ancient
times to the present day. We will discuss its conceptual and practical
origins, which laid the foundation for the field’s genesis at the
<em>Dartmouth Conference</em> in 1956. We will then survey a few early
approaches and attempts to create AI, including <em>symbolic AI</em>,
<em>perceptrons</em>, and the chatbot ELIZA. Next, we will discuss how
the <em>First AI Winter</em> and subsequent periods of reduced funding
and interest have shaped the field. Then, we will chart how the
internet, algorithmic progress, and advancements in hardware led to
increasingly rapid developments in AI from the late 1980s to the early
2010s. Finally, we will explore the modern deep learning era and see a
few examples of the power and ubiquity of present-day AI systems—and how
far they have come.</p>
<p><strong>Early historical ideas of AI.</strong> Dreams of creating
intelligent machines have been present since the earliest human
civilizations. The ancient Greeks speculated about automatons—mechanical
devices that mimicked humans or animals. It was said that Hephaestus,
the god of craftsmen, built the giant Talos from bronze to patrol an
island.</p>
<p><strong>The modern conception of AI.</strong> Research to create
intelligent machines using computers began in the 1950s, laying the
foundation for a technological revolution that would unfold over the
following century. AI development gained momentum over the decades,
supercharged by groundbreaking technical algorithmic advances,
increasing access to data, and rapid growth in computing power. Over
time, AI evolved from a distant theoretical concept into a powerful
force transforming our world.</p>
<p><strong><em>Origins and Early Concepts (1941–1956)</em></strong></p>
<p><strong>Early computing research.</strong> The concept of computers
as we know them today was formalized by British mathematician Alan
Turing at the University of Cambridge in 1936. The following years
brought the development of several electromechanical machines (including
Turing’s own <em>bombes</em> used to decipher messages encrypted with
the German Enigma code) in the turmoil of World War II and, by the
mid-1940s, the first functioning digital computers emerged in their
wake. Though rudimentary by today’s standards, the creation of these
machines—Colossus, ENIAC, the Automatic Computing Engine, and several
others—marked the dawn of the computer age and set the stage for future
computer science research.</p>
<p><strong>The Turing Test .</strong> Turing created a thought
experiment to assess if an AI could convincingly simulate human
conversation<span class="citation" data-cites="Turing1950"></span>. In
what Turing called the <em>Imitation Game</em>, a human evaluator
interacts with a human and a machine, both hidden from view. If the
evaluator fails to identify the machine’s responses reliably, then the
machine passes the test, qualifying it as intelligent. This framework
offers a method for evaluating machine intelligence, yet it has many
limitations. Critics argue that machines could pass the Turing Test
merely by mimicking human conversation without truly understanding it or
possessing intelligence. As a result, some researchers see the Turing
Test as a philosophical concept rather than a helpful benchmark.
Nonetheless, since its inception, the Turing Test has substantially
influenced how we think about machine intelligence.</p>
<p><strong><em>The Birth of AI (1956–1974)</em></strong></p>
<p><strong>The Dartmouth Conference.</strong> Dr. John McCarthy coined
the term “artificial intelligence” in a seminal conference at Dartmouth
College in the summer of 1956. He defined AI as “the science and
engineering of making intelligent machines,” laying the foundation for a
new field of study. In this period, AI research took off in earnest,
becoming a significant subfield of computer science.</p>
<p><strong>Early approaches to AI.</strong> During this period, research
in AI usually built on a framework called symbolic AI, which uses
symbols and rules to represent and manipulate knowledge. This method
theorized that symbolic representation and computation alone could
produce intelligence. Good Old-Fashioned AI (GOFAI) is an early approach
to symbolic AI that specifically involves programming explicit rules for
systems to follow, attempting to mimic human reasoning. This intuitive
approach was popular during the early years of AI research, as it aimed
to replicate human intelligence by modeling how humans think, instilling
our reasoning, decision-making, and information-processing abilities
into machines.<br />
These “old-fashioned” approaches to AI allowed machines to accomplish
well-described, formalizable tasks, but they faced severe difficulties
in handling ambiguity and learning new tasks. Some early systems
demonstrated problem-solving and learning capabilities, further
cementing the importance and potential of AI research. For instance, one
proof of concept was the General Problem Solver, a program designed to
mimic human problem-solving strategies using a trial-and-error approach.
The first <em>learning machines</em> were built in this period, offering
a glimpse into the future of machine learning.</p>
<p><strong>The first neural network.</strong> One of the earliest
attempts to create AI was the perceptron, a method implemented by Frank
Rosenblatt in 1958 and inspired by biological neurons <span
class="citation" data-cites="rosenblatt1958perceptron"></span>. The
perceptron could learn to classify patterns of inputs by adjusting a set
of numbers based on a learning rule. It is an important milestone
because it made an immense impact in the long run, inspiring further
research into deep learning and neural networks. However, scholars
initially criticized it for its lack of theoretical foundations, minimal
generalizability, and inability to separate data clusters with more than
just a straight line. Nonetheless, perceptrons prepared the ground for
future progress.</p>
<p><strong>The first chatbot.</strong> Another early attempt to create
AI was the ELIZA chatbot, a program that simulated a conversation with a
psychotherapist. Joseph Weizenbaum created ELIZA in 1966 to use pattern
matching and substitution to generate responses based on keywords in the
user’s input. He did not intend the ELIZA chatbot to be a serious model
of natural language understanding but rather a demonstration of the
superficiality of communication between humans and machines. However,
some users became convinced that the ELIZA chatbot had genuine
intelligence and empathy despite Weizenbaum’s insistence to the
contrary.</p>
<p><strong><em>AI Winters and Resurgences (1974–1995)</em></strong></p>
<p><strong>First AI Winter.</strong> The journey of AI research was not
always smooth. Instead, it was characterized by <em>hype cycles</em> and
hindered by several <em>winters</em>: periods of declining interest and
progress in AI. The late 1970s saw the onset of the first and most
substantial decline. In this period, called the <em>First AI Winter</em>
(from around 1974 to 1980), AI research and funding declined markedly
due to disillusionment and unfulfilled promises, resulting in a slowdown
in the field’s progress.</p>
<p><strong>The first recovery.</strong> After this decline, the 1980s
brought a resurgence of interest in AI. Advances in computing power and
the emergence of systems that emulate human decision-making
reinvigorated AI research. Efforts to build expert systems that imitated
the decision-making ability of a human expert in a specific field, using
pre-defined rules and knowledge to solve complex problems, yielded some
successes. While these systems were limited, they could leverage and
scale human expertise in various fields, from medical diagnosis to
financial planning, setting a precedent for AI’s potential to augment
and even replace human expertise in specialized domains.</p>
<p><strong>The second AI winter.</strong> Another stagnation in AI
research started around 1987. Many AI companies closed, and AI
conference attendance fell by two thirds. Despite widespread lofty
expectations, expert systems had proven to be fundamentally limited.
They required an arduous, expensive, top-down process to encode rules
and heuristics in computers. Yet expert systems remained inflexible,
unable to model complex tasks or show common-sense reasoning. This
winter ended by 1995, as increasing computing power and new methods
aided a resurgence in AI research.</p>
<p><strong><em>Advancements in Machine Learning
(1995–2012)</em></strong></p>
<p><strong>Accelerating computing power and the Internet.</strong> The
invention of the Internet, which facilitated rapid information sharing,
with exponential growth in computing power (often called
<em>compute</em>) helped the recovery of AI research and enabled the
development of more complex systems. Between 1995 and 2000, the number
of Internet users grew by 2100%, which led to explosive growth in
digital data. This abundant digitized data served as a vast resource for
machines to learn from, eventually driving advancements in AI
research.</p>
<p><strong>A significant victory of AI over humans.</strong> In 1997,
IBM’s AI system <em>Deep Blue</em> defeated world chess champion Garry
Kasparov, marking the first time a computer triumphed over a human in a
highly cognitive task <span class="citation"
data-cites="campbell2002deep"></span>. This win demonstrated that AI
could excel in complex problem-solving, challenging the notion that such
tasks were exclusively in the human domain. It offered an early glimpse
of AI’s potential.</p>
<p><strong>The rise of probabilistic graphical models (PGMs) <span
class="citation" data-cites="Koller2009"></span>.</strong> PGMs became
prominent in the 2000s due to their versatility, computational
efficiency, and ability to model complex relationships. These models
consist of nodes representing variables and edges indicating
dependencies between them. By offering a systematic approach to
representing uncertainty and learning from data, PGMs paved the way for
more advanced ML systems. In bioinformatics, for instance, PGMs have
been employed to predict protein interactions and gene regulatory
networks, providing insights into biological processes.</p>
<p><strong>Developments in tree-based algorithms.</strong> Decision
trees are an intuitive and widely used ML method. They consist of a
graphical representation of a series of rules that lead to a prediction
based on the input features; for example, researchers can use a decision
tree to classify whether a person has diabetes based on age, weight, and
blood pressure. However, these trees have many limitations, a tendency
to make predictions based on the training data without generalizing well
to new data (called overfitting).<br />
Researchers in the early 2000s created methods for combining multiple
decision trees to overcome these issues. <em>Random forests</em> are a
collection of decision trees trained independently on different subsets
of data and features <span class="citation"
data-cites="Breiman2001"></span>. The final prediction is the average or
majority vote of the predictions of all the trees. <em>Gradient
boosting</em> combines decision trees in a more sequential, adaptive
way, starting with a single tree that makes a rough prediction and then
adding more trees to correct the errors of previous trees <span
class="citation" data-cites="Friedman2001"></span>. Gradient-boosted
decision trees are the state-of-the-art method for tabular data (such as
spreadsheets), usually outperforming deep learning.</p>
<p><strong>The impact of support vector machines (SVMs).</strong> The
adoption of SVM models in the 2000s was a significant development. SVMs
operate by finding an optimal boundary that best separates different
categories of data points, permitting efficient classification <span
class="citation" data-cites="Cortes1995"></span>; for instance, an SVM
could help distinguish between handwritten characters. Though these
models were used across various fields during this period, SVMs have
fallen out of favor in modern machine learning due to the rise of deep
learning methods.</p>
<p><strong>New chips and even more compute.</strong> In the late 2000s,
the proliferation of massive datasets (known as <em>big data</em>) and
rapid growth in computing power allowed the development of advanced AI
techniques. Around the early 2010s, researchers began using <em>Graphics
Processing Units</em> (GPUs)—traditionally used for rendering graphics
in video games—for faster and more efficient training of intricate ML
models. Platforms that enabled leveraging GPUs for general-purpose
computing facilitated the transition to the deep learning era.</p>
<p><strong><em>Deep Learning Era (2012– )</em></strong></p>
<p><strong>Deep learning revolutionizes AI.</strong> The trends of
increasing data and compute availability laid the foundation for
groundbreaking ML techniques. In the early 2010s, researchers pioneered
applications of <em>deep learning (DL)</em>, a subset of ML that uses
artificial neural networks with many layers, enabling computers to learn
and recognize patterns in large amounts of data. This approach led to
significant breakthroughs in AI, especially in areas including image
recognition and natural language understanding.<br />
Massive datasets provided researchers with the data needed to train deep
learning models effectively. A pivotal example is the <em>ImageNet</em>
(<span class="citation" data-cites="deng2009imagenet"></span>) dataset,
which provided a large-scale dataset for training and evaluating
computer vision algorithms. It hosted an annual competition, which
spurred breakthroughs and advancements in deep learning. In 2012, the
<em>AlexNet</em> model revolutionized the field as it won the ImageNet
Large Scale Visual Recognition Challenge <span class="citation"
data-cites="krizhevsky2012advances"></span>. This breakthrough showcased
the superior performance of deep learning over traditional machine
learning methods in computer vision tasks, sparking a surge in deep
learning applications across various domains. From this point onward,
deep learning has dominated AI and ML research and the development of
real-world applications.</p>
<p><strong>Advancements in DL.</strong> In the 2010s, deep learning
techniques led to considerable improvements in <em>natural language
processing (NLP)</em>, a field of AI that aims to enable computers to
understand and generate human language. These advancements facilitated
the widespread use of virtual assistants Alexa and ChatGPT, introducing
consumers to products that integrated machine learning. Later, in 2016,
Google DeepMind’s AlphaGo () became the first AI system to defeat a
world champion Go player in a five-game match <span class="citation"
data-cites="silver2016masteringgo"></span>.</p>
<p><strong>Breakthroughs in natural language processing.</strong> In
2018, Google researchers introduced the <em>Transformer</em>
architecture, which enabled the development of highly effective NLP
models. Researchers built the first <em>large language models</em>
(LLMs) using this Transformer architecture, many layers of neural
networks, and billions of words of data. <em>Generative Pre-trained
Transformer</em> (GPT) models have demonstrated impressive and near
human-level language processing capabilities <span class="citation"
data-cites="Radford2019LanguageMA"></span>. ChatGPT was released in
November 2022 and became the first example of a viral AI product,
reaching 100 million users in just two months. The success of the GPT
models also sparked widespread public discussion on the potential risks
of advanced AI systems, including congressional hearings and calls for
regulation. In the early 2020s, AI is used for many complex tasks, from
image recognition to autonomous vehicles, and continues to evolve and
proliferate rapidly.</p>
<h2 id="types-of-ai">Types of AI</h2>
<p>The field has developed a set of concepts to describe distinct types
or levels of AI systems. However, they often overlap, and definitions
are rarely well-formalized, universally agreed upon, or precise. It is
important to consider an AI system’s particular capabilities rather than
simply placing it in one of these broad categories. Labeling a system as
a “weak AI” does not always improve our understanding of it; we need to
elaborate further on its abilities and why they are limited.<br />
This section introduces five widely used conceptual categories for AI
systems. We will present these types of AI in roughly their order of
intelligence, generality, and potential impact, starting with the least
potent AI systems.</p>
<ol>
<li><p><strong>Narrow AI</strong> can perform specific tasks.</p></li>
<li><p><strong>Artificial general intelligence</strong> (AGI) can
perform many cognitive tasks across multiple domains at a human or
superhuman level.</p></li>
<li><p><strong>Human-level AI</strong> (HLAI) could do virtually
everything humans do.</p></li>
<li><p><strong>Transformative AI</strong> (TAI) is a term for systems
with a dramatic impact on the world, at least at the level of the
Industrial Revolution.</p></li>
<li><p><strong>Artificial superintelligence</strong> (ASI) is the most
powerful, describing systems vastly outclassing human performance on
virtually all intellectual tasks <span class="citation"
data-cites="Bostrom2014"></span>.</p></li>
</ol>
<p>There are not always clear distinctions between these types; these
concepts often overlap. We can use these types to describe a system’s
level of capability, which can be roughly decomposed into its degree of
intelligence and its generality: the range of domains where it can learn
to perform tasks well. This helps us explain two key ways AI systems can
vary: an AI system can be more or less intelligent and more or less
general. These two factors are related but distinct: an AI system that
can play chess at a grandmaster level is intelligent in that domain, but
we would not consider it general because it can only play chess. On the
other hand, an advanced chatbot may show some forms of general
intelligence while not being particularly good at chess.</p>
<h3 id="narrow-ai">Narrow AI</h3>
<p><strong>Narrow AI is specialized in one area.</strong> Also called
<em>weak AI</em>, narrow AI refers to systems designed to perform
specific tasks or solve particular problems within a specialized domain
of expertise. A narrow AI has a limited domain of competence—it can
solve individual problems but is not competent at learning new tasks in
a wide range of domains. While they often excel in their designated
tasks, these limitations mean that a narrow AI does not exhibit high
behavioral flexibility. Narrow AI systems struggle to learn new
behaviors effectively, perform well outside their specific domain, or
generalize to new situations.</p>
<p><strong>Examples of narrow AI.</strong> One example of narrow AI is a
digital personal assistant that can receive voice commands and perform
tasks like transcribing and sending text messages but cannot learn how
to write an essay or drive a car. Alternatively, image recognition
algorithms can identify objects like people, plants, or buildings in
photos but do not have other skills or abilities. Another example is a
program that excels at summarizing news articles. While it can do this
narrow task, it cannot diagnose a medical condition or compose new
music, as these are outside its specific domain. More generally,
intelligent beings such as humans can learn and perform all these
tasks.</p>
<p><strong>Narrow AI vs. general AI.</strong> Some narrow AI systems
have surpassed human performance in specific tasks, such as chess.
However, these systems exhibit narrow rather than general intelligence
because they cannot learn new tasks and perform well outside their
domain. For instance, IBM’s Deep Blue famously beat world chess champion
Garry Kasparov in 1997. This system was an excellent chess player but
was only good at chess. If one tried to use Deep Blue to play a
different game, recognize faces in a picture, or translate a sentence,
it would fail miserably. Therefore, although narrow AI may be able to do
certain things better than any human could, even highly capable ones
remain limited to a small range of tasks.</p>
<h3 id="artificial-general-intelligence-agi">Artificial General
Intelligence (AGI)</h3>
<p><strong>AGI is far more generally capable.</strong> Generally
intelligent systems called AGIs can learn and perform various tasks in
various areas. Unlike narrow AIs, which only excel in specific domains,
AGI refers to more flexible systems. An AGI can apply its intelligence
to nearly any real-world task, matching or surpassing human cognitive
abilities across many domains. An AGI can reason, learn, and respond
well to new situations it has never encountered before. It can even
learn to generalize its strong performance to many domains without
requiring specialized training for each one: it could initially learn to
play chess, then continue to expand its knowledge and abilities by
learning video games, diagnosing diseases, or navigating a city.
Although it may not be helpful as a guide to implementation or even
precisely defined, AGI is a useful theoretical concept for reasoning
about the capabilities of advanced systems.</p>
<p><strong>There is no single consensus definition of AGI.</strong>
Constructing a precise and detailed definition of AGI is challenging and
often creates disagreement among experts; for instance, some argue that
an AGI must have a physical embodiment to interact with the world,
allowing it to cook a meal, move around, and see and interact with
objects. Others contend that a system could be generally intelligent
without any ability to physically interact with the world, as
intelligence does not require a human-like body. Some would say ChatGPT
is an AGI because it is not narrow and is, in many senses, general.
Still, an AI that can interact physically may be more general than a
non-embodied system. This shows the difficulty of reaching a consensus
on the precise meaning of AGI.</p>
<p><strong>Predicting AGI.</strong> Predicting when distinct AI
capabilities will appear (often called “AI timelines”) can also be
challenging. Many once believed that AI systems would master physical
tasks before tackling “higher-level” cognitive tasks such as coding or
writing. However, some existing language model systems can write
functional code yet cannot perform physical tasks such as moving a ball.
While there are many explanations for this observation—cognitive tasks
bypass the challenge of building robotic bodies; domains like coding and
writing benefit from abundant training data—the evidence suggests
experts face difficulties predicting how AI will develop.</p>
<p><strong>Risks and capabilities.</strong> Rather than debating whether
a system meets the criteria for being an AGI, evaluating a specific AI
system’s capabilities is often more helpful. Historical evidence and the
unpredictability of AI development suggest that AIs can perform
complicated tasks such as scientific research, hacking, or synthesizing
bioweapons before AIs can drive vehicles autonomously. Some highly
relevant and dangerous capabilities may arrive long before others.
Moreover, we could have narrow AI systems that can teach anyone how to
enrich uranium and build nuclear weapons but cannot learn other tasks.
These dangers show how AIs can pose risks at many different levels of
capabilities. With this in mind, instead of simply asking about AGI
(“When will AGI arrive?”), it might be more relevant and productive to
consider when AIs will be able to do particularly concerning tasks
(“When will this specific capability arrive?”).</p>
<h3 id="human-level-artificial-intelligence-hlai">Human-Level Artificial
Intelligence (HLAI)</h3>
<p><strong>Human-level artificial intelligence (HLAI) can do almost
everything humans can do.</strong> HLAIs exist when machines can perform
approximately every task as well as human workers. Some definitions of
HLAI emphasize three conditions: first, that these systems can perform
every task humans can; second, they can do it at least as well as humans
can; and third, they can do it at a lower cost. If a smart AI is highly
expensive, it may make economic sense to continue to use human labor. If
a smart AI took several minutes to think before doing a task a human
could do, its usefulness would have limitations. Like humans, an HLAI
system could hypothetically master a wide range of tasks, from cooking
and driving to advanced mathematics and creative writing. Unlike AGI,
which can perform some—but not all—the tasks humans can, an HLAI can
complete any conceivable human task. Notably, some reserve the term HLAI
to describe only cognitive tasks. Furthermore, evaluating whether a
system is “human level’ is fraught with biases. We are often biased to
dismiss or underrate unfamiliar forms of intelligence simply because
they do not look or act like human intelligence.</p>
<h3 id="transformative-ai-tai">Transformative AI (TAI)</h3>
<p><strong>Transformative AI (TAI) has impacts comparable to the
Industrial Revolution.</strong> The Industrial Revolution fundamentally
altered the fabric of human life globally, heralding an era of
tremendous economic growth, increased life expectancy, expanded energy
generation, a surge in technological innovation, and monumental social
changes. Similarly, a transformative AI could catalyze dramatic changes
in our world. The focus here is not on the specific design or built-in
capabilities of the AI itself but on the consequences of the AI system
for humans, our societies, and our economies.</p>
<p><strong>Many kinds of AI systems could be transformative.</strong> It
is conceivable that some systems could be transformative but below the
human level. To bring about dramatic change, AI does not need to mimic
the powerful systems of science fiction that behave indistinguishably
from humans or surpass human reasoning. Computer systems that can
perform tasks traditionally handled by people (narrow AIs) could also be
transformative by enabling inexpensive, scalable, and clean energy
production. Advanced AI systems could transform society without reaching
or exceeding human-level cognitive abilities, such as by allowing a wide
array of fundamental tasks to be performed at virtually zero cost. Some
systems could be transformative long after being above the human level.
Even when some forms of AGI, HLAI, or ASI are available, the technology
might take time to diffuse widely, and its economic impacts may come
years afterward, creating a <em>diffusion lag</em>.</p>
<h3 id="superintelligence-asi">Superintelligence (ASI)</h3>
<p><strong>Superintelligence (ASI) is the most advanced type of AI <span
class="citation" data-cites="Bostrom2014"></span>.</strong>
Superintelligence refers to the ability to outclass human performance in
virtually all domains of interest. A system with this set of
capabilities could have immense practical applications, including
advanced problem-solving, automation of complex tasks, and scientific
discovery. It represents the most powerful type of AI. However, it
should be noted that surpassing humans on only some capabilities does
not make an AI superintelligent—a calculator is superhuman at
arithmetic, but not a superintelligence.</p>
<p><strong>Risks of superintelligence.</strong> The risks associated
with superintelligence are substantial. ASIs could become uncontrollable
and even pose existential threats—risks to the survival of humanity.
That said, an AI system must not be superintelligent to be dangerous. An
AGI, human-level AI, or narrow AI could all pose severe risks to
humanity. These systems may vary in intelligence across different tasks
and domains, but they can be dangerous at many levels of intelligence
and generality. If a narrow AI is superhuman at a specific dangerous
task like synthesizing viruses, it could be an extraordinary hazard for
humanity.</p>
<p><strong>Superintelligence is not omnipotence.</strong> Separately, we
should not assume that superintelligence must be omnipotent or
omniscient. Superintelligence does not mean that an AI can instantly
predict how events worldwide will unfold in the far future, nor that the
system can completely predict the actions of all other agents with
perfect accuracy. Likewise, it does not mean that the ASI could
instantly overpower humanity. Moreover, many problems cannot be solved
by intelligence or contemplation alone; research and development require
real-world experimentation, which involves physical-world processes that
take a long time, presenting a key constraint to AIs’ ability to
influence the world. However, we know very little about what a system
that is significantly smarter than humans could do. Therefore, it is
difficult to make confident claims about superintelligence.</p>
<p><strong>Intelligence beyond human understanding.</strong> A
superintelligent AI would significantly outstrip human capabilities,
potentially solving problems and making discoveries beyond our
comprehension. Of course, this is not exclusive to superintelligence:
even narrow AIs solve problems humans find difficult to understand.
AlphaFold, for instance, astonished scientists by predicting the 3D
structure of proteins—a complex problem that stumped biochemists for
decades. Ultimately, a superintelligence exceeds these other types of AI
in the degree to which it exceeds human abilities across a variety of
cognitive tasks.</p>
<h3 id="summary">Summary</h3>
<p>This section provided an introduction to artificial intelligence
(AI), the broad umbrella that encompasses the area of computer science
focused on creating machines that perform tasks typically associated
with human intelligence. First, we discussed the nuances and
difficulties of defining AI and detailed its history. Then, we explored
AI systems in more detail and how they are often categorized into
different types. Of these, we surveyed the five most common—narrow AI,
human-level AI, artificial general intelligence, transformative AI, and
superintelligence—and saw how specificity is often the best way to
evaluate AIs. Considering specific capabilities and individual systems
rather than broad categories or abstractions is far more
informative.<br />
Next, we will narrow our focus to machine learning (ML), an approach
within AI that emphasizes the development of systems that can learn from
data. Whereas many classical approaches to AI relied on logical rules
and formal, structured knowledge, ML systems use pattern recognition to
extract information from data.</p>
<h2 id="machine-learning">Machine Learning</h2>
<h3 id="overview-and-definition">Overview and Definition</h3>
<p>Machine learning (ML) is a subfield of AI that focuses on developing
computer systems that can learn directly from data without following
explicit pre-set instructions <span class="citation"
data-cites="Bishop2006 Murphy2022"></span>. It accomplishes this by
creating computational models that discern patterns and correlations
within data. The knowledge encoded in these models allows them to inform
decision-making or to reason about and act in the world. For instance,
an email spam filter uses ML to improve its ability to distinguish spam
from legitimate emails as it sees more examples. ML is the engine behind
most modern AI applications, from personalized recommendations on
streaming services to autonomous vehicles. One of the most popular and
influential algorithmic techniques for ML applications is deep learning
(DL), which uses deep neural networks to process data.</p>
<p><strong>Machine learning algorithms.</strong> An <em>algorithm</em>
is a recipe for getting something done—a procedure for solving a problem
or accomplishing a task, often expressed in a precise programming
language. Machine learning (ML) models are algorithms designed to learn
from data by identifying patterns and relationships, which enables them
to make predictions or decisions as they process new inputs. They often
learn from information called training data. What makes ML models
different from other algorithms is that they automatically learn
patterns in data without explicit task-specific instructions. Instead,
they identify correlations, dependencies, or relationships in the data
and use this information to make predictions or decisions about new
data; for instance, a content curation application may use ML algorithms
to refine its recommendations.</p>
<p><strong>Benefits of ML.</strong> One of the key benefits of ML is its
ability to automate complicated tasks, enabling humans to focus on other
activities. Developers use ML for applications from medical diagnosis
and autonomous vehicles to financial forecasting and writing. ML is
becoming increasingly important for businesses, governments, and other
organizations to stay competitive and make empirically informed
decisions.</p>
<p><strong>Guidelines for understanding ML models.</strong> ML models
can be intricate and varied, making understanding their characteristics
and distinctions a challenge. It can be helpful to focus on key
high-level aspects that almost all ML systems have:</p>
<ul>
<li><p><strong>General Task: What is the primary goal of the ML
model?</strong> We design models to achieve objectives. Some example
tasks are predicting housing prices, generating images or text, or
devising strategies to win a game.</p></li>
<li><p><strong>Inputs: What data does the ML system receive?</strong>
This is the information that the model processes to deliver its
results.</p></li>
<li><p><strong>Outputs: What does the ML system produce?</strong> The
model generates these results, predictions, or decisions based on the
input data.</p></li>
<li><p><strong>Type of Machine Learning: What technique is used to
accomplish the task?</strong> This describes how the models converts its
inputs into outputs (called inference), and learn the best way to
convert its inputs into outputs (a learning process called training). An
ML system can be categorized by how it uses training data, what type of
output it generates, and how it reaches results.</p></li>
</ul>
<p>The rest of this section delves deeper into these aspects of ML
systems.</p>
<h3 id="key-ml-tasks">Key ML Tasks</h3>
<p>In this section, we will explore four fundamental ML
tasks—classification, regression, anomaly detection, and sequence
modeling—that describe different problems or types of problems that ML
models are designed to solve.</p>
<p><strong><em>Classification</em></strong></p>
<p><strong>Classification is predicting categories or classes.</strong>
In classification tasks, models use characteristics or <em>features</em>
of an input data point (example) to determine which specific category
the data point belongs to. In medical diagnostics, a classification
model might predict whether a tumor is cancerous or benign based on
features such as a patient’s age, tumor size, and tobacco use. This is
an example of <em>binary classification</em>—the special case in which
models predict one of two categories. <em>Multi-class
classification</em>, on the other hand, involves predicting one of
multiple categories. An image classification model might classify an
image as belonging to one of multiple different classes such as dog,
cat, hat, or ice cream. <em>Computer vision</em> often applies these
methods to enable computers to interpret and understand visual data from
the world. Classification is categorization: it involves putting data
points into buckets.</p>
<figure id="fig:example-binary-class">
<embed src="images/ML_background/binary_classification.pdf" />
<figcaption>ML models can classify data into different categories. <span
class="citation" data-cites="drouin2017class"></span></figcaption>
</figure>
<p><strong>The sigmoid function produces probabilistic outputs.</strong>
A sigmoid is one of several mathematical functions used in
classification to transform general real numbers into values between 0
and 1. Suppose we wanted to predict the likelihood that a student will
pass an exam or that a prospective borrower will default on a loan. The
sigmoid function is instrumental in settings like these—problems that
rely on computing probabilities. As a further example, in binary
classification, one might use a function like the sigmoid to estimate
the likelihood that a customer makes a purchase or clicks on an
advertisement. However, it is important to note that other widely used
models can provide similar probabilistic outputs without employing a
sigmoid function.<br />
</p>
<figure id="fig:logistic-classification">
<img src="images/ML_background/image79.png" />
<figcaption>Example of binary classification with a sigmoid-like
logistic curve - <span class="citation"
data-cites="wikipedia-logistic"></span></figcaption>
</figure>
<p><strong><em>Regression</em></strong></p>
<p><strong>Regression is predicting numbers.</strong> In regression
tasks, models use features of input data to predict numerical outputs. A
real estate company might use a regression model to predict house prices
from a dataset with features such as location, square footage, and
number of bedrooms. While classification models produce
<em>discrete</em> outputs that place inputs into a finite set of
categories, regression models produce <em>continuous</em> outputs that
can assume any value within a range. Therefore, regression is predicting
a continuous output variable based on one or more input variables.
Regression is estimation: it involves guessing what a feature of a data
point will be given the rest of its characteristics.</p>
<figure id="fig:simple_regression">
<img src="images/ML_background/image71.png" />
<figcaption>Example regression plot - <span class="citation"
data-cites="drouin2017class"></span></figcaption>
</figure>
<p><strong>Linear regression.</strong> One type of regression, linear
regression, assumes a linear relationship between features and predicted
values for a target variable. A linear relationship means that the
output changes at a constant rate with respect to the input variables,
such that plotting the input-output relationship on a graph forms a
straight line. Linear regression models are often helpful but have many
limitations; for instance, their assumption that the features and the
target variable are linearly related is often false. In general, linear
regression can struggle with modeling complicated data patterns in the
real world since they are roughly only as complex as their input
variables and struggle to add additional structures themselves.</p>
<p><strong><em>Anomaly Detection</em></strong></p>
<p><strong>Anomaly detection is the identification of outliers or
abnormal data points <span class="citation"
data-cites="hendrycks2018baseline"></span>.</strong> Anomaly detection
is vital in identifying hazards, including unexpected inputs, attempted
cyberattacks, sudden behavioral shifts, and unanticipated failures.
Early detection of anomalies can substantially improve the performance
of models in real-world situations.</p>
<figure>
<img src="images/ML_background/image81.png" style="width:90.0%" />
<img src="images/ML_background/image66.png" style="width:90.0%" />
<figcaption>Two examples of anomaly detection -<span class="citation"
data-cites="hendrycks-anomaly"></span></figcaption>
</figure>
<p><strong>Black swan detection is an essential problem within anomaly
detection.</strong> Black swans are unpredictable and rare events with a
significant impact on the broader world. These events are difficult to
predict because they may not have happened before, so they are not
represented in the training data that ML models use to extrapolate the
future. Due to their extreme and uncommon nature, such events make
anomaly detection challenging. In section <a href="#sec:black-swans"
data-reference-type="ref"
data-reference="sec:black-swans">[sec:black-swans]</a> in the Safety
Engineering chapter, we discuss these ideas in more detail.</p>
<p><strong><em>Sequence Modeling</em></strong></p>
<p><strong>Sequence modeling is analyzing and predicting patterns in
sequential data.</strong> Sequence modeling is a broadly defined task
that involves processing or predicting data where temporal or sequential
order matters. It may be applied to time-series data or natural language
text to capture dependencies between items in the sequence to forecast
future elements. An integral part of this process is <em>representation
learning</em>, where models learn to convert raw data into more
informative formats for the task at hand. Language models use these
techniques to predict subsequent words in a sequence, transforming
previous words into meaningful representations to detect patterns and
make predictions. There are several major subtypes of sequence modeling.
Here, we will discuss two: <em>generative modeling</em> and
<em>sequential decision-making</em>.<br />
<em>Generative modeling.</em> Generative modeling is a subtype of
sequence modeling that creates new data that resembles the input data,
thereby drawing from the same distribution of features (conditioned on
specific inputs). It can generate new outputs from many input types,
such as text, code, images, and protein sequences.<br />
<em>Sequential decision-making (SDM).</em> SDM equips a model with the
capability to make informed choices over time, considering the dynamic
and uncertain nature of real-world environments. An essential feature of
SDM is that prior decisions can shape later ones. Related to SDM is
<em>reinforcement learning (RL)</em>, where a model learns to make
decisions by interacting with its environment and receiving feedback
through rewards or penalties. An example of SDM in complex, real-world
tasks is a robot performing a sequence of actions based on its current
understanding of the environment.</p>
<h3 id="types-of-input-data">Types of Input Data</h3>
<p>In machine learning, a <em>modality</em> refers to how data is
collected or represented—the type of input data. Some models, such as
image recognition models, use only one type of input data. In contrast,
<em>multimodal</em> systems integrate information from multiple
modalities (such as images and text) to improve the performance of
learning-based approaches. Humans are naturally multimodal, as we
experience the world by seeing objects, hearing sounds, feeling
textures, smelling odors, tasting flavors, and more.<br />
Below, we briefly describe the significant modalities in ML. However,
this list is not exhaustive. Many specific types of inputs, such as data
from physical sensors, fMRI scans, topographic maps, and so on, do not
fit easily into this categorization.</p>
<ul>
<li><p><strong>Tabular data</strong>: Structured data is stored in rows
and columns, usually with each row corresponding to an observation and
each column representing a variable in the dataset. An example is a
spreadsheet of customer purchase histories.</p></li>
<li><p><strong>Text data</strong>: Unstructured textual data in natural
language, code, or other formats. An example is a collection of posts
and comments from an online forum.</p></li>
<li><p><strong>Image data</strong>: Digital representations of visual
information that can train ML models to classify images, segment images,
or perform other tasks. An example is a database of plant leaf images
for identifying species of plants.</p></li>
<li><p><strong>Video data</strong>: A sequence of visual information
over time that can train ML models to recognize actions, gestures, or
objects in the footage. An example is a collection of sports videos for
analyzing player movements.</p></li>
<li><p><strong>Audio data</strong>: Sound recordings, such as speech or
music. An example is a set of voice recordings for training speech
recognition models.</p></li>
<li><p><strong>Time-series data</strong>: Data collected over time that
represents a sequence of observations or events. An example is
historical stock price data.</p></li>
<li><p><strong>Graph data</strong>: Data representing a network or graph
structure, such as social networks or road networks. An example is a
graph that represents user connections in a social network.</p></li>
<li><p><strong>Set-valued data</strong>: Unstructured data in the form
of collections of features or input vectors. An example is point clouds
obtained from LiDAR sensors.</p></li>
</ul>
<h3 id="components-of-the-ml-pipeline">Components of the ML
Pipeline</h3>
<p>An <em>ML pipeline</em> is a series of interconnected steps in
developing a machine learning model, from training it on data to
deploying it in the real world. Next, we will examine these steps in
turn.</p>
<p><strong>Data collection.</strong> The first step in building an ML
model is data collection. Data can be collected in various ways, such as
by purchasing datasets from owners of data or scraping data from the
web. The foundation of any ML model is the dataset used to train it: the
quality and quantity of data are essential for accurate predictions and
performance.</p>
<p><strong>Selecting features and labels.</strong> After the data is
collected, developers of ML models must choose what they want the model
to do and what information to use. In ML, a <em>feature</em> is a
specific and measurable part of the data used to make predictions or
classifications. Most ML models focus on prediction. When predicting the
price of a house, features might include the number of bedrooms, square
footage, and the age of the house. Part of creating an ML model is
selecting, transforming, or creating the most relevant features for the
problem. The quality and type of features can significantly impact the
model’s performance, making it more or less accurate and efficient.</p>
<p><strong>ML aims to predict labels.</strong> A <em>label</em> (or a
<em>target</em>) is the value we want to predict or estimate using the
features. Labels in training data are only present in supervised ML
tasks, discussed later in this section. Some models use a sample with
correct labels to teach the model the output for a given set of input
features: a model could use historical data on housing prices to learn
how prices are related to features like square footage. However, other
(unsupervised) ML models learn to make predictions using unlabelled
input data—without knowing the correct answers—by identifying patterns
instead.</p>
<p><strong>Choosing an ML architecture.</strong> After ML model
developers have collected the data and chosen a task, they can process
it. An ML <em>architecture</em> refers to a model’s overall structure
and design. It can include the type and configuration of the algorithm
used and the arrangement of input and output layers. The architecture of
an ML model shapes how it learns from data, identifies patterns, and
makes predictions or decisions.</p>
<p><strong>ML models have parameters.</strong> Within an architecture,
<em>parameters</em> are adjustable values within the model that
influence its performance. In the house pricing example, parameters
might include the weights assigned to different features of a house,
like its size or location. During training, the model adjusts these
weights, or parameters, to minimize the difference between its predicted
house prices and the actual prices. The optimal set of parameters
enables the model to make the best possible predictions for unseen data,
generalizing from the training dataset.</p>
<p><strong>Training and using the ML model.</strong> Once developers
have built the model and collected all necessary data, they can begin
training and applying it. ML model <em>training</em> is adjusting a
model’s parameters based on a dataset, enabling it to recognize patterns
and make predictions. During training, the model learns from the
provided data and modifies its parameters to minimize errors.</p>
<p><strong>Model performance can be evaluated</strong> Model
<em>evaluation</em> measures the performance of the trained model by
testing it on data the model has never encountered before. Evaluating
the model on unseen data helps assess its generalizability and
suitability for the intended problem. We may try to predict housing
prices for a new country beyond the original ML model’s original
training data.</p>
<p><strong>Once ready, models are deployed.</strong> Finally, once the
model is trained and evaluated, it can be deployed in real-world
applications. ML <em>deployment</em> involves integrating the model into
a larger system, using it, and then maintaining or updating it as
needed.</p>
<h3 id="evaluating-ml-models">Evaluating ML Models</h3>
<p>Evaluation is a crucial step in model development. When developing a
machine learning model, it is essential to understand its performance.
Evaluation—the process of measuring the performance of a trained model
on new, unseen data—provides insight into how well the model has
learned. We can use different metrics to understand a model’s strengths,
weaknesses, and potential for real-world applications. These
quantitative performance measures are part of a broader context of goals
and values that inform how we can assess the quality of a model.</p>
<p><strong><em>Metrics</em></strong></p>
<p><strong>Accuracy is a measure of the overall performance of a
classification model.</strong> Accuracy is defined as the percentage of
correct predictions: <span class="math display">$$\text{Accuracy} =
\frac{\# \text{ of correct predictions}}{\# \text{ of total
predictions}}.$$</span> Accuracy can be misleading if there is an
imbalance in the number of examples of each class. For instance, if 95%
of emails received are not spam, a classifier assigning all emails to
the “not spam” category could achieve 95% accuracy. Accuracy applies
when there is a well-defined sense of right and wrong. Regression models
focus on minimizing the error in their predictions.</p>
<p><strong>Confusion matrices summarize the performance of
classification algorithms.</strong> A confusion matrix is an evaluative
tool for displaying different prediction errors. It is a table that
compares a model’s predicted values with the actual values. For example,
the performance of a binary classifier can be represented by a <span
class="math inline">2 × 2</span> confusion matrix, as shown in Figure <a
href="#fig:confusion-matrix" data-reference-type="ref"
data-reference="fig:confusion-matrix">4</a>. In this context, when
making predictions, there are four possible outcomes:</p>
<ol>
<li><p><strong>True positive (TP)</strong>: A true positive is a correct
prediction of the positive class.</p></li>
<li><p><strong>False positive (FP)</strong>: A false positive is an
incorrect prediction of the positive class, predicting positive instead
of negative.</p></li>
<li><p><strong>True negative (TN)</strong>: A true negative is a correct
prediction of the negative class.</p></li>
<li><p><strong>False negative (FN)</strong>: A false negative is an
incorrect prediction of the negative class, predicting negative instead
of positive.</p></li>
</ol>
<figure id="fig:confusion-matrix">
<img src="images/ML_background/Predicted-actual-red-green.png" />
<figcaption>Confusion matrix</figcaption>
</figure>
<p>Since each prediction must be in one of these categories, the number
of total predictions will be the sum of the number of predictions in
each category. The number of correct predictions will be the sum of true
positives and true negatives. Therefore, <span
class="math display">$$\text{Accuracy} = \frac{\text{TP} +
\text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}$$</span></p>
<p><strong>False positives vs. false negatives.</strong> The impact of
false positives and false negatives can vary greatly depending on the
setting. Which metric to choose depends on the specific context and the
error types one most wants to avoid. In cancer detection, while a false
positive (incorrectly identifying cancer in a cancer-free patient) may
cause emotional distress, unnecessary further testing, and potentially
invasive procedures for the patient, a false negative can be much more
dangerous: it may delay diagnosis and treatment that allows cancer to
progress, reducing the patient’s chances of survival. By contrast, an
autonomous vehicle with a water sensor that senses roads are wet when
they are dry (predicting false positives) might slow down and drive more
cautiously, causing delays and inconvenience, but one that senses the
road is dry when it is wet (false negatives) might end up in serious
road accidents and cause fatalities.<br />
While accuracy assigns equal cost to false positives and false
negatives, other metrics isolate one or weigh the two differently and
might be more appropriate in some settings. <em>Precision</em> and
<em>recall</em> are two standard metrics that measure the extent of the
error attributable to false positives and false negatives,
respectively.<br />
<em>Precision measures the correctness of a model’s positive
predictions.</em> This metric represents the fraction of positive
predictions that are actually correct. It is calculated as <span
class="math inline">$\frac{\text{TP}}{\text{TP} + \text{FP}}$</span>,
dividing true positives (hits) by the sum of true positives and false
positives. High precision implies that when a model predicts a positive
class, it is usually correct—but it might incorrectly classify many
positives as negatives as well. Precision is like the model’s aim: when
the system says it hit, how often is it right?<br />
<em>Recall measures a model’s breadth.</em> On the other hand, recall
measures how good a model is at finding all of the positive examples
available. It is like the model’s net: how many real positives does it
catch? It is calculated as <span
class="math inline">$\frac{\text{TP}}{\text{TP}+\text{FN}}$</span>,
signifying the fraction of real positives that the model successfully
detected. High recall means a model is good at recognizing or
“recalling” positive instances, but not necessarily that these
predictions are accurate. Therefore, a model with high recall may
incorrectly classify many negatives as positives.<br />
In simple terms, precision is about a model being right when it makes a
guess, and recall is about the model finding as many of the right
answers as possible. Together, these two metrics provide a way to
quantify how accurately and effectively a model can detect positive
examples. Moreover, there is a trade-off between precision and recall:
for a given model, increasing precision will necessarily decrease recall
and vice versa.<br />
</p>
<figure id="fig:precision-recall">
<img src="images/ML_background/true-false-positives-green-purple.png" />
<figcaption>Visualization of precision and recall - <span
class="citation" data-cites="wikipedia-precision"></span></figcaption>
</figure>
<p><strong>AUROC scores measure a model’s discernment.</strong> The
AUROC (Area Under the Receiver Operating Characteristic) score measures
how well a classification model can distinguish between different
classes. The ROC curve shows the performance of a classification model
by plotting the rate of true positives against false positives as
thresholds in a model are changed. AUROC scores range from zero to one,
where a score of 50% indicates random-chance performance and 100%
indicates perfect performance. To determine whether examples are
positive (belong to a certain class) or negative (do not belong to a
certain class), a classification model will assign a score to each
example and compare that score to a threshold or benchmark value. We can
interpret the AUROC as the probability that a positive example scores
higher than a negative example.<br />
</p>
<figure id="fig:AUROC">
<embed src="images/ML_background/roc_curve.pdf" />
<figcaption>The AUROC score increases as it moves in the northwest
direction. <span class="citation"
data-cites="wikipedia-roccurve"></span></figcaption>
</figure>
<p>Since it considers performance at all possible decision thresholds,
the AUROC is useful for comparing the performance of different
classifiers. The AUROC is also helpful in cases of imbalanced data, as
it does not depend on the ratio of positive to negative examples.</p>
<p><strong>Mean squared error (MSE) quantifies how “wrong” a model’s
predictions are.</strong> Mean squared error is a valuable and popular
metric of prediction error. It is found by taking the average of the
squared differences between the model’s predictions and the labels,
thereby ensuring that positive and negative deviations from the truth
are penalized the same and that larger mistakes are penalized heavily.
The MSE is the most popular loss function for regression problems.</p>
<p><strong>Reasonably vs. reliably solved.</strong> The distinction
between <em>reasonable</em> and <em>reliable</em> solutions can be
instrumental in developing a machine learning model, evaluating its
performance, and thinking about tradeoffs between goals. A task is
reasonably solved if a model performs well enough to be helpful in
practice, but it may still have consistent limitations or make errors. A
task is reliably solved if a model achieves sufficiently high accuracy
and consistency for safety-critical applications. While models that
reasonably solve problems may be sufficient in some settings, they may
cause harm in others. Chatbots currently give reasonable results, which
is frustrating but essentially harmless. However, if autonomous vehicles
show reasonable but not reliable results, people’s lives are at
stake.</p>
<p><strong>Goals and Tradeoffs</strong> Above and beyond quantitative
performance measures are multiple goals and values that influence how we
can assess the quality of a machine learning model. These goals—and the
tradeoffs that often arise between them—shape how models are judged and
developed.<br />
One such goal is <em>predictive power</em>, which measures the amount of
error in predictions. Inference <em>time</em> (or <em>latency</em>)
measures how quickly a machine learning model can produce results from
input data—in many applications, such as self-driving cars, prediction
speed is crucial. <em>Transparency</em> refers to the interpretability
of a machine learning model’s inner workings and how well humans can
understand its decision-making process. <em>Reliability</em> assesses
the consistency of a model’s performance over time and in varying
conditions. <em>Scalability</em> is the capacity of a model to maintain
or improve its performance as a key variable—compute, parameter count,
dataset size, and so on—–scales.<br />
Sometimes, these goals are in opposition, and improvements in one area
can come at the cost of declines in others. Therefore, developing a
machine learning model requires careful consideration of multiple
competing goals.</p>
<h2 id="types-of-machine-learning">Types of Machine Learning</h2>
<p><strong>One key dimension along which ML approaches vary is the
degree of supervision.</strong> We can divide ML approaches into groups
based on how they use the training data and what they produce as an
output. In ML, <em>supervision</em> is the process of guiding a model’s
learning with some kind of label. The model uses this label as a kind of
<em>ground truth</em> or <em>gold standard</em>: a signal that can be
trusted as accurate and used to supervise the model to achieve the
intended results better. Labels can allow the model to capture
relationships between inputs and their corresponding outputs more
effectively. Supervision is often vital to help models learn patterns
and predict new, unseen data accurately.<br />
There are distinct approaches in machine learning for dealing with
different amounts of supervision. Here, we will explore three key
approaches: supervised, unsupervised, and reinforcement learning. We
will also discuss deep learning, a set of techniques that can be applied
in any of these settings.<br />
</p>
<figure id="fig:learning-paradigm">
<img src="images/ML_background/learning_paradigms.png" />
<figcaption>Learning paradigms in machine learning </figcaption>
</figure>
<p><strong><em>Supervised Learning</em></strong></p>
<p><strong>Supervised learning is learning from labeled data.</strong>
Supervised learning is a type of machine learning that uses a labeled
dataset to learn the relationship between input data and output labels.
These labels are almost always human-generated: people will go through
examples in a dataset and give each one a label. They might be shown
pictures of dogs and asked to label the breed. The training process
involves iteratively adjusting the model’s parameters to minimize the
difference between predicted outputs and the true output labels in the
training data. Once trained, the model can predict new, unlabeled
data.</p>
<p><strong>Examples of supervised learning.</strong> Some examples of
these labeled inputs and outputs include mapping a photo of a plant to
its species, a song to its genre, or an email to either “spam” or “not
spam.” A computer can use a set of dog pictures labeled by humans to
predict the breed of any dog in any given image. Supervised learning is
analogous to a practice book, which offers a student a series of
questions (inputs) and then provides the answers (outputs) at the end of
the book. This book can help the student (like an ML model) find the
correct answers when given new questions. Without instruction or
guidance, the student must learn to answer questions correctly by
reviewing the problems and checking their answers. Over time, they learn
and improve through this checking process.</p>
<p><strong>Advantages and disadvantages.</strong> Supervised learning
can excel in classification and regression tasks. Furthermore, it can
result in high accuracy and reliable predictions when given large,
labeled datasets with well-defined features and target variables.
However, this method performs poorly on more loosely defined tasks, such
as generating poems or new images. Supervision may also require manual
labeling for the training process, which can be prohibitively
time-consuming and costly. Critically, supervised learning is
bottlenecked by the amount of labels, which can often result in less
data available than when using unsupervised learning.</p>
<p><strong><em>Unsupervised Learning</em></strong></p>
<p><strong>Unsupervised learning is learning from unlabeled
data.</strong> Unsupervised learning involves training a model on a
dataset without specific output labels. Instead of matching its inputs
to the correct labels, the model must identify patterns within the data
to help it understand the underlying relationships between the
variables. As no labels are provided, a model is left to its own devices
to discover valuable patterns in the data. In some cases, a model
leverages these patterns to generate supervisory signals, guiding its
own training. For this reason, unsupervised learning can also be called
self-supervised learning.</p>
<p><strong>Examples of unsupervised learning.</strong> Language models
use unsupervised learning to learn patterns in language using large
datasets of unlabeled text. LLMs often learn to predict the next word in
a sentence, which enables the models to understand context and language
structure without explicit labels like word definitions and grammar
instructions. After a model trains on this task, it can apply what it
learned to downstream tasks like answering questions or summarizing
texts. In image inpainting, models are trained to predict portions of
images in which information is missing or covered up. This allows the
model to learn visual features and relationships between objects that
can be used to generate new images.</p>
<figure id="fig:inpainting">
<img src="images/ML_background/image32.png" />
<figcaption>Image inpainting - <span class="citation"
data-cites="elharrouss2019"></span></figcaption>
</figure>
<p><strong>ML models exist on a spectrum of supervision.</strong>
Unsupervised and supervised learning are valuable concepts for thinking
about ML models, not a dichotomy with a clear dividing line. Therefore,
ML models are on a continuum of supervision, from datasets with clear
labels for every data point on one extreme to datasets with no labels on
the other. In between lies partial or weak supervision, which provides
incomplete or noisy labels such as hashtags loosely describing features
of images. This is analogous to a practice book where some solution
pages are excessively brief, have errors, or are omitted entirely.</p>
<p><strong>We can reframe many tasks into different types of
ML.</strong> Anomaly detection is typically framed as an unsupervised
task that identifies unusual data points without labels. However, it can
be refashioned as a supervised classification problem, such as labeling
financial transactions as “fraudulent” or “not fraudulent.” Similarly,
while stock price prediction is usually approached as a supervised
regression task, it could be reframed as a classification task in which
a model predicts whether a stock price will increase or decrease. The
choice in framing depends on the task’s specific requirements, the data
available, and which frame gives a more useful model. Ultimately, this
flexibility allows us to better cater to our goals and problems.</p>
<p><strong><em>Reinforcement Learning</em></strong></p>
<p><strong>Reinforcement learning (RL) is learning from agent-gathered
data.</strong> Reinforcement learning focuses on training artificial
agents to make decisions and improve their performance based on
responses from their environment. It assumes that tasks can be modeled
as goals to be achieved by an agent maximizing rewards from its
environment. RL is distinctive since it does not require pre-collected
data, as the agent can begin with no information and interact with its
environment to learn new things and acquire new data. This approach will
be discussed in more depth later in the Section .</p>
<p><strong>Examples of RL.</strong> RL can help robots learn to navigate
an unknown environment by taking actions and receiving feedback in the
form of rewards or penalties based on performance. Through trial and
error, agents learn to make better decisions and maximize rewards by
adjusting their actions or improving their model of the environment. It
refines its strategy based on the consequences of its activities. RL
enables agents to learn techniques and decision-making skills through
interaction with their environment, which can adapt to dynamic and
uncertain situations. However, it requires a well-designed reward
function and can be computationally expensive, especially for complex
environments with many possibilities for states and actions.</p>
<p><strong><em>Deep Learning</em></strong></p>
<p><strong>Deep learning (DL) is a set of techniques that can be used in
many learning settings.</strong> Deep learning uses neural networks with
many layers to create models that can learn from large datasets.
<em>Neural networks</em> are the building blocks of deep learning models
and use layers of interconnected nodes to transform inputs into outputs.
The structure and function of biological neurons loosely inspired their
design. Deep learning is not a new distinct learning type but rather a
computational approach that can accomplish any of the three types of
learning discussed above. It is most applicable to unsupervised learning
tasks as it can perform well without any labels; for instance, a deep
neural network trained for object recognition in images can learn to
identify patterns in the raw pixel data.</p>
<p><strong>Advantages and challenges in DL.</strong> Deep learning
excels in handling high-dimensional and complex data, providing critical
capabilities in image recognition, natural language processing, and
generative modeling. In ML, <em>dimensionality</em> denotes the number
of features or variables in the data, each representing a unique
dimension. High-dimensional data has many features, as in image
recognition, where each pixel can be a feature. However, deep learning
also requires vast data and substantial computational power. Moreover,
the models can be challenging to interpret.</p>
<h3 id="conclusion">Conclusion</h3>
<p>AI is one of the most impactful and rapidly developing fields of
computer science. Artificial intelligence involves developing computer
systems that can perform tasks that typically require human
intelligence, from visual perception to decision-making.<br />
Machine learning is an approach to AI that involves developing models
that can learn from data to perform tasks without being explicitly
programmed. A robust approach to understanding any machine learning
model is breaking it down into its fundamental components: the task, the
input data, the output, and the type of machine learning it uses.
Different approaches to ML offer various ways to tackle complex tasks
and solve real-world problems. Deep learning is a powerful and popular
method that uses many-layered neural networks to identify intricate
patterns in large datasets. The following section will delve deeper into
deep learning and its applications in artificial intelligence.</p>
<h1 id="deep-learning">Deep Learning</h1>
<h3 id="introduction-1">Introduction</h3>
<p>In this section, we present the fundamentals of deep learning (DL), a
branch of machine learning that uses neural networks to learn from data
and perform complex tasks <span class="citation"
data-cites="LeCun2015"></span>. First, we will consider the essential
building blocks of deep learning models and explore how they learn.
Then, we will discuss the history of critical architectures and see how
the field developed over time. Finally, we will explore how deep
learning is reshaping our world by reviewing a few of its groundbreaking
applications.</p>
<h3 id="why-deep-learning-matters">Why Deep Learning Matters</h3>
<p>Deep learning is a remarkably useful, powerful, and scalable
technology that has been the primary source of progress in machine
learning since the early 2010s. Deep learning methods have dramatically
advanced the state-of-the-art in computer vision, speech recognition,
natural language processing, drug discovery, and many other areas.</p>
<p><strong>Performance.</strong> Some deep learning models have
demonstrated better-than-human performance in specific tasks, though
unreliably. These models have excelled in tasks such as complex image
recognition and outmatched world experts in chess, Go, and challenging
video games such as StarCraft. However, their victories are far from
comprehensive or absolute. Model performance is variable, and deep
learning models sometimes make errors or misclassifications obvious to a
human observer. Therefore, despite their impressive accomplishments in
specific tasks, deep learning models have yet to consistently surpass
human intelligence or capabilities across all tasks or domains.</p>
<p><strong>Real-world usefulness.</strong> Beyond games and academia,
deep learning techniques have proven useful in a wide variety of
real-world applications. They are increasingly integrated into everyday
life, from healthcare and social media to chatbots and autonomous
vehicles. Deep learning can generate product recommendations, predict
energy load in a power grid, fly a drone, or create original works of
art.</p>
<p><strong>Scalability.</strong> Deep learning models are highly
scalable and positioned to continue to advance in capability as data,
hardware, and training techniques progress. A key strength of these
models is their ability to process and learn from increasingly large
amounts of data. Many traditional machine learning algorithms’
performance gains taper off with additional data; by contrast, the
performance of deep learning models improves faster and longer.</p>
<h3 id="defining-deep-learning">Defining Deep Learning</h3>
<p>Deep learning is a type of machine learning that uses neural networks
with many layers to learn and extract useful patterns from large
datasets. It is characterized by its ability to learn hierarchical
representations of the world.</p>
<p><strong>ML/DL distinction.</strong> As we saw in the previous
section, ML is the field of study that aims to give computers the
ability to learn without explicitly being programmed. DL is a highly
adaptable and remarkably effective approach to ML. Deep learning
techniques are employed in and represent the cutting edge of many areas
of machine learning, including supervised, unsupervised, and
reinforcement learning.</p>
<figure id="fig:venn-definitions">
<img src="images/ML_background/AI_ML_DL_Venn.png" />
<figcaption>Relationship between AI, machine learning, and deep
learning</figcaption>
</figure>
<p><strong>Automatically learned representations.</strong>
Representations are, in general, stand-ins or substitutes for the
objects they represent. For example, the word “airplane” is a simple
representation of a complex object. Similarly, ML systems use
representations of data to complete tasks. Ideally, these
representations are distillations that capture all essential elements or
features of the data without extraneous information.<br />
While many traditional ML algorithms build representations from features
hand-picked and engineered by humans, features are learned in deep
learning. The primary objective of deep learning is to enable models to
learn useful features and meaningful representations from data. These
representations, which capture the underlying patterns and structure of
the data, form the base on which a model solves problems. Therefore,
model performance is directly related to representation quality. The
more insightful and informative a model’s representations are, the
better it can complete tasks. Thus, the key to deep learning is learning
good representations.</p>
<p><strong>Hierarchical representations.</strong> Deep learning models
represent the world as a nested hierarchy of concepts or features. In
this hierarchy, features build on one another to capture progressively
more abstract features. Higher-level representations are deﬁned by and
computed in terms of simpler ones. In object detection, for example, a
model may learn first to recognize edges, then corners and contours, and
finally parts of objects. Each set of features builds upon those that
precede it:</p>
<ol>
<li><p>Edges are (usually) readily apparent in raw pixel data.</p></li>
<li><p>Corners and contours are collections of edges.</p></li>
<li><p>Object parts are edges, corners, and contours.</p></li>
<li><p>Objects are collections of object parts.</p></li>
</ol>
<p>This is analogous to how visual information is processed in the human
brain. Edge detection is done in early visual areas like the primary
visual cortex, more complex shape detection in temporal regions, and a
complete visual scene is assembled in the brain’s frontal regions.
Hierarchical representations enable deep neural networks to learn
abstract concepts and develop sophisticated models of the world. They
are essential to deep learning and why it is so powerful.</p>
<h3 id="what-deep-learning-models-do">What Deep Learning Models Do</h3>
<p><strong>Deep learning models learn complicated relationships in
data.</strong> In general, machine learning models can be thought of as
a way of transforming any input into a meaningful output. Deep learning
models are an especially useful kind of machine learning model that can
capture an extensive family of relationships between input and
output.</p>
<p><strong>Function approximation.</strong> In theory, neural
networks—the backbone of deep learning models—can learn almost any
function that maps inputs to outputs, given enough data and a suitable
network architecture. Under some strong assumptions, a sufficiently
large neural network can approximate any continuous function (like <span
class="math inline"><em>y</em> = <em>a</em><em>x</em><sup>2</sup> + <em>b</em><em>x</em> + <em>c</em></span>)
with a combination of weights and biases. For this reason, neural
networks are sometimes called “universal function approximators.” While
largely theoretical, this idea provides an intuition for how deep
learning models achieve such immense flexibility and utility in their
tasks.</p>
<p><strong>Challenges and limitations.</strong> Deep learning models do
not have unlimited capabilities. Although neural networks are very
powerful, they are not the best suited to all tasks. Like any other
model, they are subject to tradeoffs, limitations, and real-world
constraints. In addition, the performance of deep neural networks often
depends on the quality and quantity of data available to train the
model, the algorithms and architectures used, and the amount of
computational power available.</p>
<h3 id="summary-1">Summary</h3>
<p>Deep learning is an approach to machine learning that leverages
multi-layer neural networks to achieve impressive performance. Deep
learning models can capture a remarkable family of relationships between
inputs and outputs by developing hierarchical representations. They have
a number of advantages over traditional ML models, including scaling
more effectively, learning more sophisticated relationships with less
human input, and adapting more readily to different tasks with
specialized components. Next, we will make our understanding more
concrete by looking more closely at exactly what these components are
and how they operate.</p>
<h2 id="model-building-blocks">Model Building Blocks</h2>
<p>In this section, we will explore some of the foundational building
blocks of deep learning models. We will begin by defining what a neural
network is and then discuss the fundamental elements of neural networks
through the example of multi-layer perceptrons (MLPs), one of the most
basic and common types of deep learning architecture. Then, we will
cover a few more technical concepts, including activation functions,
residual connections, convolution, and self-attention. Finally, we will
see how these concepts come together in the <em>Transformer</em>,
another type of deep learning architecture.</p>
<p><strong>Neural networks.</strong> Neural networks are a type of
machine learning algorithm composed of layers of interconnected nodes or
neurons. They are loosely inspired by the structure and function of the
human brain. <em>Neurons</em> are the basic computational units of
neural networks. In essence, a neuron is a function that takes in a
weighted sum of its inputs and applies an <em>activation function</em>
to transform it, generating an output signal that is passed along to
other neurons.</p>
<p><strong>Biological inspiration.</strong> The “artificial neurons” in
neural networks were named after their biological counterparts. Both
artificial and biological neurons operate on the same basic principle.
They receive inputs from multiple sources, process them by performing a
computation, and produce outputs that depend on the inputs—in the case
of biological neurons, firing only when a certain threshold is exceeded.
However, while biological neurons are intricate physical structures with
many components and interacting cells, artificial neurons are simplified
computational units designed to mimic a few of their
characteristics.</p>
<figure id="fig:neurons">
<img src="images/ML_background/artificial biological neuron.png" />
<figcaption>Comparison of biological and artificial neurons. - <span
class="citation"
data-cites="wikineuron wikiperceptron"></span></figcaption>
</figure>
<p><strong>Building blocks.</strong> Neural networks are made of simple
building blocks that can produce complex abilities when combined at
scale. Despite their simplicity, the resulting network can display
remarkable behaviors when thousands—or even millions—of artificial
neurons are joined together. Neural networks consist of densely
connected layers of neurons, each contributing a tiny fraction to the
overall processing power of the network. Within this basic blueprint,
there is much room for variation; for instance, neurons can be connected
in many ways and employ various activation functions. These network
structure and design differences shape what and how a model can
learn.</p>
<h3 id="multi-layer-perceptrons">Multi-Layer Perceptrons</h3>
<p>Multi-layer perceptrons (MLPs) are a foundational neural network
architecture consisting of multiple layers of nodes, each performing a
weighted sum of its inputs and passing the result through an activation
function. They belong to a class of architectures known as “feedforward”
neural networks, where information flows in only one direction, from one
layer to the next. MLPs are composed of at least three layers: an
<em>input layer</em>, one or more <em>hidden layers</em>, and an
<em>output layer</em>.</p>
<p><strong>The input layer serves as the entry point for data into a
network.</strong> The input layer consists of nodes that encode
information from input data to pass on to the next layer. Unlike in
other layers, the nodes do not perform any computation. Instead, each
node in the input layer captures some small raw input data and directly
relays this information to the nodes in the subsequent layer. As with
other ML systems, input data for neural networks comes in many forms.
For illustration, we will focus on just one: image data. Specifically,
we will draw from the classic example of digit recognition with
MNIST.<br />
The MNIST (Modified National Institute of Standards and Technology)
database is a large collection of images of handwritten digits, each
with dimensions 28 <span class="math inline">×</span> 28. Consider a
neural network trained to classify these images. The input layer of this
network consists of 784 nodes, each corresponding to the grayscale value
of a pixel in a given image.<br />
</p>
<figure id="fig:pixel-mapping">
<img src="images/ML_background/image54.png" />
<figcaption>Mapping from pixels of an example image to neurons in the
input layer of a network - <span class="citation"
data-cites="hula2018"></span></figcaption>
</figure>
<p><strong>The output layer is the final layer of a neural
network.</strong> The output layer contains neurons representing the
results of the computations performed within the network. Like inputs,
neural network outputs come in many forms, such as predictions or
classifications. In the case of MNIST (a classification task), the
output is categorical, predicting the digit represented by a particular
image.<br />
For classification tasks, the number of neurons in the output layer is
equal to the number of possible classes. In the MNIST example, the
output layer will have ten neurons, one for each of the ten classes
(digits 0-9). The value of each neuron represents the predicted
probability that an example belongs to that class. The output value of
the network is the class of the output neuron with the highest
value.</p>
<p><strong>Hidden layers are the intermediate layers between the input
and output layers.</strong> Each hidden layer is a collection of neurons
that receive outputs from the previous layer, perform a computation, and
pass the results to the next layer. These are “hidden” because they are
internal to the network and not directly observable from its inputs or
outputs. These layers are where representations of features are
learned.</p>
<p><strong>Weights represent the strength of the connection between two
neurons.</strong> Every connection is associated with a weight that
determines how much the input signal from a given neuron will influence
the output of the next neuron. This value represents the importance or
contribution of the first neuron to the second. The larger the
magnitude, the greater the influence. Neural networks learn by modifying
the values of their weights, which we will explore shortly.</p>
<p><strong>Biases are additional learned parameters used to adjust
neuron outputs.</strong> Every neuron has a bias that helps control its
output. This bias acts as a constant term that shifts the activation
function along the input axis, allowing the neuron to learn more
complex, flexible decision boundaries. Similar to the constant <span
class="math inline"><em>b</em></span> of a linear equation <span
class="math inline"><em>y</em> = <em>m</em><em>x</em> + <em>b</em></span>,
the bias allows shifting the output of each layer. In doing so, biases
increase the range of the representations a neural network can
learn.</p>
<p><strong>Activation functions control the output or “activation” of
neurons.</strong> Activation functions are nonlinear functions applied
to each neuron’s weighted input sum within a neural network layer. They
are mathematical equations that control the output signal of the
neurons, effectively determining the degree to which each neuron
“fires.”<br />
Each neuron in a network takes some inputs, multiplies them by weights,
adds a bias, and applies an activation function. The activation function
transforms this weighted input sum into an output signal. For many
activation functions, the more input a neuron receives, the more it
activates, translating to a larger output signal.</p>
<p><strong>Activation functions allow for intricate
representations.</strong> Without activation functions, neural networks
would operate similarly to linear regression models, with added layers
failing to contribute any complexity to the model’s representations.
Activation functions enable neural networks to learn and express more
sophisticated patterns and relationships by managing the output of
neurons.</p>
<p><strong>Single-layer and multi-layer networks.</strong> Putting all
of these elements together, single-layer neural networks are the
simplest form of neural network. They have only one hidden layer,
comprising an input layer, an output layer, and a hidden layer.
Multi-layer neural networks add more hidden layers in the middle. These
networks are the basis of deep learning models.</p>
<p><strong>Multi-layer neural networks are required for hierarchical
representations.</strong> While single-layer networks can learn many
things, they cannot learn the hierarchical representations that form the
cornerstone of deep learning. Layers provide the scaffolding of the
pyramid. No layers means no hierarchy. As the features learned in each
layer build on those of previous layers, additional hidden layers enable
a neural network to learn more sophisticated and powerful
representations. Simply put, more layers capture more features at more
levels of abstraction.<br />
</p>
<figure id="fig:multilayer-nn">
<img src="images/ML_background/image59.png" />
<figcaption>A multi-layer neural network</figcaption>
</figure>
<p><strong>Neural networks as matrix multiplication.</strong> If we put
aside the intuitive diagrammatic representation, a neural network is a
mathematical function that takes in a set of input values and produces a
set of output values via a series of steps. All the neurons in a layer
can be represented as a list or <em>vector</em> of activations. In any
layer, this activation vector is multiplied with an input and then
transformed by applying an <em>element-wise nonlinear function</em> to
the result. This is the layer’s output, which becomes the input to the
next layer. The network as a whole is the composition of all of its
layers.</p>
<p><strong>A toy example.</strong> Consider an MLP with two hidden
layers, activation function g, and an input <span
class="math inline"><em>x</em></span>. This network could be expressed
as <span
class="math inline"><em>W</em><sub>3</sub><em>g</em>(<em>W</em><sub>2</sub><em>g</em>(<em>W</em><sub>1</sub><em>x</em>))</span>:</p>
<ol>
<li><p>In the input layer, the input vector <span
class="math inline"><em>x</em></span> is passed on.</p></li>
<li><p>In the first hidden layer,</p>
<ol>
<li><p>the input vector <span class="math inline"><em>x</em></span> is
multiplied by the weight vector, <span
class="math inline"><em>W</em><sub>1</sub></span>, yielding <span
class="math inline"><em>W</em><sub>1</sub><em>x</em></span>,</p></li>
<li><p>then the activation function <span
class="math inline"><em>g</em></span> is applied, yielding <span
class="math inline"><em>g</em>(<em>W</em><sub>1</sub><em>x</em>)</span>,</p></li>
<li><p>which is passed on to the next layer.</p></li>
</ol></li>
<li><p>In the second hidden layer,</p>
<ol>
<li><p>the vector passed to the layer is multiplied by the weight
vector, <span class="math inline"><em>W</em><sub>2</sub></span>,
yielding <span
class="math inline"><em>W</em><sub>2</sub><em>g</em>(<em>W</em><sub>1</sub><em>x</em>)</span>,</p></li>
<li><p>then the activation function <span
class="math inline"><em>g</em></span> is applied, yielding <span
class="math inline"><em>g</em>(<em>W</em><sub>2</sub><em>g</em>(<em>W</em><sub>1</sub><em>x</em>))</span>,</p></li>
<li><p>which is passed on to the output layer.</p></li>
</ol></li>
<li><p>In the output layer,</p>
<ol>
<li><p>the input to the layer is multiplied by the weight vector, <span
class="math inline"><em>W</em><sub>3</sub></span>, yielding <span
class="math inline"><em>W</em><sub>3</sub><em>g</em>(<em>W</em><sub>2</sub><em>g</em>(<em>W</em><sub>1</sub><em>x</em>))</span>,</p></li>
<li><p>which is the output vector.</p></li>
</ol></li>
</ol>
<p>This process is mathematically equivalent to matrix multiplication.
This trait has significant implications for the computational properties
of neural networks. Since matrix multiplication lends itself to being
run in parallel, this equivalence allows specialized, more efficient
processors such as GPUs to be used during training.</p>
<p><strong>Summary.</strong> MLPs are models of a versatile and popular
type of neural network that has been successfully applied to many tasks.
They are often a key component in many larger, more sophisticated deep
learning architectures. However, MLPs have limitations and are only
sometimes the best-suited approach to a task. Some of the building
blocks we will see later on address the shortcomings of MLPs and
critical issues that can arise in deep learning more generally. Before
that, we will look at activation functions—the mechanisms that control
how and when information is transmitted between neurons—in more
detail.</p>
<h3 id="key-activation-functions">Key Activation Functions</h3>
<p>Activation functions are a vital component of neural networks. They
introduce nonlinearity, which allows the network to model intricate
patterns and relationships in data. By defining the activations of each
neuron within the network, activation functions act as informational
gatekeepers that control data transfer from one layer of the network to
the next.</p>
<p><strong>Using activation functions.</strong> There are many
activation functions, each with unique properties and applications. Even
within a single network, different layers may use other activation
functions. The selection and placement of activation functions can
significantly change the network’s capability and performance. In most
cases, the same activation will be applied in all the hidden layers
within a network.<br />
While many possible activation functions exist, only a handful are
commonly used in practice. Here, we highlight four that are of
particular practical or historical significance. Although there are many
other functions and variations of each, these four—ReLU, GELU <span
class="citation" data-cites="hendrycks2023gaussian"></span>, sigmoid,
and softmax—have been highly influential in developing and applying deep
learning. The Transformer architecture, which we will describe later,
uses GELU and softmax functions. Historically, many architectures used
ReLUs and sigmoids. Together, these functions illustrate the essential
characteristics of the properties and uses of activation functions in
neural networks.</p>
<p><strong>Rectified Linear Unit (ReLU).</strong> The rectified linear
unit (ReLU) function is a piecewise linear function that returns the
input value for positive inputs and zero for negative inputs <span
class="citation" data-cites="Nair2010"></span>. It is the identity
function <span
class="math inline">(<em>f</em>(<em>x</em>)=<em>x</em>)</span> for
positive inputs and zero otherwise. This means that if a neuron’s
weighted input sum is positive, it will be passed directly to the
following layer without any modification. However, no signal will be
passed on if the sum is negative. Due to its piecewise nature, the graph
of the ReLU function takes the form of a distinctive “kinked” line. Due
to its computational efficiency, the ReLU function was widely used and
played a critical role in developing more sophisticated deep learning
architectures.<br />
</p>
<figure id="fig:relu-function">
<embed src="images/ML_background/relu.pdf" />
<figcaption>The ReLU activation function, <span
class="math inline">ReLU(<em>x</em>) = max {0, <em>x</em>}</span>,
passes on positive inputs.</figcaption>
</figure>
<p><strong>Gaussian error linear unit (GELU).</strong> The GELU
(Gaussian error linear unit) function is an upgrade of the ReLU function
that uses approximation to smooth out the non-differentiable component.
This is important for optimization. It is “Gaussian” because it
leverages the Gaussian cumulative distribution function (CDF), <span
class="math inline"><em>Φ</em>(<em>x</em>)</span>. The GELU has been
widely used in and contributed to the success of many current models,
including Transformer-based language models.</p>
<figure id="fig:gelu-function">
<embed src="images/ML_background/gelu.pdf" />
<figcaption>The GELU activation function, <span
class="math inline">GELU(<em>x</em>) = <em>x</em> ⋅ <em>Φ</em>(<em>x</em>)</span>,
smoothes out the ReLU function.</figcaption>
</figure>
<p><strong>Sigmoid.</strong> A sigmoid is a smooth, differentiable
function that maps any real-valued numerical input to a value between
zero and one. It is sometimes called a <em>squashing function</em>
because it compresses all real numbers to values in this range. When
graphed, it forms a characteristic S-shaped curve. We explored the
sigmoid function in the previous section.</p>
<figure id="fig:sigmoid-function">
<embed src="images/ML_background/sigmoid.pdf" />
<figcaption>The Sigmoid activation function, <span
class="math inline">$\sigma(x) = \frac{1}{1 + e^{-x}}$</span>, has a
characteristic S-shape.</figcaption>
</figure>
<p><strong>Softmax.</strong> Softmax is a popular activation function
due to its ability to model multi-class probabilities. Unlike other
activation functions that operate on each input individually, softmax
considers all inputs simultaneously to create a probability distribution
across many dimensions. This is useful in settings with multiple classes
or categories, such as natural language processing, where each word in a
sentence can belong to one of numerous classes.<br />
The softmax function can be considered a generalization of the sigmoid
function. While the sigmoid function maps a single input value to a
number between 0 and 1, interpreted as a binary probability of class
membership, softmax normalizes a set of real values into a probability
distribution over multiple classes. Though it is typically applied to
the output layer of neural networks for multi-class classification
tasks—an example of when different activation functions are used within
one network—softmax may also be used in intermediate layers to readjust
weights at bottleneck locations within a network.<br />
We can revisit the example of handwritten digit recognition. In this
classification task, softmax is applied in the last layer of the network
as the final activation function. It takes in a 10-dimensional vector of
the raw outputs from the network and rescales the values to generate a
probability distribution over the ten predicted classes. Each class
represents a digit from 0 to 9, and each output value represents the
probability that an input image is an instance of a given class. The
digit corresponding to the highest probability will be selected as the
network’s prediction.<br />
Now, having explored ReLU, GELU, sigmoid, and softmax, we will set aside
activation functions and turn our attention to other building blocks of
deep learning models.</p>
<h3 id="residual-connections">Residual Connections</h3>
<p><strong>Residual connections create alternative pathways in a
network, preserving information.</strong> Also known as <em>skip</em>
connections, residual connections provide a pathway for information to
bypass specific layers or groups of layers (called <em>blocks</em>) in a
neural network <span class="citation" data-cites="He2016"></span>.
Without residual connections, all information must travel sequentially
through every layer of the network, undergoing continual, significant
change as each layer receives and transforms the output of the previous
one. Residual connections allow data to skip these transformations,
preserving its original content. With residual connections, layers can
access more than just the previous layer’s representations as
information flows through and around each block in the network.
Consequently, lower-level features learned in earlier layers can
contribute more directly to the higher-level features of deeper layers,
and information can be more readily preserved.</p>
<figure id="fig:feed-forward-network">
<img src="images/ML_background/image53.png" />
<figcaption>Traditional feedforward network and a network with residual
connections.</figcaption>
</figure>
<p><strong>Residual connections facilitate learning.</strong> Residual
connections improve learning dynamics in several ways by facilitating
the flow of information during the training process. This improves
iterative and hierarchical feature representations, particularly for
deeper networks.<br />
Neural networks typically learn by decomposing data into a hierarchy of
features, where each layer learns a distinct representation. Residual
connections allow for a different kind of learning in which learned
representations are gradually refined. Each block improves upon the
representation of the previous block, but the overall meaning captured
by each layer remains consistent across successive blocks. This allows
feature maps learned in earlier layers to be reused and networks to
learn representations (such as <em>identity mappings</em>) in deeper
layers that may otherwise not be possible due to optimization
difficulties.<br />
Residual connections are general purpose, used in many different problem
settings and architectures. By facilitating the learning process and
expanding the kinds of representations networks can learn, they are a
valuable building block that can be a helpful addition to a wide variety
of networks.</p>
<h3 id="convolution">Convolution</h3>
<p>In machine learning, convolution is a process used to detect patterns
or features in input data by applying a small matrix called a
<em>filter</em> or <em>kernel</em> and looking for cross-correlations.
This process involves <em>sliding</em> the filter over the input data,
systematically comparing relevant sections using matrix multiplication
with the filter, and recording the results in a new matrix called a
<em>feature map</em>.</p>
<figure id="fig:convolution-1">
<img src="images/ML_background/convolution.jpeg" />
<figcaption>Convolution operation - <span class="citation"
data-cites="wikiconvolution"></span></figcaption>
</figure>
<p><strong>Convolutional layers.</strong> Convolutional layers are
specialized layers that perform the “convolution” operation to detect
local features in the input data. These layers commonly comprise
multiple filters, each learning a different feature. Convolution is
considered a localized process because the filter usually operates on
small, specific regions of the input data at a time (such as parts of an
image). This allows the network to recognize features regardless of
their position in the input data, making convolution well-suited for
tasks like image recognition.</p>
<p><strong>Convolutional neural networks (CNNs).</strong> Convolution
has become a key technique in modern computer vision models because it
effectively captures local features in images and can deal with
variations in their position or appearance. This helps improve the
accuracy of models for tasks like object detection or facial recognition
compared to fully connected networks. Convolutional neural networks
(CNNs) use convolution to process spatial data, such as images or
videos, by applying convolutional filters that extract local features
from the input.<br />
Convolution was instrumental in the transition of deep learning from
MLPs to more sophisticated architectures and has maintained significant
influence, especially in vision-related tasks.</p>
<h3 id="self-attention">Self-Attention</h3>
<p><strong>Self-attention can produce more coherent
representations.</strong> Self-attention encodes the relationships
between elements in a sequence to better understand and represent the
information within the sequence. In self-attention, each element attends
to every other element by determining its relative importance and
selectively focusing on the most relevant connections.<br />
</p>
<figure id="fig:relations-example">
<img src="images/ML_background/image56.png" />
<figcaption>Example of relationships captured by two different attention
heads - <span class="citation"
data-cites="Vaswani2017"></span></figcaption>
</figure>
<p>This process allows the model to capture dependencies and
relationships within the sequence, even when they are separated by long
distances. As a result, deep learning models can create a more
context-aware representation of the sequence. When summarizing a long
book, self-attention can help the model understand which parts of the
text are most relevant and central to the overall meaning, leading to a
more coherent summary.</p>
<h3 id="transformers">Transformers</h3>
<p>The Transformer is a groundbreaking deep learning model that
leverages self-attention <span class="citation"
data-cites="Vaswani2017"></span>. It is a very general and versatile
architecture that can achieve outstanding performance across many data
types. The model itself consists of a series of Transformer
blocks.<br />
</p>
<div class="wrapfigure">
<p><span>r</span><span>5.5cm</span> <img
src="images/ML_background/image64.png" style="width:5.5cm"
alt="image" /></p>
</div>
<p>A <em>Transformer block</em> primarily combines self-attention and
MLPs (as we saw earlier) with optimization techniques such as residual
connections and layer normalization.</p>
<p><strong>Large language models (LLMs).</strong> LLMs are a class of
language models with many parameters (often in the billions) trained on
vast quantities of data. These models excel in various language tasks,
including question-answering, text generation, coding, translation, and
sentiment analysis. Most LLMs, such as the Generative Pre-trained
Transformer (GPT) series, utilize Transformers because they can
effectively model long-range dependencies.</p>
<h3 id="summary-2">Summary</h3>
<p>Deep learning models are networks composed of many layers of
interconnected nodes. The structure of this network plays a vital role
in shaping how a model functions. Creating a successful model requires
carefully assembling numerous components. Different components are used
in different settings, and each building block serves a unique purpose,
contributing to a model’s overall performance and capabilities.<br />
This section discussed multi-layer perceptrons (MLPs), activation
functions, residual connections, convolution, and self-attention,
culminating with an introduction to the Transformer architecture. We saw
how MLPs, an archetypal deep learning model, paved the way for other
architectures and remain an essential component of many more
sophisticated models. Many building blocks each play a distinct role in
the structure and function of a model.<br />
Activation functions like ReLU, softmax, and GELU introduce nonlinearity
in networks, enabling models to learn complex patterns. Residual
connections facilitate the flow of information in a network, thereby
enabling the training of deeper networks. Convolution uses sliding
filters to allow models to detect local features in input data, an
especially useful capability in vision-related tasks. Self-attention
enables models to weigh the relevance of different inputs based on their
context. By leveraging these mechanisms mechanism to handle complex
dependencies in sequential data, Transformers revolutionized the field
of natural language processing (NLP).</p>
<h2 id="training-and-inference">Training and Inference</h2>
<p>Having explored the components of deep learning models, we will now
explore how the models work. First, we will briefly describe training
and inference: the two key phases of developing a deep learning model.
Next, we will examine learning mechanics and see how the training
process enables models to learn and continually refine their
representations. Then, we will discuss a few techniques and approaches
to learning and training deep learning models and consider how model
evaluation can help us understand a model’s potential for real-world
applications.<br />
<strong>Training is learning and inference is executing.</strong> As we
saw previously in section <a href="#sec:AI-and-ML"
data-reference-type="ref" data-reference="sec:AI-and-ML">2</a>,
<em>training</em> is the process through which the model learns from
data. During training, a model is fed data and makes iterative parameter
adjustments to predict target outcomes better. <em>Inference</em> is the
process of using a trained model to make predictions on new, unseen
data. Inference is when a model applies what it has learned during
training. We will now turn to training and examine how models learn in
more detail.</p>
<h3 id="mechanics-of-learning">Mechanics of Learning</h3>
<p>In deep learning, training is a carefully coordinated system
involving loss functions, optimization algorithms, backpropagation, and
other techniques. It allows a model to refine its predictions
iteratively. By making incremental adjustments to its parameters,
training enables a model to gradually reduce its error, improving its
performance over time.</p>
<p><strong>Loss quantifies a model’s error.</strong> Loss is a measure
of a model’s error, used to evaluate its performance. It is calculated
by a <em>loss function</em> that compares target and predicted values to
measure how well the neural network models “fits” the training data.
Typically, neural networks are trained by systematically minimizing this
function. There are many different kinds of loss functions. Here, we
will present two: cross entropy loss and mean squared error (MSE).<br />
<em>Cross entropy loss.</em> Cross entropy is a concept from information
theory that measures the difference between two probability
distributions. In deep learning, cross entropy loss is often used in
classification problems, where it compares the probability distribution
predicted by a model and the target distribution we want the model to
predict.<br />
Consider a binary classification problem where a model is tasked with
classifying images as either apples or oranges. When given an image of
an apple, a perfect model would predict “apple” with 100% probability.
In other words, with classes [apple, orange], the target distribution
would be [1, 0]. The cross entropy would be low if the model predicts
“apple” with 90% probability (outputting a predicted distribution of
[0.9, 0.1]). However, if the model predicts “orange” with 99%
probability, it would have a much higher loss. The model learns to
generate predictions closer to the true class labels by minimizing the
cross entropy loss during training.<br />
Cross entropy quantifies the difference between predicted and true
probabilities. If the predicted distribution is close to the true
distribution, the cross entropy will be low, indicating better model
performance. High cross entropy, on the other hand, signals poor
performance. When used as a loss function, the more incorrect the
model’s predictions are, the larger the error and, in turn, the larger
the training update.<br />
<em>Mean squared error (MSE).</em> Mean squared error is one of the most
popular loss functions for regression problems. It is calculated as the
average of the squared differences between target and predicted
values.</p>
<p><span class="math display">$$MSE = \frac{1}{n} \sum^{n}_{i=1} (y_i -
\hat{y}_i)^2$$</span></p>
<p>MSE gives a good measure of how far away an output is from its target
in a way that is not affected by the direction of errors. Like cross
entropy, MSE provides a larger error signal the more wrong the output
guess, helping the training process converge more quickly. One weakness
of MSE is that it is highly sensitive to outliers, as squaring amplifies
large differences, although there are variants and alternatives such as
mean absolute error (MAE) and Huber loss which are more robust to
outliers.</p>
<p><strong>Loss is minimized through optimization.</strong> Optimization
is the process of minimizing (or maximizing) an objective function. In
deep learning, optimization involves finding the set of parameters that
minimize the loss function. This is achieved with
<em>optimizers</em>–—algorithms that adjust a model’s parameters, such
as weights and biases, to reduce the loss.</p>
<p><strong>Gradient descent is a crucial optimization
algorithm.</strong> Gradient descent is a foundational optimization
algorithm that provides the basis for many advanced optimizers used in
deep learning. It was among the earliest techniques developed for
optimization.<br />
To understand the basic idea behind gradient descent, imagine a
blindfolded hiker standing on a hill trying to reach the bottom of a
valley. With each step, they can feel the slope of the hill beneath
their feet and move in the direction that goes downhill the most. While
the hiker cannot tell where exactly they are going or where they are
ending up, they can continue this process, always taking steps toward
the steepest descent until they have reached the lowest point.<br />
In machine learning, the hill is the loss function, and the steps are
updates to the model’s parameters. The direction of steepest descent is
calculated using the gradients (derivatives) of the loss function with
respect to the model’s parameters.<br />
</p>
<figure id="fig:gradient-example">
<img src="images/ML_background/gradient-graph-design.png" />
<figcaption>Gradient descent over loss landscape given two different
weight initializations - <span class="citation"
data-cites="raj-enjoyalg"></span></figcaption>
</figure>
<p>The size of the steps is determined by the <strong>learning
rate</strong>, a parameter of the model configuration (known as a
<em>hyperparameter</em>) used to control how much a model’s weights are
changed with each update. If the learning rate is too large, the high
learning rate may destroy information faster than information is
learned. However, the optimization process may be very slow if the
learning rate is too small. Therefore, proper learning rate selection is
often key to effective training.<br />
Though powerful, gradient descent in its simplest form can be quite
slow. Several variants, including <em>Adam</em> (Adaptive Moment
Estimation), were developed to address these weaknesses and are more
commonly used in practice.</p>
<p><strong>Backpropagation facilitates parameter updates.</strong>
Backpropagation is a widely used method to compute the gradients in a
neural network <span class="citation"
data-cites="Rumelhart1986"></span>. This process is essential for
updating the model’s parameters and makes gradient descent possible.
Backpropagation is a way to send the error signal from the output layer
of the neural network back to the input layer. It allows the model to
understand how much each parameter contributes to the overall error and
adjust them accordingly to minimize the loss.</p>
<p><strong>Steps to training a deep learning model.</strong> Putting all
of these components together, training is a multi-step process that
typically involves the following:<br />
</p>
<ol>
<li><p><strong>Initialization</strong>: A model’s parameters (weights
and biases) are set to some initial values, often small random numbers.
These values define the starting point for the model’s training and can
significantly influence its success.</p></li>
<li><p><strong>Forward Propagation</strong>: Input data is passed
through the model, layer by layer. The neurons in each layer perform
their specific operations via weights, biases, and activation functions.
Once the final layer is reached, an output is produced. This procedure
can be carried out on individual examples or on <em>batches</em> of
multiple data points.</p></li>
<li><p><strong>Loss Calculation</strong>: The model’s output is compared
to the target output using a loss function that quantifies the
difference between predicted and actual values. The loss represents the
model’s error—how far its output was from what it should have
been.</p></li>
<li><p><strong>Backpropagation</strong>: The error is propagated back
through the model, starting from the output layer and going backward to
the input layer. This process calculates gradients that determine how
much each parameter contributed to the overall loss.</p></li>
<li><p><strong>Parameter Update</strong>: The model’s weights and biases
are adjusted using an optimization algorithm based on the gradients.
This is typically done using gradient descent or one of its
variants.</p></li>
<li><p><strong>Iteration</strong>: Steps 2–5 are repeated many times,
often reaching millions or billions of iterations. With each pass, the
loss should decrease as the model’s predictions improve.</p></li>
<li><p><strong>Stopping Criterion</strong>: Training continues until the
model reaches a stopping point, which can be defined in many ways. We
may stop training when the loss stops decreasing or when the model has
gone through the entire training dataset a specific number of
times.</p></li>
</ol>
<p>While this sketch provides a high-level overview of the training
process, many factors can shape its course. For example, the network
architecture and choice of loss function, optimizer, batch size,
learning rate, and other hyperparameters influence how training
proceeds. Moreover, different methods and approaches to learning
determine how training is carried out. We will explore some of these
techniques in the next section.</p>
<h3 id="training-methods-and-techniques">Training Methods and
Techniques</h3>
<p>Effective training is essential to the ability of deep learning
models to learn how to accomplish tasks. Various methods have been
developed to address key issues many models face in training. Some
techniques offer distinct approaches to learning, whereas others solve
specific computational difficulties. Each has unique characteristics and
applications that can significantly enhance a model’s performance and
adaptability. Different techniques are often used together, like a
recipe using many ingredients.<br />
In this section, we limit our discussion to <em>pre-training</em>,
<em>fine-tuning</em>, and <em>few-shot learning</em>. These three
methods illustrate different ways of approaching the learning process.
Notably, there are ways to learn during training (during backpropagation
and model weight adjustment), and there are also ways to learn after
training (during inference). Pre-training and fine-tuning belong to the
former, and few-shot learning belongs to the latter.</p>
<p><strong>Pre-training is the bulk of generic training.</strong>
Pre-training is training a model on vast quantities of data to give the
model an array of generally useful representations that it can use to
achieve specific tasks. If we want a model that can write movie scripts,
we want it to have a broad education, knowing rules about grammar and
language and how to write more generally, rather than just seeing
existing movie scripts.<br />
Pre-training endows models with weights that capture a rich set of
learned representations from the outset rather than being assigned
random values. This can offer several advantages over training for
specific purposes from scratch, including faster and more effective
training on downstream tasks. Indeed, the name <em>pre-training</em> is
somewhat of a historical artifact. As pre-training makes up most of the
development process for many models, pre-training and training have
become synonymous.<br />
<strong>Tokenization</strong> is a fundamental preprocessing step in
machine learning and natural language processing (NLP). It involves
breaking down a continuous piece of text, such as a sentence or a
document, into smaller units called tokens. Tokens are typically words
or subword units like punctuation, numbers, or special characters.
Tokenization allows a machine learning model to work with text data as
discrete units, making it easier to process and analyze.</p>
<p><strong>Models can either be fine-tuned or used only
pre-trained.</strong> Pre-trained models can be used as is (known as
<em>off-the-shelf</em>) or subjected to further training (known as
<em>fine-tuned</em>) on a target task or dataset. In natural language
processing and computer vision, it is common to use models that have
been pre-trained on large datasets. Many CNNs are pre-trained on the
ImageNet dataset, enabling them to learn many essential characteristics
of the visual world.</p>
<p><strong>Fine-tuning specializes models for specific tasks.</strong>
Fine-tuning is the process of adapting a pre-trained model to a new
dataset or task through additional training. In fine-tuning, the weights
from the pre-trained model are used as the starting point for the new
model. Then, some or all layers are trained on the new task or data,
often with a lower learning rate.<br />
Layers that are not trained are said to be <em>frozen</em>. Their
weights will remain unchanged to preserve helpful representations
learned in pre-training. Typically, layers are modified in reverse
order, from the output layer toward the input layer. This allows the
more specialized, high-level representations of later layers to be
tailored to the new task while conserving the more general
representations of earlier layers.</p>
<p><strong>After training, few-shot learning can teach new
capabilities.</strong> Few-shot learning is a method that enables models
to learn and adapt quickly to new tasks with limited data. It works best
when a model has already learned good representations for the tasks it
needs to perform. In few-shot learning, models are trained to perform
tasks using a minimal number of examples. This approach tests the
model’s ability to learn quickly and effectively from a small dataset.
Few-shot learning can be used to train an image classifier to recognize
new categories of animals after seeing only a few images of each
animal.</p>
<p><strong>Zero-shot learning.</strong> Zero-shot learning is an extreme
version of few-shot learning. It tests a model’s ability to perform on
characteristically new data without being provided any examples during
training. The goal is to enable the model to generalize to new classes
or tasks by leveraging its understanding of relationships in the data
derived from seen examples to predict new, unseen examples.<br />
Zero-shot learning often relies on additional information, such as
attributes or natural language descriptions of unseen data, to bridge
the gap between known and unknown. For instance, consider a model
trained to identify common birds, where each species is represented by
images and a set of attributes (such as size, color, diet, and range) or
a brief description of the bird’s appearance and behavior. The model is
trained to associate the images with these descriptions or attributes.
When presented with the attributes or description of a new species, the
model can use this information to infer characteristics about the
unknown bird and recognize it in images.</p>
<p><strong>LLMs, few-shot, and zero-shot learning.</strong> Some large
language models (LLMs) have demonstrated a capacity to perform few- and
zero-shot learning tasks without explicit training. As model and
training datasets increased in size, these models developed the ability
to solve a variety of tasks when provided with a few examples (few-shot)
or only instructions describing the task (zero-shot) during inference;
for instance, an LLM can be asked to classify a paragraph as having
positive or negative sentiments without specific training. These
capabilities arose organically as the models increased in size and
complexity, and their unexpected emergence raises questions about what
enables LLMs to perform these tasks, especially when they are only
explicitly trained to predict the next token in a sequence. Moreover, as
these models continue to evolve, this prompts speculation about what
other capabilities may arise with greater scale.</p>
<p><strong>Summary.</strong> There are many training techniques used in
deep learning. Pre-training and fine-tuning are the foundation of many
successful models, allowing them to learn valuable representations from
one task or dataset and apply them to another. Few-shot and zero-shot
learning enable models to solve tasks based on scarce or no example
data. Notably, the emergence of few- and zero-shot learning capabilities
in large language models illuminates the potential for these models to
adapt and generalize beyond their explicit training. Ongoing
advancements in training techniques continue to drive the growth of AI
capabilities, highlighting both exciting opportunities and important
questions about the future of the field.</p>
<h2 id="history-and-timeline-of-key-architectures">History and Timeline
of Key Architectures</h2>
<p>Having built our technical understanding of deep learning models and
how they work, we will see how these concepts come together in some of
the groundbreaking architectures that have shaped the field. We will
take a chronological tour of key deep learning models, from the
pioneering LeNet in 1989 to the revolutionary Transformer-based BERT and
GPT in 2018. These architectures, varying in design and purpose, have
paved the way for developing increasingly sophisticated and capable
models. While the history of deep learning extends far beyond these
examples, this snapshot sheds light on a handful of critical moments as
neural networks evolved from a marginal theory in the mid-1900s to the
vanguard of artificial intelligence development by the early 2010s.</p>
<p><strong>LeNet paves the way for future deep learning models <span
class="citation" data-cites="lecun1998gradient"></span>.</strong> LeNet
is a convolutional neural network (CNN) proposed by Yann LeCun and his
colleagues at Bell Labs in 1989. This prototype was the first practical
application of backpropagation, and after multiple iterations of
refinement, LeCun et al. presented the flagship model, LeNet-5, in 1998.
This model demonstrated the utility of neural networks in everyday
applications and inspired many deep learning architectures in the years
to follow. However, due to computational constraints, CNNs did not rise
in popularity for over a decade after LeNet-5 was released.</p>
<p><strong>Recurrent neural networks (RNNs) use feedback loops to
remember.</strong> RNNs are a neural network architecture designed to
process sequential or time-series data, such as text and speech. They
were developed to address failures of traditional feedforward neural
networks in modeling the temporal dependencies inherent to these types
of data. RNNs incorporate a concept of “memory” to capture patterns that
occur over time, like trends in stock prices or weather observations and
relationships between words in a sentence. They use a feedback loop with
a hidden state that stores information from prior inputs, giving them
the ability to “remember” and take historical information into account
when processing future inputs. While this marked a significant
architectural advancement, RNNs were difficult to train and struggled to
learn patterns that occur over more extended amounts of time.</p>
<p><strong>Long short-term memory (LSTM) networks improved memory <span
class="citation" data-cites="hochreither1997lstm"></span>.</strong>
LSTMs are a type of RNN that address some of the shortcomings of
standard RNNs, allowing them to model long-term dependencies more
effectively. LSTMs introduce three <em>gates</em> (input, output, and
forget) to the memory cell of standard RNNs to regulate the flow of
information in and out of the unit. These gates determine how much
information is let in (input gate), how much information is retained
(forget gate), and how much information is passed along (output gate).
This approach allows the network to learn more efficiently and maintain
relevant information for longer.</p>
<p><strong>AlexNet achieves unprecedented performance in image
recognition <span class="citation"
data-cites="krizhevsky2012advances"></span>.</strong> As we saw in
section <a href="#sec:AI-and-ML" data-reference-type="ref"
data-reference="sec:AI-and-ML">2</a>, the <strong>ImageNet
Challenge</strong> was a large-scale image recognition competition that
spurred the development and adoption of deep learning methods for
computer vision. The challenge involved classifying images into 1,000
categories using a dataset of over one million images.<br />
In 2012, a CNN called <em>AlexNet</em>, developed by Alex Krizhevsky,
Ilya Sutskever, and Geoffrey Hinton, achieved a breakthrough performance
of 15.3% top-5 error rate, beating the previous best result of 26.2% by
a large margin and winning the ImageNet Large Scale Visual Recognition
Challenge (ILSVRC). AlexNet consists of eight layers: five convolutional
layers and three fully connected layers. It uses a ReLU activation
function and specialized techniques such as dropout and data
augmentation to improve accuracy.<br />
<strong>ResNets employ residual connections <span class="citation"
data-cites="He2016"></span>.</strong> ResNets were introduced in 2015 by
Microsoft researchers Kaiming He and collaborators. The original model
was the first architecture to implement residual connections. By adding
these connections to a traditional 34-layer network, the authors were
able to achieve great success. In 2015, it won first place in the
ImageNet classification challenge with a top-5 error rate of 3.57%.</p>
<p><strong>Transformers introduce self-attention.</strong> The
Transformer architecture was introduced by Vaswani et al. in their
revolutionary paper “Attention is All You Need.” Like RNNs and LSTMs,
Transformers are a type of neural network that can process sequential
data. However, the approach used in the Transformer was markedly
different from those of its predecessors. The Transformer uses
self-attention mechanisms that allow the model to focus on relevant
parts of the input and the output.</p>
<p>BERT and GPT, both launched in 2018, are two models based on the
Transformer architecture <span class="citation"
data-cites="Radford2019LanguageMA"></span>.</p>
<p><strong>BERT uses pre-training and bidirectional processing <span
class="citation" data-cites="Devlin2019"></span>.</strong> BERT is a
Transformer-based model that can learn contextual representations of
natural language by pre-training on large-scale corpora. Unlike previous
models that process words in one direction (left-to-right or
right-to-left), BERT takes a bidirectional approach. It is pre-trained
on massive amounts of text to perform masked language modeling and next
sentence prediction tasks. Then, the pre-trained model can be fine-tuned
on various natural language understanding tasks, such as question
answering, sentiment analysis, and named entity recognition. BERT was
the first wide-scale, successful use of Transformers, and its contextual
approach allowed it to achieve state-of-the-art results on several
benchmarks.</p>
<p><strong>The GPT models use scale and unidirectional
processing.</strong> The GPT models are a series of Transformer-based
language models launched by OpenAI. The size of these models and scale
at which they were trained led to a remarkable improvement in fluency
and accuracy in various language tasks, significantly advancing the
state-of-the-art in natural language processing. One of the key reasons
GPT models are more popular than BERT models is because they are better
at generating text. While BERT learns really good representations
through being trained to fill in blanks in the middle of sentences, GPT
models are trained to predict what comes next, enabling them to generate
long-form sequences (e.g. sentences, paragraphs, and essays) much more
naturally.<br />
Many important developments have been left out in this brief timeline.
Perhaps more importantly, future developments might revolutionize model
architectures in new ways, perhaps bringing to light older innovations
that have currently fallen to the wayside. Next, we will explore some
common applications of deep learning models.</p>
<h2 id="applications">Applications</h2>
<p>Deep learning has seen a dramatic rise in popularity since the early
2010s, increasingly becoming a part of our daily lives. Its applications
are broad, powering countless services and technologies across many
industries, some of which are highlighted below.</p>
<p><strong>Communication and entertainment.</strong> Deep learning
powers the chatbots and generative tools that sparked the surge in
global interest in AI that began in late 2022. It fuels the
recommendation systems of many streaming services like Netflix, YouTube,
and Spotify, curating personalized content based on viewing or listening
habits. Social media platforms, like Facebook or Instagram, use deep
learning for image and speech recognition to enable features such as
auto-tagging in photos or video transcription. Personal assistants like
Siri, Google Assistant, and Alexa utilize deep learning techniques for
speech recognition and natural language understanding, providing us with
more natural, interactive voice interfaces.</p>
<p><strong>Transportation and logistics.</strong> Deep learning is
central to the development of autonomous vehicles. It helps these
vehicles understand their environment, recognize objects, and make
decisions. Retail and logistics companies like Amazon use deep learning
for inventory management, sales forecasting, and to enable robots to
navigate their warehouses.</p>
<p><strong>Healthcare.</strong> Deep learning has been used to assist in
diagnosing diseases, analyzing medical images, predicting patient
outcomes, and personalizing treatment plans. It has played a significant
role in drug discovery, reducing the time and costs associated with
traditional methods.<br />
Beyond this, deep learning is also used in cybersecurity, agriculture,
finance, business analytics, and many other settings that can benefit
from decision making based on large unstructured datasets. With the more
general abilities of LLMs, the impact of deep learning is set to disrupt
more industries, such as through automatic code generation and
writing.</p>
<p>Deep learning has come a long way since its early days, with
advancements in architectures, techniques, and applications driving
significant progress in artificial intelligence. Deep learning models
have been used to solve complex problems and provide valuable insights
in many different domains. As data and computing power become more
available and algorithmic techniques continue to improve in the years to
come, we can expect deep learning to become even more prevalent and
impactful.<br />
In the next section, we will discuss scaling laws: a set of principles
which can quantitatively predict the effects of more data, larger
models, and more computing power on the performance of deep learning
models. These laws shape how deep learning models are constructed.</p>
<h1 id="sec:scaling-laws">Scaling Laws</h1>
<h3 id="introduction-2">Introduction</h3>
<p>Compelling evidence shows that increases in the performance of many
AI systems can be modeled with equations called <em>scaling laws</em>.
Common knowledge suggests that larger models with more data will perform
better, frequently reiterated in phrases like “add more layers” or “use
more data.” Scaling laws make this folk knowledge mathematically
precise. In this section, we show that the performance of a deep
learning model scales according to parameter count and dataset size—both
of which are primarily bottlenecked by the computational resources
available. Scaling laws describe the relationship between a model’s
performance and primary inputs.</p>
<h3 id="conceptual-background-power-laws">Conceptual Background: Power
Laws </h3>
<p>Power laws are mathematical equations that model how a particular
quantity varies as the power of another. In power laws, the variation in
one quantity is proportional to a power (exponent) of the variation in
another. The power law <span
class="math inline"><em>y</em> = <em>b</em><em>x</em><sup><em>a</em></sup></span>
states that the change in <span class="math inline"><em>y</em></span> is
directly proportional to the change in <span
class="math inline"><em>x</em></span> raised to a certain power <span
class="math inline"><em>a</em></span>. If <span
class="math inline"><em>a</em></span> is <span
class="math inline">2</span>, then when <span
class="math inline"><em>x</em></span> is doubled, <span
class="math inline"><em>y</em></span> will quadruple. One real-world
example is the relation between the area of a circle and its radius. As
the radius changes, the area changes as a square of the radius: <span
class="math inline"><em>y</em> = <em>π</em><em>r</em><sup>2</sup></span>.
This is a power-law equation where <span
class="math inline"><em>b</em> = <em>π</em></span> and <span
class="math inline"><em>a</em> = 2</span>. The volume of a sphere has a
power-law relationship with the sphere’s radius as well: <span
class="math inline">$y = \frac{4}{3} \pi r^3$</span> (so <span
class="math inline">$b=\frac{4}{3}\pi$</span> and <span
class="math inline"><em>a</em> = 3</span>). <em>Scaling laws</em> are a
particular kind of power law that describe how deep learning models
scale. These laws relate a model’s loss with model properties (such as
the number of model parameters or the dataset size used to train the
model).</p>
<p><strong>Log-log plots can be used to visualize power laws.</strong>
Log-log plots can help make these mathematical relationships easier to
understand and identify. Consider the power law <span
class="math inline"><em>y</em> = <em>b</em><em>x</em><sup><em>a</em></sup></span>
again. Taking the logarithm of both sides, the power law becomes <span
class="math inline">log (<em>y</em>) = <em>a</em>log (<em>x</em>) + log (<em>b</em>)</span>.
This is a linear equation (in the logarithmic space) where <span
class="math inline"><em>a</em></span> is the slope and <span
class="math inline">log (<em>b</em>)</span> is the y-intercept.
Therefore, a power-law relationship will appear as a straight line on a
log-log plot (such as <a href="#fig:log-log-plot"
data-reference-type="ref" data-reference="fig:log-log-plot">20</a>),
with the <em>slope</em> of the line corresponding to the
<em>exponent</em> in the power law.<br />
</p>
<figure id="fig:log-log-plot">

<figcaption>A log-log plot for the power law <span
class="math inline"><em>y</em> = <em>b</em><em>x</em><sup><em>a</em></sup></span>.
This shows that the power law results in a straight line.</figcaption>
</figure>
<p><strong>Power laws are remarkably ubiquitous.</strong> Power laws are
a robust mathematical framework that can describe, predict, and explain
a vast range of phenomena in both nature and society. Power laws are
pervasive in urban planning: log-log plots relating variables like city
population to metrics such as the percentage of cities with at least
that population often result in a straight line (see Fig <a
href="#fig:city-pop" data-reference-type="ref"
data-reference="fig:city-pop">21</a>). Similarly, animals’ metabolic
rates are proportional to an exponent of their body mass, showcasing a
clear power law. In social media, the distribution of user activity
often follows a power law, where a small fraction of users generate most
of the content (which means that the frequency of content generation
<span class="math inline"><em>y</em></span> is proportional to the
number of active users <span class="math inline"><em>x</em></span>
multiplied by some constant and raised to some exponent: <span
class="math inline"><em>y</em> = <em>b</em><em>x</em><sup><em>a</em></sup></span>).
Power laws govern many other things, such as the frequency of word usage
in a language, the distribution of wealth, the magnitude of earthquakes,
and more.</p>
<figure id="fig:city-pop">
<img src="images/ML_background/image72.png" />
<figcaption>Power laws in city populations - from <span class="citation"
data-cites="Newman2005"></span></figcaption>
</figure>
<h2 id="scaling-laws-in-deep-learning">Scaling Laws in Deep
Learning</h2>
<h3 id="introduction-3">Introduction</h3>
<p><strong>Power laws in the context of DL are called scaling laws <span
class="citation"
data-cites="hestness2017deep kaplan2020scaling"></span>.</strong>
Scaling laws predict loss given model size and dataset size in a
power-law relationship. Model size is usually measured in parameters,
while dataset size is measured in tokens. As both variables increase,
the model’s loss tends to decrease. This decrease in loss with scale
often follows a power law: the loss drops substantially, but not
linearly, with increases in data and model size. For instance, if we
doubled the number of parameters, the loss does not just halve: it might
decrease to one-fourth or one-eighth, depending on the exponent in the
scaling law. This power-law behavior in AI systems allows researchers to
anticipate and strategize how to improve models by investing more in
increasing the data or the parameters.</p>
<p><strong>Scaling laws in DL predict loss based on model size and
dataset size.</strong> In deep learning, power-law relationships exist
between the model’s performance and other variables. These scaling laws
can forecast the performance of a model given different values for its
parameters, dataset, and amount of computational resources. For
instance, we can estimate a model’s loss if we were to double its
parameter count or halve the training dataset size. Scaling laws show
that it is possible to accurately predict the loss of an ML system using
just two primary variables:</p>
<ol>
<li><p><span class="math inline"><em>N</em></span>: The size of the
model, measured in the number of <em>parameters</em>. Parameters are the
weights in a model that are adjusted during training. The number of
parameters in a model is a rough measure of its <em>capacity</em>, or
how much it can learn from a dataset.</p></li>
<li><p><span class="math inline"><em>D</em></span>: The size of the
<em>dataset</em> the model is trained on, measured in tokens, pixels, or
other fundamental units. The modality of these tokens depends on the
model’s task. For example, tokens are subunits of language in natural
language processing and images in computer vision. Some models are
trained on datasets consisting of tokens of multiple
modalities.</p></li>
</ol>
<p>Improving model performance is typically bottlenecked by one of these
variables.</p>
<p><strong>The computational resources used to train a model are vital
for scaling.</strong> This factor, called <em>compute</em>, is most
often measured by the number of calculations performed over a certain
time. The key metric for compute is FLOP/s, the number of floating-point
operations the computer performs per second. Practically, increasing
compute means training with more processors, more powerful processors,
or for a longer time. Models are often allocated a set budget for
computation: scaling laws can determine the ideal model and dataset size
given that budget.</p>
<p><strong>Computing power underlies both model size and dataset
size.</strong> More computing power enables larger models with more
parameters and facilitates the collection and processing of more tokens
of training data. Essentially, greater computational resources
facilitate the development of more sophisticated AI models trained on
expanded datasets. Therefore, scaling is contingent on increasing
computation.</p>
<h3 id="the-chinchilla-scaling-law-an-influential-example">The
Chinchilla Scaling Law: an Influential Example</h3>
<p><strong>The Chinchilla scaling law emphasizes data over model size
<span class="citation"
data-cites="hoffmann2022training"></span>.</strong> One significant
research finding that shows the importance of scaling laws was the
successful training of the LLM “Chinchilla.” A small model with only 70
billion parameters, Chinchilla outperformed much larger models because
it was trained on far more tokens than pre-existing models. This led to
the <em>Chinchilla scaling law</em>, which provides a scaling law that
depends on parameter count and data. This law demonstrated that larger
models require much more data than was standard.</p>
<figure id="fig:chinchilla">
<img src="images/ML_background/image48.png" />
<figcaption>Chinchilla scaling law - lighter colors indicate less loss -
from <span class="citation"
data-cites="chinchilla-image"></span></figcaption>
</figure>
<p><strong>The Chinchilla scaling law equation encapsulates these
relationships.</strong> The Chinchilla scaling law is estimated to be
<span class="math display">$$\label{eq:chinchilla}
    L(N,D) = 406.4N^{-0.34} + 410.7D^{-0.28} +
\underbrace{1.69}_{\text{Irreducible Error}}$$</span> In equation <a
href="#eq:chinchilla" data-reference-type="ref"
data-reference="eq:chinchilla">[eq:chinchilla]</a>, <span
class="math inline"><em>N</em></span> represents parameter count, <span
class="math inline"><em>D</em></span> represents dataset size, and <span
class="math inline"><em>L</em></span> stands for loss. This equation
describes a power-law relationship. Understanding this law can help us
understand the interplay between these factors, and knowing these values
helps developers make optimal decisions about investments in increasing
model and dataset size.</p>
<p><strong>Scaling laws for DL hold across many modalities and orders of
magnitude.</strong> An <em>order of magnitude</em> is a factor of 10x—if
something increases by an order of magnitude, it increases by 10 times.
In deep learning, evidence suggests that scaling laws hold across many
orders of magnitude of parameter count and dataset size. This implies
that the same scaling relationships are still valid for both a small
model trained on hundreds of tokens or a massive model trained on
trillions of tokens. Scaling laws have continued to hold even as model
size increases dramatically.<br />
</p>
<figure id="fig:enter-label">
<img src="images/ML_background/image74.png" />
<figcaption>Scaling laws for different DL models - <span
class="citation" data-cites="brown2020language"></span></figcaption>
</figure>
<h3 id="discussion">Discussion</h3>
<p><strong>Scaling laws are not universal for ML models.</strong> Not
all models follow scaling laws. These relationships are stronger for
some types of models than others. Generative models such as large
language models tend to follow regular scaling laws—as model size and
training data increase in scale, performance improves smoothly and
predictably in a relationship described by a power-law equation. But for
discriminative models such as image classifiers, clear scaling laws
currently do not emerge. Performance may plateau even as dataset size or
model size increase.</p>
<p><strong>Better learning algorithms can boost model performance across
the board.</strong> An improved algorithm increases the constant term in
the scaling law, allowing models to perform better with a given number
of tokens or parameters. However, crafting better learning algorithms is
quite difficult. Therefore, improving DL models generally focuses on
increasing the core variables for scaling: tokens and parameters.</p>
<p><strong>The bitter lesson: scaling beats intricate, expert-designed
systems.</strong> Hard-coding AI systems to follow pre-defined processes
using expert insights has proven slower and more failure-prone than
building large models that learn from large datasets. The following
observation is Richard Sutton’s "bitter lesson" <span class="citation"
data-cites="sutton2019bitter"></span>:<br />
</p>
<ol>
<li><p>AI researchers have often tried to build knowledge into
systems,</p></li>
<li><p>"This always helps in the short term [...], but in the long run
it plateaus and it even inhibits further progress,</p></li>
<li><p>Breakthrough progress eventually arrives by an opposing approach
based on scaling computation by search and learning."</p></li>
</ol>
<p>This suggests that it is easier to create machines that can learn
than to have humans manually encode them with knowledge. For now, the
most effective way to do this seems to be scaling up deep learning
models such as LLMs. This lesson is “bitter” because it shows that
simpler scaling approaches tend to beat more elegant and complex
techniques designed by human researchers—demoralizing for researchers
who spent years developing those complex approaches. Rather than just
human ingenuity, scale and computational power are also key factors that
drive progress in AI.</p>
<h3 id="conclusion-1">Conclusion</h3>
<p><strong>In AI, scaling laws describe how loss changes with model and
dataset size.</strong> We observed that the performance of a DL model
scales according to the number of parameters and tokens—both shaped by
the amount of compute used. Evidence from generative models like LLMs
indicates a smooth reduction in loss with increases in model size and
training data, adhering to a clear scaling law. Scaling laws are
especially important for understanding how changes in variables like the
amount of data used can have substantial impacts on the model’s
performance.</p>
<h1 id="sec:reinf-learning">Reinforcement Learning</h1>
<h3 id="introduction-4">Introduction</h3>
<p>This section provides a high-level overview of reinforcement learning
(RL). We introduce the basic structure and vocabulary of RL problems and
explore how RL is used.<br />
Reinforcement learning techniques attempt to automate the capacity for
an agent to learn from its actions and their consequences in an
environment <span class="citation"
data-cites="Sutton2018 kaelbling1996reinforcement"></span>. This is
distinct from other ML problems, where a system can learn from an
existing dataset. Instead, an RL system (or <em>agent</em>) learns the
hard way, collecting data through experience. An RL agent must learn how
to explore different possible actions to attain as much reward as
possible. Reward measures the agent’s progress towards its goal and acts
as feedback in the learning process.<br />
Reinforcement learning takes its approach from how animals and humans
learn to interact with the world. If someone puts their hand on a hot
stove, the resulting pain is a significant negative reward, making them
unlikely to repeat the same behavior. As intelligent agents, we learn
through the way the world responds to our actions. We mostly try to
pursue good outcomes, defined by their positive rewards, and avoid bad
outcomes, which lead to negative rewards. In this way, teaching
automated systems through RL is similar to training a dog. When a dog
performs the desired behavior, such as rolling over on command, its
trainers might reward it with a treat. The trainers repeat this process
as the dog gradually refines its behavior to perform as they
desire.<br />
RL techniques have been involved in recent successes in AI. In 2017,
DeepMind’s AlphaGo became the first computer program to defeat a world
champion in Go, a game long considered the most challenging classical
game for AI <span class="citation"
data-cites="silver2016masteringgo"></span>. In 2020, an RL agent
developed in less than a year by a small Maryland company won a clear
victory over an Air Force pilot in simulated one-on-one combat. RL
techniques have been most successful in smaller-scale and less realistic
environments, such as video games, where a very accurate simulated
environment is readily available. However, games can also be designed to
represent real-world environments.</p>
<h2
id="learning-sequential-decision-making-through-trial-and-error">Learning
Sequential Decision Making through Trial and Error</h2>
<p><strong>Trial and error.</strong> All RL problems involve an agent
learning by trial and error to make sequences of decisions. RL agents
usually start without prior knowledge of the environment. Over time, the
agent accumulates information about its environment and uses it to make
better decisions. An agent might learn that bumping into walls
constantly does not lead to good outcomes and avoid the walls to find a
viable path.</p>
<p><strong>Sequential decision making.</strong> Making sequential
decisions effectively is essential to achieving complicated, long-term
goals. Accomplishing such goals requires not only that prudent actions
are taken but also that they are taken in an appropriate order. To an
agent, it is often unclear how one action may impact later actions, but
what it does will affect the future and must be modeled. Since decisions
are sequential in RL tasks, RL agents might be better suited to
achieving complicated, long-term goals.<br />
RL agents can also learn to navigate uncertainty. An agent may not know
if a particular choice is good or bad, and the world may change (on its
own or in response to its choices), but the agent must act regardless. A
self-driving car navigating a busy intersection must sequence its
actions–—accelerate, brake, or turn–—based on the positions and speeds
of other vehicles, and its current actions will affect these other parts
of the environment. To tackle these difficulties, some RL agents are
designed to learn from their interactions to develop a sophisticated
understanding of their environments to plan their actions.</p>
<p><strong>Sequence modeling.</strong> Sequential decision making (SDM)
is a subtype of the general <em>sequence modeling</em> task, which
involves analyzing and predicting patterns in ordered data. In RL, this
data is predominantly in the form of observations of the environment
over time. The RL system learns to make decisions based on this
information, considering its knowledge of the environment, uncertainty
about the world, and past choices.</p>
<p><strong>Distinguishing RL.</strong> Reinforcement learning differs
from supervised and unsupervised learning. A key difference between
these paradigms is how the ML system obtains information. Supervised and
unsupervised learning tasks learn efficient and useful representations
on fixed, precompiled datasets. Supervised learning goes about this
directly: learning by example. Unsupervised learning addresses similar
problems with less guidance, uncovering the latent patterns in unlabeled
data without being explicitly told whether it is right or wrong. It is
akin to solving a puzzle without seeing the picture on the box. By
contrast, RL systems must acquire new information through actions. They
receive almost no data at the outset and learn to achieve a goal by
actively engaging with an environment and seeing what happens.</p>
<h2 id="the-reinforcement-learning-problem">The Reinforcement Learning
Problem</h2>
<p>We can use two toy examples to introduce the central concepts and
terms of reinforcement learning. First, we consider the multi-armed
bandit—a classic reinforcement learning problem—starting with a simple
situation and gradually adding complications to build up to a full
reinforcement learning problem. Next, we use a Gridworld problem to
introduce the agent-environment loop, a framework for understanding
reinforcement learning problems. Finally, we discuss policies: the
decisions that RL agents make.</p>
<h3 id="multi-armed-bandits">Multi-Armed Bandits</h3>
<p>In casino lingo, “one-armed bandits” refers to slot machines, which
have a single lever that takes people’s money. Playing a slot machine is
not a reinforcement learning problem because there is only one possible
action with a random outcome. However, suppose there is a slot machine
with two or more levers. Then, a gambler must decide which lever to pull
every time they play. Assuming the levers affect the device differently,
one lever might win them more over the long run (such as over 500
pulls). How does one decide which levers to pull and in what order? This
problem is called the <em>multi-armed bandit</em> problem.</p>
<p><strong>Exploration-exploitation trade-off.</strong> Multi-armed
bandit problems exemplify a dilemma at the heart of reinforcement
learning: the trade-off between exploration and exploitation. RL agents
must choose between exploring actions with high uncertainty and
exploiting actions with a high expected reward. In a multi-armed bandit
problem, the agent starts without knowing anything about the expected
reward of any given lever. Therefore, the first thing it has to do is
gain information about the levers. Trying different actions with
uncertain rewards is called <em>exploration</em>. By contrast, taking
actions with a high expected value is called <em>exploitation</em>. In
this phase, the agent chooses the actions it expects to yield the
highest rewards based on its current understanding of the
environment.<br />
Balancing this trade-off between exploration and exploitation is crucial
to the success of an RL agent. If the agent explores too much, it might
waste resources on low payoff actions or even stumble into actions with
negative payoffs. Conversely, if the agent exploits too early, it might
settle for suboptimal actions, missing out on potential higher rewards.
The challenge lies in determining when and how to shift between
exploration and exploitation to maximize the overall benefit. A series
of possible strategies aim to make this trade-off, with varying degrees
of sophistication; in the following, we outline two simple examples.</p>
<p><strong>Greedy strategies.</strong> First, the agent pulls each lever
once and records the payoff it receives from each pull. Then, it pulls
whichever lever initially resulted in the highest payoff, which it
continues to pull as many times as possible. This is a greedy strategy
because the agent only tries each lever once and exploits from then on.
However, the problem with a greedy strategy is that the agent cannot be
sure that the lever it settles on actually has the highest expected
utility.</p>
<p><strong>Epsilon-greedy strategies.</strong> After pulling each lever
once, an agent might decide to keep exploring as well. One thing it
could so is pull the lever it currently thinks has the highest expected
payoff 95% of the time and pull a random lever the other 5% of the time.
This is called an epsilon-greedy strategy, where “epsilon” refers to the
small probability of pulling a random lever. Epsilon-greedy is a much
better strategy than greedy if there are enough pulls ahead of the
agent, since the agent will figure out which lever is best to pull over
time. After enough pulls, the agent will pull the most profitable lever
95% of the time.</p>
<p><strong>Adding context to the bandit problem.</strong> So far, the
best lever to pull is always the same and the agent just has to figure
out which one it is. However, suppose each lever has two payoff
distributions: one for when a light on the slot machine is green, and
one for when the light is red. The best lever to pull now depends on the
context (the color of the light), which varies randomly. In this
environment, the agent needs to take the context into account when it
chooses a lever to pull. This is called a <em>contextual multi-armed
bandit</em> problem.</p>
<p><strong>Adding interaction to the bandit problem.</strong> Suppose
further that the light on the slot machine doesn’t vary randomly, but
instead changes according to which lever the agent pulls. For example,
it might turn green after the second lever is pulled or red if once both
levers have been pulled twice. It might be better for the agent to pull
a lever with a lower expected payoff at the moment, but will change the
context such that the agent can access a greater total profit in the
long run. Therefore, the agent needs to be able to estimate expected
payoffs over single actions and sequences of actions.</p>
<h3 id="the-full-reinforcement-learning-problem">The Full Reinforcement
Learning Problem</h3>
<p>The problem we face now is to come up with a strategy that tells the
agent (1) which lever to pull, (2) in varying contexts, and (3) taking
into account that its actions can affect that context. The bandit
problem is now a <em>full reinforcement learning problem</em>.<br />
</p>
<figure id="fig:multi-bandit">
<img src="images/ML_background/RL problem.jpeg" />
<figcaption>Multi-armed bandit, contextual bandit, and full RL problems.
- <span class="citation" data-cites="xie2020"></span></figcaption>
</figure>
<p>The strategies we considered above—–<em>greedy</em> and
<em>epsilon-greedy</em>–—aren’t suitable anymore. To develop better
strategies, we’ll need more terms and concepts. In particular, we’ll
introduce the agent-environment loop as a more technical framework for
RL. We’ll also replace our example: instead of the multi-armed bandit,
we’ll use a problem called Gridworld.</p>
<p><strong>Gridworld.</strong> In Gridworld, an agent learns to find a
particular cell (the goal) in its environment (a two-dimensional grid)
while avoiding other cells (hazards). The agent can move into any
adjacent open cell. Moving into an open cell is neutral, moving into a
hazard is very bad, and moving into the goal is very good.<br />
</p>
<figure id="fig:gridworld">
<img src="images/ML_background/gridworld.png" />
<figcaption>Gridworld: a simple environment used as a toy model in
reinforcement learning.</figcaption>
</figure>
<p><strong>The agent-environment loop.</strong> The relationship between
an RL agent and its environment can be described by the
<em>agent-environment loop</em>. At each <em>time step</em> in this
loop, the agent takes some <em>action</em> which affects the
<em>environment</em>. Then, the agent receives a <em>reward signal</em>
and a state observation from the environment. Using Gridworld as our
example, we discuss each of those terms below.<br />
</p>
<figure id="fig:agent-environment">
<img src="images/ML_background/image36.png" />
<figcaption>The agent-environment loop - from <span class="citation"
data-cites="wikipedia-markov"></span></figcaption>
</figure>
<p><strong><em>Environment and state</em></strong></p>
<p><strong>The environment and state are the world.</strong> In
Gridworld, the agent is located on a particular cell and can move to
adjacent cells. The environment includes the grid itself, its
boundaries, the location of the agent, and anything the agent may
encounter. When the environment changes (such as when the Gridworld
agent moves), we say that its state changes. A state observation is the
information (other than reward) an agent receives from the environment.
In Gridworld, an observation might be the agent’s position as well as
the positions of the goal, hazards, and barriers. At each time step, the
state of the environment updates, and the agent takes a snapshot
observation of the available information in the environment.</p>
<p><strong>Environments can be fully or partially observable.</strong>
When an agent can observe all of the information in its environment, we
say that the environment is <em>fully observable</em>. In contrast, when
an agent can only observe some of the information about the state of the
environment, we say that the environment is <em>partially
observable</em>. The environment in chess is fully observable because
all of the information about the state of the game is available to both
players. By contrast, the environment of a poker game is only partially
observable: players do not know other players’ cards. Our example,
Gridworld, is fully observable.</p>
<p><strong><em>Actions and action spaces</em></strong></p>
<p><strong>Agents choose actions from their action space.</strong>
Agents affect their environment by taking actions. At each time step,
they choose an action from the set of all actions available to them.
This set is called the <em>action space</em>. In Gridworld, the agent
takes an action from the action space that describes each possible move
to an unoccupied adjacent cell. The agent cannot move into a space
occupied by a barrier or outside of the dimensions of the grid.</p>
<p><strong>Action spaces can be continuous or discrete.</strong> A
<em>discrete</em> action space describes a finite number of possible
actions. The action space of our Gridworld agent is discrete—at most,
consisting of up, down, left, and right. The same is true for the action
space of a multi-armed bandit agent, which consists of a finite number
of levers available to pull. In contrast, a <em>continuous</em> action
space, such as possible directions of movement in a real
three-dimensional space, has infinitely many possible actions.</p>
<p><strong>Agents get rewards.</strong> At each time step, agents
receive a reward from the environment. This reward is represented by a
number, which can be positive, negative, or zero. (This means that, in
contrast to the everyday use of the word “reward,” a reward can be a bad
outcome.) The agent in Gridworld might receive a reward of 10 for
finding the goal, <span class="math inline"> − 10</span> for
encountering a hazard, and 0 otherwise. In this case, encountering a
danger cancels out the reward from achieving the goal, so hazards should
always be avoided. However, exploring neutral squares is free, so this
can be done as much as possible.</p>
<p><strong>Rewards are often discounted.</strong> In general, an RL
agent’s goal is to maximize the sum of rewards it receives over the long
term. Sometimes, though, we want an agent to value short-term rewards
more than long-term rewards, such as encouraging agents to behave in a
more risk-averse way or to account for their uncertainty. In these
cases, we multiply future rewards by a <em>discount factor</em> (<span
class="math inline"><em>γ</em></span>), and the agent maximizes the
<em>discounted</em> long-term reward. If the reward at time <span
class="math inline"><em>t</em></span> is <span
class="math inline"><em>R</em><sub><em>t</em></sub></span>, the
discounted long-term reward is: <span
class="math inline"><em>R</em><sub><em>t</em></sub> + <em>γ</em><sup>1</sup><em>R</em><sub><em>t</em> + 1</sub> + <em>γ</em><sup>2</sup><em>R</em><sub><em>t</em> + 2</sub> + ... + <em>γ</em><sup><em>n</em></sup><em>R</em><sub><em>t</em> + <em>n</em></sub></span>.</p>
<p><strong>Rewards are designed to suit the problem.</strong> The
process of creating the right rewards for an RL problem is called
<em>reward shaping</em>. In Gridworld, we might want the agent to find
the goal more quickly rather than take a long, roundabout path. To
encourage this, we could use rewards to incentivize speed or,
equivalently, penalize slowness. If the default reward is 0 when the
agent does not encounter the goal or a hazard, reaching the goal in a
few time steps would be rewarded the same as reaching the goal in 1,000
time steps. However, if we change this reward to <span
class="math inline"> − 1</span>, the agent has an incentive to reach the
goal more quickly.</p>
<h3 id="policies">Policies</h3>
<p>An RL agent’s <em>policy</em> (often denoted <span
class="math inline"><em>π</em></span>) gives an action for each possible
state observation. In simple environments, one can think of a policy as
a two-column matrix, where the first column lists all possible state
observations, and the second column lists each corresponding action. The
strategies we described in the context of the multi-armed bandit
problem—greedy and epsilon-greedy—are examples of policies an agent
could use when the environment has a single state. In the contextual
multi-armed bandit problem, we added a second state. A policy over two
states might look like: “If green, pull lever X; if red, pull lever
Y.”</p>
<p><strong>Policies can be deterministic or stochastic.</strong> A
<em>deterministic policy</em> assumes that all actions are taken with
certainty. In the bandit problem, the <em>greedy</em> strategy is a
deterministic policy. In Gridworld, a deterministic policy may be to
always move to the right or to follow a particular path to the goal. In
contrast, a <em>stochastic policy</em> assumes that actions are taken
with some randomness, such as randomizing between pulling different
levers when following an <em>epsilon-greedy</em> strategy. In Gridworld,
a stochastic policy might be to choose randomly among actions for the
first few time steps.</p>
<p><strong>Optimal policies.</strong> An optimal policy is a policy
that, if followed, maximizes the sum of (discounted) rewards. In
Gridworld, the optimal policy would describe the shortest route to the
goal from any starting location. In more complex problems, however,
computing the optimal policy is often intractable. For example, in
chess, an optimal policy would need to give the best move in all <span
class="math inline"> ∼ 10<sup>45</sup></span> possible board positions.
However, approximating the optimal policy has still produced RL chess
agents that far surpass the human level of play.</p>
<p>This section introduced the agent-environment loop as a technical
framework for talking about reinforcement learning problems. In place of
the “strategies” we considered in the multi-armed bandit problem, we
introduced the idea of a policy, which gives an action at each time step
according to the agent’s observation of its environment. In Gridworld,
we could manually define a policy for the agent to follow–—but that
would defeat the purpose of the problem, which is to have the agent
learn for itself. The next section explores how reinforcement learning
algorithms automate designing policies.</p>
<h2 id="methods-for-solving-rl-problems">Methods for Solving RL
Problems</h2>
<p><strong>RL algorithms develop better policies.</strong> RL algorithms
serve as the blueprints to guide the learning process, aiming to
discover the optimal strategy for the agent in its environment. While
all RL algorithms share a common theme of learning through trial and
error—exploring the environment, observing the results, and adjusting
behavior based on rewards and penalties—their unique characteristics
arise from the specific approach they take toward learning. Specific RL
algorithms are distinguished by how or what their agent learns.</p>
<p><strong>Model-based vs Model-free RL.</strong> We can organize
different RL algorithms into two broad approaches based on their
computational learning strategies: <em>model-based</em> and
<em>model-free</em>. In <em>model-based</em> RL algorithms, the agent
constructs an internal model of its environment. The agent learns how
its world operates and represents cause-and-effect relationships, using
this model to predict the outcomes of actions and optimize its behavior.
An RL agent playing chess could develop a model of the game,
representing the rules and understanding that specific configurations
win the game. It can use this model to plan its moves, simulating
different sequences to anticipate their results before making a move.
Model-based methods are most applicable to problems in environments with
easily representable dynamics.<br />
In contrast, <em>model-free</em> RL methods learn directly from
experience without explicitly modeling the underlying mechanics of the
environment. Instead, the agent develops associations between actions
and rewards. These simple associations serve as a kind of “intuition”
for projecting what actions lead to the most reward in given situations.
Consider an RL agent learning to play Pac-Man: it does not comprehend
the game’s design or the ghosts’ behavior. Instead, through trial and
error, it learns strategies like “eating a power pellet and then chasing
ghosts leads to high scores.” The AI uses these associations, rather
than an internal model of the game, to guide its future actions.
Model-free approaches are often best for problems where the
environment’s dynamics are complex or unknown and abundant data is
available for learning.</p>
<p><strong>Policy-based vs Value-based RL.</strong> Another distinction
is how an RL agent represents what is “good” in its environment,
learning how good states or actions are, how good policies are, or both
at the same time. In <em>policy-based</em> RL methods, the agent focuses
on learning an effective policy: a set of rules guiding its actions in
response to different states. Rather than calculating the value of each
state or action, the agent optimizes the policy based on the observed
rewards of its actions. The agent may pursue an action because it is
part of a good policy without knowing exactly how good the action itself
is. An RL agent learning how to balance a pole on a cart for a robotics
task does not explicitly calculate the value of different pole angles or
cart positions. Instead, it adjusts its policy—how it responds to
various states—to optimize for the goal of keeping the pole balanced. It
might learn the policy “if the pole tilts to the right, move the cart to
the right.” This approach is often best for handling continuous action
spaces but struggles with finding the optimal policy in large or complex
environments due to the vast number of potential policies.<br />
Conversely, <em>value-based</em> RL methods prioritize learning the
value of different states or actions within the environment. The agent
evaluates and ranks states or actions based on their potential to yield
rewards, selecting the ones with the highest estimated reward. An RL
agent playing tic-tac-toe evaluates each move’s value (putting a mark in
an empty cell) based on the likelihood of that move leading to a win and
chooses the action with the best outcome. The agent learns to take
actions that maximize the cumulative reward over time without specifying
a policy.</p>
<p><strong>An agent’s value function estimates the expected reward of a
state or state-action pair.</strong> Internally, an agent might estimate
how good or bad certain states are. The agent learns a <em>value
function</em> to keep track of these estimates, which it can use to
estimate the returns of different actions. Value functions can also
estimate how well a policy performs in the long run by calculating the
expected cumulative reward of following a policy over time. An agent in
Gridworld could learn a value function to estimate the reward of moving
into a particular space. Alternatively, an RL agent in a car’s GPS might
use a value function to estimate the value of different routes based on
the expected travel time.<br />
</p>
<figure id="fig:value-methods">
<img src="images/ML_background/two-types-method.png" />
<figcaption>Two value-based methods: state-value and action-value (Q-)
functions - <span class="citation" data-cites="huggingface"></span>.
</figcaption>
</figure>
<p><strong>Value functions can focus on the value of a state or an
action.</strong> There are two main types of value functions in
reinforcement learning: <em>state-value functions</em> and
<strong>action-value functions</strong>. A state-value function (<span
class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>)
estimates the expected cumulative reward that an agent will receive when
starting from a state <span class="math inline"><em>s</em></span> and
following a policy <span class="math inline"><em>π</em></span>. We can
estimate the <em>state values</em> of each location in Gridworld,
assuming the agent follows a random policy. In every area, the agent
moves to a random adjacent open cell, repeating this process until it
bumps into the goal or a hazard. The state-values of cells near the goal
are higher than those far away from the goal and much higher than those
near the hazard. The state-value function for this policy gives us an
estimate of how likely it is that the agent will reach the goal or hit a
hazard from each cell.<br />
An action-value function <span
class="math inline">(<em>Q</em><sup><em>π</em></sup>(<em>s</em>,<em>a</em>))</span>
estimates the expected cumulative reward that an agent will receive by
taking an action <span class="math inline"><em>a</em></span> from a
state <span class="math inline"><em>s</em></span> and following a policy
<span class="math inline"><em>π</em></span>. In other words, it
estimates the “value” of taking a particular action in a particular
state. Action-values are often referred to as <em>Q-values</em>, and the
value functions that output them are called <em>Q-functions</em>. One
way to approach an optimal policy in RL is by learning the
Q-function.</p>
<p><strong>Q-learning.</strong> Q-learning is one of the oldest and most
popular reinforcement learning algorithms. Q-learning approaches the RL
problem by estimating how good it is to take actions in each state.
Chess has numerous potential moves (actions) for each position on the
board (state). Q-learning would assign a score (Q-value) to each
state-action pair, indicating how beneficial each move is likely to be.
The goal is to determine the optimal Q-function—the rule that most
accurately assigns these scores—by mapping every state-action pair to
its optimal Q-value. When the environment has a finite number of states
and moves, the Q-function can be represented as a table, where each row
represents a state, and each column is an action.<br />
Therefore, Q-learning is the process of updating the Q-values of each
action iteratively based on the outcomes observed in the environment. At
each time step, the agent chooses the action with the highest associated
Q-value from those available to it in its current state. This generates
a new observation and reward, from which the agent updates its
estimates. Each table entry will converge to its optimal value through
many iterations.</p>
<p><strong>Q-learning in Gridworld.</strong> In Gridworld, we have four
actions (North, South, East, West) and 25 states (unique grid cells).
Therefore, the Q-function table would have 100 Q-values, one for every
state-action pair:<br />
</p>
<div id="tab:q-val-table">
<table>
<caption>Example Q-value table for Gridworld.</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">North</th>
<th style="text-align: center;">South</th>
<th style="text-align: center;">East</th>
<th style="text-align: center;">West</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">(0,0)</td>
<td style="text-align: center;">Q((0,0), North)</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">Q((0,0), West)</td>
</tr>
<tr class="even">
<td style="text-align: center;">(0,1)</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">...</td>
</tr>
<tr class="odd">
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">...</td>
</tr>
<tr class="even">
<td style="text-align: center;">(4,5)</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">...</td>
</tr>
<tr class="odd">
<td style="text-align: center;">(5,5)</td>
<td style="text-align: center;">Q((5,5), North)</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">Q((5,5), West)</td>
</tr>
</tbody>
</table>
</div>
<p>In the beginning, all the Q-value estimates are zero because the
agent has no information about the environment and will take random
actions. Eventually, the agent reaches a positive or negative reward
state. Say, for example, our agent gets lucky and finds its way to cell
(0,4) and chooses to go east. It will receive a reward of +1, and its
estimate of the Q-value for ((0,4), East) will be increased accordingly.
The agent learns that moving East is a good action to take in the state
(0,4), and it will be more likely to take this action the next time it
is in the same state.<br />
Over time, the negative and positive Q-values will propagate through the
table. The agent will repeat this process many times as it explores the
states in the environment and tries different actions, and if all goes
well, its estimates will converge to the optimal Q-function. By learning
which actions are desirable to take in which states, the agent learns
how to navigate the world successfully. As the environment becomes more
complicated and action space becomes larger, computing exact Q-values
becomes less computationally feasible. Instead, we might try to estimate
the Q-function using a less computationally taxing method. One such
method involves using neural networks to estimate the Q-function. This
is an example of a method known as deep RL.</p>
<figure id="fig:enter-label">
<img src="images/ML_background/image51.png" />
<figcaption>Example Q-values in Gridworld.</figcaption>
</figure>
<h2 id="deep-reinforcement-learning">Deep Reinforcement Learning</h2>
<p>Deep reinforcement learning (deep RL) is the area of machine learning
that leverages deep learning (DL) to help solve reinforcement learning
(RL) problems. Deep learning, which uses neural networks to compress and
extract information from vast amounts of data, can be applied to RL
problems such as sequential decision making. In this setting, the RL
setting provides signals to help neural networks learn useful
representations for the task at hand.</p>
<p><strong>Deep learning is a substantial part of nearly all major RL
successes in recent years.</strong> As the complexity of modeling even
seemingly simple problems can explode at astonishing rates, using RL to
accomplish interesting tasks demands powerful, scalable methods. Some of
the ways deep learning can be used in RL include:</p>
<ul>
<li><p><strong>Environment.</strong> In model-based RL, deep learning
can be used to learn the dynamics model of the environment, which can
then be used for planning and decision-making.</p></li>
<li><p><strong>Policy function.</strong> Deep learning can help optimize
complex policies for continuous action spaces.</p></li>
<li><p><strong>Value function.</strong> Deep neural networks such as
Deep Q-Network (DQN) can be used to estimate the Q-function. This is
particularly useful for problems with many, or even infinite, possible
state-action pairs. Instead of capturing the Q-function in a
state-action matrix, it can be directly encoded in a neural network,
which takes the state-action pair as an input and outputs the
action-value.</p></li>
</ul>
<p>In the next section, we explore a few recent successes of deep RL.
Along the way, we’ll introduce a few more high-level techniques used to
solve common problems to conclude our discussion of RL.</p>
<h2 id="sec:examples-of-rl">Examples of RL</h2>
<h3 id="alphago">AlphaGo</h3>
<p>In 1997, IBM’s Deep Blue made history by becoming the first AI to
defeat a world-class chess player. Yet it was not until nearly two
decades later that an AI managed to surpass humans in the game of Go,
when AlphaGo won against the best player in the world in 2017. Go is
similar to structure to chess, both being fully observable two-player
turn-based games, but Go is far more complicated. Unlike chess, with
only 20 possible first moves, Go starts with a staggering 361 choices.
This complexity outstrips the capabilities of the brute-force
computations that Deep Blue could rely on to evaluate positions.
Therefore, AlphaGo had to rely on the more sophisticated methods of deep
RL to master the game. By playing millions of games against itself,
AlphaGo honed its strategy and emerged as the world’s top Go player.</p>
<p><strong>RL agents can learn by competing against themselves.</strong>
One training method that has proven effective is <em>self-play</em>. In
part, this is because an RL model presents itself with an appropriate
opponent for an amateur: not a novice, who it could win against easily
without learning anything new, or a grandmaster, who would win against
it decisively enough that it might not understand how it could have
acted differently to prevent a loss. Instead, the best way for an
amateur to improve is to play against opponents of similar skill levels.
The insight of self-play is that a RL agent can learn from players of a
similar strength by playing against versions of itself. This allows the
agent to improve and reach a superhuman level of play without external
intervention.</p>
<h3 id="atari">Atari</h3>
<p>The Atari games are a collection of video games developed between the
1970s and 1980s for the Atari 2600 console, characterized by their
simple graphics, straightforward controls, and challenging gameplay.
Some of the most popular Atari games are Space Invaders, Pac-Man, and
Pong. Each game has its own unique set of rules, goals, and strategies.
As they provide a diverse set of challenges that an RL agent must learn
to overcome, Atari games have long served as a popular testing ground to
assess the capabilities of RL algorithms. They provide a platform to
evaluate how well RL agents can navigate complex environments and master
motor skills. In 2013, the DeepMind team proposed the original set of 57
Atari titles as an RL benchmark. Since then, RL agents have met or
surpassed human performance in every single one of these games <span
class="citation" data-cites="mnih2013playing"></span>. However, some
games took years longer to solve—and inspired new learning methods.</p>
<p><strong>Sparse rewards present challenges to learning.</strong> One
particularly difficult game for RL agents to solve is Montezuma’s
Revenge. In this game, players must make a long series of precise
movements to reach the first reward. They must navigate through a series
of mazes to collect treasure, including a key. The key is necessary to
advance to the next level. The challenge lies in the fact that an RL
agent that moves randomly at first is unlikely to ever encounter this
first reward, let alone complete a level. These rewards are rare and
hard to find. It also requires a delicate balance between exploration
and exploitation, as the key is often hidden in parts of the maze the
agent would not visit unless it is exploring, but exploring too much can
lead to running into traps or enemies. To reach any reward at all, the
player must perform a complex sequence of actions where a single mistake
can lead to losing a life. Learning this game through trial and error
can take a long time.<br />
Montezuma’s Revenge is a classic example of the problem of <em>sparse
rewards</em>, where the agent receives few signals to guide learning. In
environments like this, rewards are few and far between, depriving RL
agents of the feedback they need to learn effective strategies. In
contrast, a game like Breakout (in which a paddle at the bottom of the
environment is used to deflect a ball towards bricks to break them) is
much easier for RL agents. Even random movements are likely to deflect
the ball, break a block, and earn a reward reasonably quickly.</p>
<p><strong>Preferences for novelty can help overcome sparse
rewards.</strong> An RL agent which prefers <em>novelty</em> is rewarded
for new actions and observations. This method is how an RL agent
eventually solved Montezuma’s Revenge: it learned to find the first
reward simply by seeking out new observations. Imagine an RL agent
rewarded for novelty finds itself in an empty room with a single closed
door. After aimlessly wandering around the room for a while, the RL
agent will eventually “get bored” and figure out how to open the door,
venturing out of the room. In this way, novelty-seeking agents can
exhibit intelligent behavior without being explicitly rewarded for any
particular goal. There are many similar ways to develop RL agents and
promote optimal behavior.<br />
The exploration of Atari games and the development of novel methods to
tackle them have significantly advanced the field of reinforcement
learning, providing valuable insights into the learning process of RL
agents.</p>
<h3 id="starcraft">StarCraft</h3>
<p>In 2019, DeepMind’s RL system <em>AlphaStar</em> defeated an expert
human player 5-0 in a StarCraft II match <span class="citation"
data-cites="Vinyals2019"></span>. StarCraft II is a complicated
real-time strategy game that pits two players against each other in a
vast battlefield. StarCraft has many more possible actions and states
than chess, Go, or Atari games, making it a formidable challenge for
players and AIs alike.</p>
<p><strong>AlphaStar learned to overcome significant
uncertainty.</strong> The game’s difficulty is further amplified by its
partially observable environment, as most of the map is shrouded in a
fog of war, and players can only see areas where they have deployed
units. This means players must constantly act under uncertainty, making
educated guesses about the state of the environment and making
predictions about their opponent’s actions and strategies. AlphaStar’s
ability to overcome the challenges of uncertainty, a large action space,
and the need for strategic planning underscores the potential of RL in
navigating complicated, uncertain environments.</p>
<h3 id="diplomacy">Diplomacy</h3>
<p>In 2022, Facebook AI Research (FAIR) developed <em>Cicero</em>, a
program that reached human-level performance in the strategy game
Diplomacy <span class="citation" data-cites="Bakhtin2022"></span>. In
this game, players compete to take over a model of Europe against the
backdrop of the First World War. One of the central features of
Diplomacy is that players communicate in natural language to form
alliances and negotiate in between each turn. Cicero navigated not only
the game’s moves but also natural language processing and
generation.</p>
<p><strong>Cicero learned how to deceive.</strong> Cicero’s success
required learning how to deceive human players. It is documented to have
sometimes convinced other players to agree to lopsided deals or break
promises when it was advantageous. In other words, Cicero developed a
capacity for deception. This result is not surprising: success in
Diplomacy often requires deception.</p>
<h3 id="conclusion-2">Conclusion</h3>
<p>This section explores the major concepts, terms, and techniques
involved in reinforcement learning. First, we used two simple
examples—Multi-Armed Bandits and Gridworld—to outline the major concepts
and terms used in RL. At each time step, the agent takes some action
drawn from the action space, which affects the state of the environment.
Then, the agent receives a reward signal and a state observation from
the environment. The agent acts according to its policy, which specifies
an action for every possible state observation. The agent’s value
function computes the expected cumulative reward of a certain state
(state-value) or state-action pair (Q-value) given a certain policy.
Q-values can be used for Q-learning, one popular learning algorithm, to
update an agent’s policy toward an optimal policy.<br />
After outlining the major concepts and terms used in RL, we discussed
some powerful techniques used in groundbreaking RL systems. First, deep
RL uses deep learning to estimate value functions and policies in tasks
that are too complex to allow exact computation. Second, self-play
expands the ability of RL systems in competitive tasks by repeatedly
pitting the system against itself. Finally, RL systems can overcome the
problem of sparse rewards through preferences for novelty.</p>
<h1 id="conclusion-3">Conclusion</h1>
<p>Understanding the technical underpinnings of AI systems—the
underlying models and algorithms, how they work, how they are used, and
how they are evaluated—is essential to understanding the safety, ethics,
and societal impact of these technologies. This foundation equips us
with the necessary grounding and context to identify and critically
analyze their capabilities and potential, as well as the risks that they
pose. It allows us to discern potential pitfalls in their design,
implementation, and deployment and devise strategies to ensure their
safe, ethical, and beneficial use.</p>
<h2 id="summary-3">Summary</h2>
<p>In this chapter, we presented the fundamentals of artificial
intelligence (AI) and its subfield, machine learning (ML), which aims to
create systems that can learn without being explicitly instructed. We
examined its foundational principles, methodologies, and evolution,
detailing key techniques, concepts, and practical applications.</p>
<p><strong>Artificial intelligence.</strong> We first discussed AI, the
vast umbrella that encapsulates the idea of machines performing tasks
typically associated with human intelligence. AI and its conceptual
origins date back to the 1940s and 1950s when the project of creating
“intelligent machines” came to the fore. The field experienced periods
of flux over the following decades, waxing and waning until the modern
deep learning era was ushered in by the groundbreaking release of
AlexNet in 2012, driven by increased data availability and advances in
hardware.</p>
<p><strong>Defining AI.</strong> The term “artificial intelligence” has
many meanings, and the capabilities of AI systems exist on a continuum.
Five widely used conceptual categories to distinguish between different
types of AI are narrow AI, artificial general intelligence (AGI),
human-level AI (HLAI), transformative AI (TAI), and artificial
superintelligence (ASI). While these concepts provide a basis for
thinking about the intelligence and generality of AI systems, they are
not well-defined or complete, often overlapping and used in different,
conflicting ways. Therefore, in evaluating risk, it is essential to
consider AI systems based on their specific capabilities instead of
broad categorizations.</p>
<p><strong>The ML process.</strong> We presented a general framework for
understanding ML models by considering five aspects of a model: its
task, input data, output, and what type of machine learning it uses. We
then discussed each of these aspects in turn. We explored common tasks
for ML models, including classification, regression, anomaly detection,
and sequence modeling. We highlighted a few of the many types of data
that these models work with and discussed the model development process.
Creating an ML model is a multi-step process that typically includes
data collection, model selection, training, evaluation, and deployment.
Measuring the performance of a model in evaluation is a critical step in
the development process. We surveyed several metrics used to achieve
this, as well as the broader, often conflicting goals that guide this
process.</p>
<p><strong>Types of ML.</strong> We discussed different approaches to
machine learning and how these categories are neither well-defined nor
complete, even though distinctions are often drawn between different
“types” of machine learning. We surveyed four common approaches to ML:
supervised learning, unsupervised learning, reinforcement learning, and
deep learning. At a high level, supervised learning is learning from
labeled data, unsupervised learning is learning from unlabeled data, and
reinforcement learning is learning from agent-gathered data. Deep
learning techniques are used in all three settings, leveraging deep
neural networks to achieve remarkable results.</p>
<p><strong>Deep learning.</strong> We then examined deep learning in
more depth. We saw how, beyond its use of multi-layer neural networks,
deep learning is characterized by its ability to learn hierarchical
representations that provide deep learning models with great flexibility
and power. Machine learning models are functions that capture
relationships between inputs and outputs with representations that allow
them to capture an especially broad family of relationships.</p>
<p><strong>Components of DL models.</strong> We explored the essential
components of deep learning models and neural networks. Through the
example of multi-layer perceptrons (MLPs), we broke down neural
networks, structures composed of layers of neurons, into an input layer,
an output layer, one or more hidden layers, weights, biases, and
activation functions. We highlighted a few significant activation
functions and examined other fundamental building blocks of deep
learning models, including residual connections, convolution, and
self-attention. We also presented influential architectures, such as
CNNs and Transformers.</p>
<p><strong>Processes in DL models.</strong> We discussed how deep
learning models learn and are used in training and inference. We walked
through the steps to training a deep learning model, beginning with
initialization and then cycling through sending information forward to
make a prediction, measuring its error or quality, sending this error
backward, and adjusting parameters accordingly until a stopping
criterion is reached. We discussed training techniques such as
pre-training, fine-tuning, few-shot learning, and zero-shot learning,
and how training typically involves a combination of many methods and
techniques used in conjunction. We considered the importance of
scalability, computational efficiency, and interpretability in
evaluating deep learning models and their suitability for deployment. We
plotted the course of technical and architectural development in the
field, from LeNet in 1989 to BERT and GPT models in 2018. We considered
real-world applications of deep learning in communication and
entertainment, transportation and logistics, and healthcare.</p>
<p><strong>Scaling laws.</strong> Scaling laws describe mathematical
relationships between model performance and key factors like model size
and dataset size in deep learning. These power law equations show that
as models grow in parameters and are trained on more data, their loss
tends to decrease substantially, though not linearly. Scaling up
computational resources used to train a model can enable an increase in
both model parameters and the amount of data used in training.
Researchers can leverage scaling laws to determine optimal model and
dataset sizes given computational constraints. for example, the
Chinchilla model from Google DeepMind demonstrates that smaller models
can outperform larger ones if given much more training data. Scaling
laws hold across many modalities and orders of magnitude. However, these
laws only appear to apply to certain types of model such as generative
models.</p>
<p><strong>Reinforcement learning.</strong> Finally, we turned our
attention to reinforcement learning and explored the field’s basic
concepts, terminology, and techniques. We examined RL problems, which
provide the general framework through which reinforcement learning
attempts to automate the capacity of an agent to learn from its actions
and their consequences in an environment. In these problems, agents
learn to make decisions through trial and error. Through the toy
examples of multi-armed bandits and Gridworld, we introduced the major
concepts and terms used in reinforcement learning. Lastly, we saw a few
examples of groundbreaking applications of RL in gaming, including
AlphaGo, AlphaStar, and Cicero.</p>
<h2 id="key-takeaways">Key Takeaways</h2>
<p><strong>AI created fundamental changes, offering benefits and
presenting risks.</strong> AI is increasingly being integrated into
business, education, healthcare, communication, and other aspects of our
lives. As its influence grows, so too does its potential for harm. AI
systems and their use, whether to automate intricate processes in
manufacturing or to personalize experiences in retail and entertainment,
have direct, real-world impacts. Moreover, AI introduces unique risks,
as it involves the creation of systems that have the ability to learn
and pursue objectives. While highly advanced or general AI systems often
garner the most attention, AI can pose risks at any level of capability.
Even a narrowly-designed model in a critical infrastructure system could
cause significant damage or disruption if it malfunctions or is
exploited. Therefore, it is essential to consider the risks and impact
of AI across multiple scales: immediate <em>and</em> long-term,
individual <em>and</em> societal.</p>
<p><strong>Predicting the trajectory and impact of AI is a difficult and
uncertain task.</strong> We have already seen AI systems that achieved
better-than-human performance in specific, narrow tasks such as image
recognition or playing chess. As technological advancement continues, it
is possible that we may see more general, advanced, and transformative
systems that can perform a wider array of tasks and adapt to more
complex scenarios. However, forecasting these developments involves a
high degree of uncertainty, both in terms of rate and direction of AI
advancement and in the potential risks and capabilities of individual
systems. The progression of AI is not linear and is influenced by a
multitude of factors, including advancements in algorithms, hardware,
data availability, policy regulations, and economic conditions. Hence, a
prudent approach to AI development involves continuous vigilance,
rigorous testing, and adaptable strategies to manage both expected and
unanticipated challenges.</p>
<p><strong>Deep learning has emerged as a potent and defining force in
AI, driving significant advancements and innovations.</strong> The
strength of deep learning models lies in their ability to learn and
generalize from extensive datasets. Deep learning models can process
massive quantities of information, discerning patterns and relationships
that are too complex for traditional algorithms. These powerful
capabilities are integral to many tools and systems we encounter in our
daily lives. Deep learning is the engine behind systems that recognize
patterns in images and speech, enabling technologies like automatic
photo tagging and voice-activated virtual assistants. It powers
autonomous vehicles, chatbots, predictive modeling tools, and
translation applications. The scope of deep learning’s influence is vast
and continues to expand, underscoring its pivotal role in the future of
AI.<br />
Despite their accomplishments, DL and ML systems have many flaws and
limitations. Their performance and capabilities depend on a number of
factors, including the quality and quantity of training data, the
algorithms and architectures used, and the amount of computational power
available. In addition, these systems often struggle to generalize
beyond the specific contexts they were trained in. They lack the common
sense and contextual knowledge that humans often use to make decisions,
leading to errors that may seem obvious to human observers. Moreover,
they often act as <em>black boxes</em> where the internal mechanisms
driving their predictions are <em>opaque</em>, a trait that can be
especially problematic in high-stakes settings.</p>
<p><strong>Understanding ML models is essential to identifying risks
associated with their use.</strong> Deep learning models are prone to
absorbing and replicating biased patterns embedded in their training
data, leading to skewed and possibly discriminatory decisions. Equipped
with this knowledge, we can anticipate such issues and proactively
implement measures to address them, creating more equitable outcomes.
Moreover, an informed understanding of the technical intricacies of AI
systems is instrumental to assessing and incorporating crucial factors
like robustness and interpretability in their development and
deployment. In doing so, we are better able to do things like improve
model performance in various environments and against adversarial
attacks and provide more transparency and accountability. Consequently,
understanding ML models is key to ensuring the responsible and equitable
use of these powerful tools.</p>
<p><strong>The methodologies employed to train models determine the
capabilities and impact of these systems.</strong> Many techniques
exist, each with distinct strengths, weaknesses, and associated safety
or ethical considerations. A system trained with reinforcement learning
might adopt risky or harmful behaviors if these behaviors are
accidentally rewarded during training. This can pose considerable risks,
especially in real-world, high-stakes environments. Similarly, a model
trained using few-shot learning techniques, which rely on learning from
a minimal number of examples, might make crucial decisions based on an
insufficient amount of data. This can introduce inaccuracies,
potentially causing harm or undesired outcomes. Therefore, understanding
the intricacies of different training methods and techniques is vital to
developing robust and ethical AI systems.</p>
<p><strong>AI is poised to become increasingly capable and influential
as technology progresses.</strong> Some of recent AI growth is driven by
advancements in algorithms and techniques, which enable systems to learn
and make predictions more accurately and efficiently as they increase in
sophistication. Concurrently, the development of more powerful and
cost-effective computing technology is facilitating the deployment and
scaling of AI systems, making them more accessible, widespread, and
capable. Moreover, the expanding volume and variety of data available
for training is significantly enhancing their ability to learn and
generalize, thereby broadening their applicability. Together, these
factors—techniques, compute, and data—are accelerating the advancement
of AI systems. AI’s impact on individuals, industries, and society in
the future is only becoming more profound.</p>
