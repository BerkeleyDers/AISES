<!-- Single Agent Safety -->

<h1 id="introduction">3.1 Introduction</h1>
<p>To understand the risks associated with artificial intelligence (AI),
we begin by examining the challenge of making single agents safe. In this chapter, 
we review core components of this challenge including monitoring, robustness, alignment and systemic safety.<p>
To start, we discuss the problem of monitoring AI systems. The opaqueness of machine learning (ML) systems — 
their ``black-box'' nature — hinders our ability to fully comprehend how they make decisions and what their 
intentions, if any, may be. In addition, models may spontaneously develop qualitatively new and unprecedented
 ``emergent'' capabilities as they become more advanced (for example, when we make them larger, train them for
 longer periods, or expose them to more data). Models may also contain hidden functionality that is hard to detect,
 such as backdoors that cause them to behave abnormally in a very small number of circumstances.<p>
Next, we turn to problem of building models that are robust to adversarial attacks. 
In many cases, it is not possible to perfectly specify our idealized goals for an AI system. Inadequately specified
 goals can lead to systems diverging from our idealized goals, and introduce vulnerabilities that adversaries can attack and exploit. <p>
We then pivot to the topic of control of AI systems. We start by exploring the issue of deception, categorizing the
 varied forms of deception (including those already observed in existing AI systems), and analyze the risks involved
 in AI systems deceiving human and AI evaluators. We also explore the possible conditions that could give rise to power-seeking
 agents and the ways in which this could lead to particularly harmful risks. We discuss some techniques that have potential to 
help with making AI systems more controllable and reducing the inherent hazards they may pose through representation control and
 unlearning specific capabilities or knowledge.<p>
Beyond making individual AIs more safe, we discuss how AI research can contribute to systemic safety by influencing the context 
within which ML systems are used and reducing the risk that ML systems fail or are misused. AI can be used to create or improve 
tools to defend against risks from AI. For example, AI can be applied to reduce risks from pandemic diseases, cyber-attacks or disinformation.<p>
We conclude by explaining how researchers trying to improve AI safety can unintentionally improve the capabilities of AI systems
 and potentially end up increasing the overall risks that they pose. To avoid this, it is important to pick research topics carefully
 to minimise the impacts that successful research will have on capabilities.<p>
This chapter argues that, even when considered in isolation, individual
AI systems can pose catastrophic risks. As we will see in subsequent
chapters, many of these risks become more pronounced when considering
multi-agent systems, misuse, and arms race dynamics.<p>
</p>
