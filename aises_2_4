<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html lang="en" xmlns:epub="http://www.idpf.org/2007/ops" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Introduction to AI Safety Ethics, and Society</title>
<meta http-equiv="default-style" content="text/html; charset=UTF-8"/>
<link rel="stylesheet" type="text/css" href="../style.css"/>
</head>
<body>
<div class="chapter">
<h1 class="section" id="sec2-4">2.4 SCALING LAWS</h1>
<h3 class="section0"><i>Introduction</i></h3>
<p class="nonindent">Compelling evidence shows that increases in the performance of many AI systems can be modeled with equations called <i>scaling laws.</i> Machine learning researchers have often found that larger models with more data usually perform better, and scaling laws attempt to quantify this folk knowledge. In this section, we discuss how the performance of deep learning models has scaled according to parameter count and dataset size, both of which factors are primarily bottlenecked by the computational resources available. Scaling laws describe the relationship between a model&#x0027;s performance and the computational inputs that it receives.</p>
<h3 class="section"><i>Conceptual Background: Power Laws</i></h3>
<p class="nonindent1">Scaling laws are a type of power law. Power laws are mathematical equations that model how a particular quantity varies as the power of another. In power laws, the variation in one quantity is proportional to a power (exponent) of the variation in another. The power law <i>y</i> = <i>bx<sup>a</sup></i> states that the change in <i>y</i> is directly proportional to the change in <i>x</i> raised to a certain power <i>a.</i> If <i>a</i> is 2, then when <i>x</i> is doubled, <i>y</i> will quadruple. One real-world example is the relation between the area of a circle and its radius. As the radius changes, the area changes as a square of the radius: <i>y</i> = &#x03C0;<i>r</i><sup>2</sup>. This is a power-law equation where <i>b</i> = &#x03C0; and <i>a</i> = 2. The volume of a sphere has a power-law relationship with the sphere&#x0027;s radius as well: <span class="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>y</mi><mo>=</mo><mfrac><mn>4</mn><mn>3</mn></mfrac><mi>&#x3C0;</mi><msup><mi>r</mi><mn>3</mn></msup></math></span> (so <span class="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>b</mi><mo>=</mo><mfrac><mn>4</mn><mn>3</mn></mfrac><mi>&#x3C0;</mi></math></span> and <i>a</i> = 3). <i>Scaling laws</i> are a particular kind of power law that describe how deep learning models scale. These laws relate a model&#x0027;s loss with model properties (such as the number of model parameters or the dataset size used to train the model).</p>
<p class="nonindent1"><b><i>Log-log plots can be used to visualize power laws.</i></b> Log-log plots can help make these mathematical relationships easier to understand and identify. Consider the power law <i>y</i> = <i>bx<sup>a</sup></i> again. Taking the logarithm of both sides, the power law becomes log(<i>y</i>) = <i>a</i> log(<i>x</i>) + log(<i>b</i>). This is a linear equation (in the logarithmic space) where <i>a</i> is the slope and log(<i>b</i>) is the <i>y</i>-intercept. Therefore, a power-law relationship will appear as a straight line on a log-log plot (such as 2.22), with the <i>slope</i> of the line corresponding to the <i>exponent</i> in the power law.</p>
<figure id="fig:log-log-plot"><img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/power_law_v2.png" class="tb-img-full" style="width:80%"/></figure>
<figure>
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/log_power_law_v2.png" class="tb-img-full" style="width:80%"/>
<p class="tb-caption">FIGURE 2.22. An object in free falling in a vacuum falls a distance proportion to the square of the time. On a log-log plot, this power law looks like a straight line.</p>
</figure>
<p class="nonindent1"><b><i>Power laws are remarkably ubiquitous.</i></b> Power laws are a robust mathematical framework that can describe, predict, and explain a vast range of phenomena in both nature and society. Power laws are pervasive in urban planning: log-log plots relating variables like city population to metrics such as the percentage of cities with at least that population often result in a straight line (see Fig 2.23). Similarly, animals&#x2019; metabolic rates are proportional to an exponent of their body mass, showcasing a clear power law. In social media, the distribution of user activity often follows a power law, where a small fraction of users generate most of the content (which means that the frequency of content generation <i>y</i> is proportional to the number of active users <i>x</i> multiplied by some constant and raised to some exponent: <i>y</i> = <i>bx<sup>a</sup></i>). Power laws govern many other things, such as the frequency of word usage in a language, the distribution of wealth, the magnitude of earthquakes, and more.</p>

<figure id="fig:city-pop">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/city_power_laws_regular_v2.png" class="tb-img-full" style="width:80%"/>
</figure>
<figure id="fig:city-pop-log">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/city_power_laws_log_v2.png" class="tb-img-full" style="width:80%"/>
<p class="tb-caption">FIGURE 2.23. Power laws are used in many domains, such as city planning.</p>
</figure>
<h2 class="section" id="sec2-4-1">2.4.1 Scaling Laws in Deep Learning</h2>
<h2 class="section"><i>Introduction</i></h2>
<p class="nonindent1"><b><i>Power laws in the context of deep learning are called (neural) scaling laws.</i></b> Scaling laws [1, 2] predict loss given model size and dataset size in a power law relationship. Model size is usually measured in parameters, while dataset size is measured in tokens. As both variables increase, the model&#x0027;s loss tends to decrease. This decrease in loss with scale often follows a power law: the loss drops substantially, but not linearly, with increases in data and model size. For instance, if we doubled the number of parameters, the loss does not just halve: it might decrease to one-fourth or one-eighth, depending on the exponent in the scaling law. This power-law behavior in AI systems allows researchers to anticipate and strategize on how to improve models by investing more in increasing the data or the parameters.</p>
<p class="nonindent1"><b><i>Scaling laws in deep learning predict loss based on model size and dataset size.</i></b> In deep learning, we have observed power-law relationships between the model&#x0027;s performance and other variables that have held consistently over eight orders of magnitude as the amount of compute used to train models has scaled. These scaling laws can forecast the performance of a model given different values for its parameters, dataset, and amount of computational resources. For instance, we can estimate a model&#x0027;s loss if we were to double its parameter count or halve the training dataset size. Scaling laws show that it is possible to accurately predict the loss of an ML system using just two primary variables:</p>
<ol class="num">
<li><i>N</i>: The size of the model, measured in the number of <i>parameters.</i> Parameters are the weights in a model that are adjusted during training. The number of parameters in a model is a rough measure of its <i>capacity,</i> or how much it can learn from a dataset.</li>
<li><i>D</i>: The size of the <i>dataset</i> the model is trained on, measured in tokens, pixels, or other fundamental units. The modality of these tokens depends on the model&#x0027;s task. For example, tokens are subunits of language in natural language processing and images in computer vision. Some models are trained on datasets consisting of tokens of multiple modalities.</li>
</ol>
<p class="nonindent1">Improving model performance is typically bottlenecked by one of these variables.</p>
<p class="nonindent1"><b><i>The computational resources used to train a model are vital for scaling.</i></b> This factor, often referred to as <i>compute,</i> is most often measured by the number of calculations performed over a certain time. The key metric for compute is FLOP/s, the number of floating-point operations the computer performs per second. Practically, increasing compute means training with more processors, more powerful processors, or for a longer time. Models are often allocated a set budget for computation: scaling laws can determine the ideal model and dataset size given that budget.</p>
<p class="nonindent1"><b><i>Computing power underlies both model size and dataset size.</i></b> More computing power enables larger models with more parameters and facilitates the collection and processing of more tokens of training data. Essentially, greater computational resources facilitate the development of more sophisticated AI models trained on expanded datasets. Therefore, scaling is contingent on increasing computation.</p>
<h3 class="section"><i>The Chinchilla Scaling Law: an Influential Example</i></h3>
<p class="nonindent1"><b><i>The Chinchilla scaling law emphasizes data over model size [3].</i></b> One significant research finding that shows the importance of scaling laws was the successful training of the LLM &#x201C;Chinchilla.&#x201D; A small model with only 70 billion parameters, Chinchilla outperformed much larger models because it was trained on far more tokens than pre-existing models. This led to the development of the <i>Chinchilla scaling law:</i> a scaling law that accounts for parameter count and data. This law demonstrated that larger models require much more data than was typically assumed at the time to achieve the desired gains in performance.</p>
<figure id="fig:chinchilla">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/chinchilla_scaling_2.jpeg" class="tb-img-full" style="width: 60%"/>
<p class="tb-caption">FIGURE 2.24. Chinchilla scaling laws provide an influential estimate of compute-optimal scaling laws, specifying the optimal ratio of model parameters and training tokens for a given training compute budget in FLOPs. The green lines show projections of optimal model size and training token count based on the number of FLOPs used to train Google&#x0027;s Gopher model [4].</p>
</figure>
<p class="nonindent1"><b><i>The Chinchilla scaling law equation encapsulates these relationships.</i></b> The Chinchilla scaling law is estimated to be</p>
<p class="eq"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>L</mi><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mn>406.4</mn><msup><mi>N</mi><mrow data-mjx-texclass="ORD"><mo>&#x2212;</mo><mn>0.34</mn></mrow></msup><mo>+</mo><mn>410.7</mn><msup><mi>D</mi><mrow data-mjx-texclass="ORD"><mo>&#x2212;</mo><mn>0.28</mn></mrow></msup><mo>+</mo><munder><mrow data-mjx-texclass="OP"><munder><mn>1.69</mn><mo>&#x23DF;</mo></munder></mrow><mrow data-mjx-texclass="ORD"><mtext>Irreducible Error</mtext></mrow></munder></math> <span class="right">(2.1)</span></p>
<p class="nonindent1">In equation 2.1, <i>N</i> represents parameter count, <i>D</i> represents dataset size, and <i>L</i> stands for loss. This equation describes a power-law relationship. Understanding this law can help us understand the interplay between these factors, and knowing these values helps developers make optimal decisions about investments in increasing model and dataset size.</p>
<p class="nonindent1"><b><i>Scaling laws for deep learning hold across many modalities and orders of magnitude.</i></b> An <i>order of magnitude</i> refers to a tenfold increase&#x2014;if something increases by an order of magnitude, it becomes 10 times larger. In deep learning, evidence suggests that scaling laws hold across many orders of magnitude of parameter count and dataset size. This implies that the same scaling relationships are still valid for both a small model trained on hundreds of tokens or a massive model trained on trillions of tokens. Scaling laws have continued to hold even as model size increases dramatically.</p>
<figure id="fig:swiss_cheese">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/test_loss.png" class="tb-img-full"/>
<p class="tb-caption">Figure 2.25 The scaling laws for different deep learning models look remarkably similar [2].</p>
</figure>
<h3 class="section"><i>Discussion</i></h3>
<p class="nonindent"><b>Scaling laws are not universal for ML models.</b> Not all models follow scaling laws. These relationships are stronger for some types of models than others. Generative models such as large language models tend to follow regular scaling laws&#x2014;as model size and training data increase in scale, performance improves smoothly and predictably in a relationship described by a power-law equation. But for discriminative models such as image classifiers, clear scaling laws currently do not emerge. Performance may plateau even as dataset size or model size increase.</p>
<p class="nonindent1"><b><i>Better learning algorithms can boost model performance across the board.</i></b> An improved algorithm increases the constant term in the scaling law, allowing models to perform better with a given number of tokens or parameters. However, crafting better learning algorithms is quite difficult. Therefore, improving deep learning models generally focuses on increasing the core variables for scaling: tokens and parameters.</p>
<p class="nonindent1"><b><i>The bitter lesson: scaling beats intricate, expert-designed systems.</i></b> Hard-coding AI systems to follow pre-defined processes using expert insights has proven slower and more failure-prone than building large models that learn from large datasets. The following observation is Richard Sutton&#x0027;s &#x201C;bitter lesson&#x201D; [5]:</p>
<ol class="num">
<li>AI researchers have often tried to build knowledge into systems,</li>
<li>&#x201C;This always helps in the short term [&#x2026;], but in the long run it plateaus and it even inhibits further progress,</li>
<li>Breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning.&#x201D;</li>
</ol>
<p class="nonindent1">This suggests that it is easier to create machines that can learn than to have humans manually encode them with knowledge. For now, the most effective way to do this seems to be scaling up deep learning models such as LLMs. This lesson is &#x201C;bitter&#x201D; because it shows that simpler scaling approaches tend to beat more elegant and complex techniques designed by human researchers&#x2014;demoralizing for researchers who spent years developing those complex approaches. Rather than human ingenuity alone, scale and computational power are also key factors that drive progress in AI.</p>
<p class="nonindent1">It is worth noting that while the general trend of improved performance through scaling has held over many order of magnitude of computation, the equations used to model this trend are subject to criticism and debate. The original scaling laws identified by a team at OpenAI in 2020 were superseded by the Chinchilla scaling laws described above, which may in turn be replaced in future. While there do seem to be interesting and important regularities at work, the equations that have been developed are less well-established than in some other areas of science, such as the laws of thermodynamics.</p>
<h3 class="section"><i>Conclusion</i></h3>
<p class="nonindent"><b>In AI, scaling laws describe how loss changes with model and dataset size.</b> We observed that the performance of a deep learning model scales according to the number of parameters and tokens&#x2014;both shaped by the amount of compute used. Evidence from generative models like LLMs, observed over eight orders of magnitude of training compute, indicates a smooth reduction in loss as model size and training data increase, following a clear scaling law. Scaling laws are especially important for understanding how changes in variables like the amount of data used can have substantial impacts on the model&#x0027;s performance.</p>
<h2 class="section">References</h2>
<p class="ref">[1] Joel Hestness et al. <i>Deep Learning Scaling is Predictable, Empirically</i>. 2017. arXiv: 1712.00409 [cs.LG].</p>
<p class="ref">[2] Jared Kaplan et al. <i>Scaling Laws for Neural Language Models</i>. 2020. arXiv: 2001.08361 [cs.LG].</p>
<p class="ref">[3] Jordan Hoffmann et al. <i>Training Compute-Optimal Large Language Models</i>. 2022. arXiv: 2203.15556 [cs.CL].</p>
<p class="ref">[4] nostalgebraist. <i>chinchilla&#x0027;s wild implications</i>. 2022. url: <a href="https://www.alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications">https://www.alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications</a> (visited on 09/28/2023).</p>
<p class="ref">[5] Rich Sutton. <i>The Bitter Lesson</i>. url: <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a> (visited on 09/28/2023).</p>
</div>
</body>
</html>
