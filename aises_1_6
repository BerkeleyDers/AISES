<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html lang="en" xmlns:epub="http://www.idpf.org/2007/ops" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Introduction to AI Safety Ethics, and Society</title>
<meta http-equiv="default-style" content="text/html; charset=UTF-8"/>
<link rel="stylesheet" type="text/css" href="../style.css"/>
</head>
<body>
<div class="chapter">
<h1 class="section" id="sec1-6">1.6 Discussion of Connections Between Risks</h1>
<p class="nonindent">So far, we have considered four sources of AI risk separately, but they also interact with each other in complex ways. We give some examples to illustrate how risks are connected.</p>
<p class="nonindent1">Imagine, for instance, that a corporate AI race compels companies to prioritize the rapid development of AIs. This could increase organizational risks in various ways. Perhaps a company could cut costs by putting less money toward information security, leading to one of its AI systems getting leaked. This would increase the probability of someone with malicious intent having the AI system and using it to pursue their harmful objectives. Here, an AI race can increase organizational risks, which in turn can make malicious use more likely.</p>
<p class="nonindent1">In another potential scenario, we could envision the combination of an intense AI race and low organizational safety leading a research team to mistakenly view general capabilities advances as “safety.” This could hasten the development of increasingly capable models, reducing the available time to learn how to make them controllable. The accelerated development would also likely feed back into competitive pressures, meaning that less effort would be spent on ensuring models were controllable. This could give rise to the release of a highly powerful AI system that we lose control over, leading to a catastrophe. Here, competitive pressures and low organizational safety can reinforce AI race dynamics, which can undercut technical safety research and increase the chance of a loss of control.</p>
<p class="nonindent1">Competitive pressures in a military environment could lead to an AI arms race, and increase the potency and autonomy of AI weapons. The deployment of AI-powered weapons, paired with insufficient control of them, would make a loss of control more deadly, potentially existential. These are just a few examples of how these sources of risk might combine, trigger, and reinforce one another.</p>
<p class="nonindent1">It is also worth noting that many existential risks could arise from AIs amplifying existing concerns. Power inequality already exists, but AIs could lock it in and widen the chasm between the powerful and the powerless, perhaps even enabling an unshakable global totalitarian regime. Similarly, AI manipulation could undermine democracy, which would also increase the risk of an irreversible totalitarian regime. Disinformation is already a pervasive problem, but AIs could exacerbate it to a point where we fundamentally undermine our ability to reach consensus or sense a shared reality. AIs could develop more deadly bioweapons and reduce the required technical expertise for obtaining them, greatly increasing existing risks of bioterrorism. AI-enabled cyberattacks could make war more likely, which would increase existential risk. Dramatically accelerated economic automation could lead to long-term erosion of human control and enfeeblement. Each of those issues—power concentration, disinformation, cyberattacks, automation—is causing ongoing harm, and their exacerbation by AIs could eventually lead to a catastrophe from which we might not recover.</p>
<p class="nonindent1">As we can see, ongoing harms, catastrophic risks, and existential risks are deeply intertwined. Historically, existential risk reduction has focused on <em>targeted</em> interventions such as technical AI control research, but the time has come for <em>broad</em> interventions [1] like the many sociotechnical interventions outlined in this chapter.</p>
<p class="nonindent1">In mitigating existential risk, it does not make practical sense to ignore other risks. Ignoring ongoing harms and catastrophic risks normalizes them and could lead us to “drift into danger” [2], as further discussed in chapter Safety Engineering. Overall, since existential risks are connected to less extreme catastrophic risks and other standard risk sources, and because society is increasingly willing to address various risks from AIs, we believe that we should not solely focus on <em>directly</em> targeting existential risks. Instead, we should consider the diffuse, <em>indirect</em> effects of other risks and take a more comprehensive approach to risk management.</p>
<h2 class="section">References</h2>
<p class="ref">[1] Nick Beckstead. <i>On the overwhelming importance of shaping the far future</i>. 2013.</p>
<p class="ref">[2] Jens Rasmussen. &#x201C;Risk management in a Dynamic Society: A Modeling Problem&#x201D;. English. In: <i>Proceedings of the Conference on Human Interaction with Complex Systems,</i> 1996.</p>
</div>
</body>
</html>
