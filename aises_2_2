<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html lang="en" xmlns:epub="http://www.idpf.org/2007/ops" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Introduction to AI Safety Ethics, and Society</title>
<meta http-equiv="default-style" content="text/html; charset=UTF-8"/>
<link rel="stylesheet" type="text/css" href="../style.css"/>
</head>
<body>
<div class="chapter">
<h1 class="section" id="sec2-2">2.2 ARTIFICIAL INTELLIGENCE &#x0026; MACHINE LEARNING</h1>
<p class="nonindent">Artificial intelligence (AI) is reshaping our society, from its small effects on daily interactions to sweeping changes across many industries and implications for the future of humanity. This section explains what AI is, discusses what AI can and cannot do, and helps develop a critical perspective on the potential benefits and risks of artificial intelligence. Firstly, we will discuss what AI means, its different types, and its history. Then, in the second part of this section, we will analyze the field of machine learning (ML).</p>
<h2 class="section" id="sec2-2-1">2.2.1 Artificial Intelligence</h2>
<h3 class="section0"><i>Definition</i></h3>
<p class="nonindent1"><b><i>Defining Artificial Intelligence.</i></b> In general, AI systems are computer systems performing tasks typically associated with intelligent beings (such as problem solving, making decisions, and forecasting future events) [1]. However, due to its fast-paced evolution and the variety of technologies it encompasses, AI lacks a universally accepted definition, leading to varying interpretations. Moreover, the term is used to refer to different but related ideas. Therefore, it is essential to understand the contexts in which people use the term. For instance, AI can refer to a branch of computer science, a type of machine, a tool, a component of business models, or a philosophical idea. We might use the term to discuss physical objects with human-like capabilities, like robots or smart speakers. We may also use AI in a thought experiment that prompts questions about what it means to be intelligent or human and encourages debates on the ethics of learning and decision-making machines. This book primarily uses AI to refer to an intelligent computer system.</p>
<p class="nonindent1"><b><i>Different meanings of intelligence.</i></b> While intelligence is fundamental to AI, there is no widespread consensus on its definition [2]. Generally, we consider something intelligent if it can learn to achieve goals in various environments. Therefore, one definition of intelligence is the ability to learn, solve problems, and perform tasks to achieve goals in various changing, hard-to-predict situations. Some theorists see intelligence as not just one skill among others but the ultimate skill that allows us to learn all other abilities. Ultimately, the line between what is considered <i>intelligent</i> and what is not is often unclear and contested.</p>
<p class="nonindent1">Just as we consider animals and other organisms intelligent to varying degrees, AIs may be regarded as intelligent at many different levels of capability. An artificial system does not need to surpass all (or even any) human abilities for some people to call it intelligent. Some would consider GPT intelligent, and some would not. Similarly, outperforming humans at specific tasks does not automatically qualify a machine as intelligent. Calculators are usually much better than humans at performing rapid and accurate mathematical calculations, but this does not mean they are intelligent in a more general sense.</p>
<p class="nonindent1"><b><i>Continuum of intelligence.</i></b> Rather than classifying systems as &#x201C;AI&#x201D; or &#x201C;not AI,&#x201D; it is helpful to think of the capabilities of AI systems on a continuum. Evaluating the intelligence of particular AI systems by their capabilities is more helpful than categorizing each AI using theoretical definitions of intelligence. Even if a system is imperfect and does not understand everything as a human would, it could still learn new skills and perform tasks in a helpful, meaningful way. Furthermore, an AI system that is not considered human-level or highly intelligent could pose serious risks; for example, weaponized AIs such as autonomous drones are not generally intelligent but still dangerous. We will dive into these distinctions in more detail when we discuss the different types of AI. First, we will explore the rich history of AI and see its progression from myth and imagination to competent, world-changing technology.</p>
<h3 class="section"><i>History</i></h3>
<p class="nonindent">We will now follow the journey of AI, tracing its path from ancient times to the present day. We will discuss its conceptual and practical origins, which laid the foundation for the field&#x0027;s genesis at the <i>Dartmouth Conference</i> in 1956. We will then survey a few early approaches and attempts to create AI, including <i>symbolic AI, perceptrons,</i> and the chatbot ELIZA. Next, we will discuss how the <i>First AI Winter</i> and subsequent periods of reduced funding and interest have shaped the field. Then, we will chart how the internet, algorithmic progress, and advancements in hardware led to increasingly rapid developments in AI from the late 1980s to the early 2010s. Finally, we will explore the modern deep learning era and see a few examples of the power and ubiquity of present-day AI systems&#x2014;and how far they have come.</p>
<p class="nonindent1"><b><i>Early historical ideas of AI.</i></b> Dreams of creating intelligent machines have been present since the earliest human civilizations. The ancient Greeks speculated about automatons&#x2014;mechanical devices that mimicked humans or animals. It was said that Hephaestus, the god of craftsmen, built the giant Talos from bronze to patrol an island.</p>
<p class="nonindent1"><b><i>The modern conception of AI.</i></b> Research to create intelligent machines using computers began in the 1950s, laying the foundation for a technological revolution that would unfold over the following century. AI development gained momentum over the decades, supercharged by groundbreaking technical algorithmic advances, increasing access to data, and rapid growth in computing power. Over time, AI evolved from a distant theoretical concept into a powerful force transforming our world.</p>
<h4 class="section"><b><i>Origins and Early Concepts (1941&#x2013;1956)</i></b></h4>
<p class="nonindent"><b><i>Early computing research.</i></b> The concept of computers as we know them today was formalized by British mathematician Alan Turing at the University of Cambridge in 1936. The following years brought the development of several electromechanical machines (including Turing&#x0027;s own <i>bombes</i> used to decipher messages encrypted with the German Enigma code) in the turmoil of World War II and, by the mid-1940s, the first functioning digital computers emerged in their wake. Though rudimentary by today&#x0027;s standards, the creation of these machines&#x2014;Colossus, ENIAC, the Automatic Computing Engine, and several others&#x2014;marked the dawn of the computer age and set the stage for future computer science research.</p>
<p class="nonindent1"><b><i>The Turing Test.</i></b> Turing created a thought experiment to assess if an AI could convincingly simulate human conversation [3]. In what Turing called the <i>Imitation Game,</i> a human evaluator interacts with a human and a machine, both hidden from view. If the evaluator fails to identify the machine&#x0027;s responses reliably, then the machine passes the test, qualifying it as intelligent. This framework offers a method for evaluating machine intelligence, yet it has many limitations. Critics argue that machines could pass the Turing Test merely by mimicking human conversation without truly understanding it or possessing intelligence. As a result, some researchers see the Turing Test as a philosophical concept rather than a helpful benchmark. Nonetheless, since its inception, the Turing Test has substantially influenced how we think about machine intelligence.</p>
<h4 class="section"><b><i>The Birth of AI (1956&#x2013;1974)</i></b></h4>
<p class="nonindent"><b><i>The Dartmouth Conference.</i></b> Dr. John McCarthy coined the term &#x201C;artificial intelligence&#x201D; in a seminal conference at Dartmouth College in the summer of 1956. He defined AI as &#x201C;the science and engineering of making intelligent machines,&#x201D; laying the foundation for a new field of study. In this period, AI research took off in earnest, becoming a significant subfield of computer science.</p>
<p class="nonindent1"><b><i>Early approaches to AI.</i></b> During this period, research in AI usually built on a framework called symbolic AI, which uses symbols and rules to represent and manipulate knowledge. This method theorized that symbolic representation and computation alone could produce intelligence. Good Old-Fashioned AI (GOFAI) is an early approach to symbolic AI that specifically involves programming explicit rules for systems to follow, attempting to mimic human reasoning. This intuitive approach was popular during the early years of AI research, as it aimed to replicate human intelligence by modeling how humans think, instilling our reasoning, decision-making, and information-processing abilities into machines.</p>
<p class="nonindent1">These &#x201C;old-fashioned&#x201D; approaches to AI allowed machines to accomplish well-described, formalizable tasks, but they faced severe difficulties in handling ambiguity and learning new tasks. Some early systems demonstrated problem-solving and learning capabilities, further cementing the importance and potential of AI research. For instance, one proof of concept was the General Problem Solver, a program designed to mimic human problem-solving strategies using a trial-and-error approach. The first <i>learning machines</i> were built in this period, offering a glimpse into the future of machine learning.</p>
<p class="nonindent1"><b><i>The first neural network.</i></b> One of the earliest attempts to create AI was the perceptron, a method implemented by Frank Rosenblatt in 1958 and inspired by biological neurons [4]. The perceptron could learn to classify patterns of inputs by adjusting a set of numbers based on a learning rule. It is an important milestone because it made an immense impact in the long run, inspiring further research into deep learning and neural networks. However, scholars initially criticized it for its lack of theoretical foundations, minimal generalizability, and inability to separate data clusters with more than just a straight line. Nonetheless, perceptrons prepared the ground for future progress.</p>
<p class="nonindent1"><b><i>The first chatbot.</i></b> Another early attempt to create AI was the ELIZA chatbot, a program that simulated a conversation with a psychotherapist. Joseph Weizenbaum created ELIZA in 1966 to use pattern matching and substitution to generate responses based on keywords in the user&#x0027;s input. He did not intend the ELIZA chatbot to be a serious model of natural language understanding but rather a demonstration of the superficiality of communication between humans and machines. However, some users became convinced that the ELIZA chatbot had genuine intelligence and empathy despite Weizenbaum&#x0027;s insistence to the contrary.</p>
<h4 class="section"><b><i>AI Winters and Resurgences (1974&#x2013;1995)</i></b></h4>
<p class="nonindent"><b><i>First AI Winter.</i></b> The journey of AI research was not always smooth. Instead, it was characterized by <i>hype cycles</i> and hindered by several <i>winters:</i> periods of declining interest and progress in AI. The late 1970s saw the onset of the first and most substantial decline. In this period, called the <i>First AI Winter</i> (from around 1974 to 1980), AI research and funding declined markedly due to disillusionment and unfulfilled promises, resulting in a slowdown in the field&#x0027;s progress.</p>
<p class="nonindent1"><b><i>The first recovery.</i></b> After this decline, the 1980s brought a resurgence of interest in AI. Advances in computing power and the emergence of systems that emulate human decision-making reinvigorated AI research. Efforts to build expert systems that imitated the decision-making ability of a human expert in a specific field, using pre-defined rules and knowledge to solve complex problems, yielded some successes. While these systems were limited, they could leverage and scale human expertise in various fields, from medical diagnosis to financial planning, setting a precedent for AI&#x0027;s potential to augment and even replace human expertise in specialized domains.</p>
<p class="nonindent1"><b><i>The second AI winter.</i></b> Another stagnation in AI research started around 1987. Many AI companies closed, and AI conference attendance fell by two thirds. Despite widespread lofty expectations, expert systems had proven to be fundamentally limited. They required an arduous, expensive, top-down process to encode rules and heuristics in computers. Yet expert systems remained inflexible, unable to model complex tasks or show common-sense reasoning. This winter ended by 1995, as increasing computing power and new methods aided a resurgence in AI research.</p>
<h4 class="section"><b><i>Advancements in Machine Learning (1995&#x2013;2012)</i></b></h4>
<p class="nonindent"><b><i>Accelerating computing power and the Internet.</i></b> The invention of the Internet, which facilitated rapid information sharing, with exponential growth in computing power (often called <i>compute</i>) helped the recovery of AI research and enabled the development of more complex systems. Between 1995 and 2000, the number of Internet users grew by 2100%, which led to explosive growth in digital data. This abundant digitized data served as a vast resource for machines to learn from, eventually driving advancements in AI research.</p>
<p class="nonindent1"><b><i>A significant victory of AI over humans.</i></b> In 1997, IBM&#x0027;s AI system <i>Deep Blue</i> defeated world chess champion Garry Kasparov, marking the first time a computer triumphed over a human in a highly cognitive task [5]. This win demonstrated that AI could excel in complex problem-solving, challenging the notion that such tasks were exclusively in the human domain. It offered an early glimpse of AI&#x0027;s potential.</p>
<p class="nonindent1"><b><i>The rise of probabilistic graphical models (PGMs) [6].</i></b> PGMs became prominent in the 2000s due to their versatility, computational efficiency, and ability to model complex relationships. These models consist of nodes representing variables and edges indicating dependencies between them. By offering a systematic approach to representing uncertainty and learning from data, PGMs paved the way for more advanced ML systems. In bioinformatics, for instance, PGMs have been employed to predict protein interactions and gene regulatory networks, providing insights into biological processes.</p>
<p class="nonindent1"><b><i>Developments in tree-based algorithms.</i></b> Decision trees are an intuitive and widely used ML method. They consist of a graphical representation of a series of rules that lead to a prediction based on the input features; for example, researchers can use a decision tree to classify whether a person has diabetes based on age, weight, and blood pressure. However, these trees have many limitations, a tendency to make predictions based on the training data without generalizing well to new data (called overfitting).</p>
<p class="nonindent1">Researchers in the early 2000s created methods for combining multiple decision trees to overcome these issues. <i>Random forests</i> are a collection of decision trees trained independently on different subsets of data and features [7]. The final prediction is the average or majority vote of the predictions of all the trees. <i>Gradient boosting</i> combines decision trees in a more sequential, adaptive way, starting with a single tree that makes a rough prediction and then adding more trees to correct the errors of previous trees [8]. Gradient-boosted decision trees are the state-of-the-art method for tabular data (such as spreadsheets), usually outperforming deep learning.</p>
<p class="nonindent1"><b><i>The impact of support vector machines (SVMs).</i></b> The adoption of SVM models in the 2000s was a significant development. SVMs operate by finding an optimal boundary that best separates different categories of data points, permitting efficient classification [9]; for instance, an SVM could help distinguish between handwritten characters. Though these models were used across various fields during this period, SVMs have fallen out of favor in modern machine learning due to the rise of deep learning methods.</p>
<p class="nonindent1"><b><i>New chips and even more compute.</i></b> In the late 2000s, the proliferation of massive datasets (known as <i>big data</i>) and rapid growth in computing power allowed the development of advanced AI techniques. Around the early 2010s, researchers began using <i>Graphics Processing Units</i> (GPUs)&#x2014;traditionally used for rendering graphics in video games&#x2014;for faster and more efficient training of intricate ML models. Platforms that enabled leveraging GPUs for general-purpose computing facilitated the transition to the deep learning era.</p>
<h4 class="section"><b><i>Deep Learning Era (2012&#x2013;)</i></b></h4>
<p class="nonindent"><b><i>Deep learning revolutionizes AI.</i></b> The trends of increasing data and compute availability laid the foundation for groundbreaking ML techniques. In the early 2010s, researchers pioneered applications of <i>deep learning (DL),</i> a subset of ML that uses artificial neural networks with many layers, enabling computers to learn and recognize patterns in large amounts of data. This approach led to significant breakthroughs in AI, especially in areas including image recognition and natural language understanding.</p>
<p class="nonindent1">Massive datasets provided researchers with the data needed to train deep learning models effectively. A pivotal example is the <i>ImageNet</i> ([10]) dataset, which provided a large-scale dataset for training and evaluating computer vision algorithms. It hosted an annual competition, which spurred breakthroughs and advancements in deep learning. In 2012, the <i>AlexNet</i> model revolutionized the field as it won the ImageNet Large Scale Visual Recognition Challenge [11]. This breakthrough showcased the superior performance of deep learning over traditional machine learning methods in computer vision tasks, sparking a surge in deep learning applications across various domains. From this point onward, deep learning has dominated AI and ML research and the development of real-world applications.</p>
<p class="nonindent1"><b><i>Advancements in DL.</i></b> In the 2010s, deep learning techniques led to considerable improvements in <i>natural language processing (NLP),</i> a field of AI that aims to enable computers to understand and generate human language. These advancements facilitated the widespread use of virtual assistants Alexa and ChatGPT, introducing consumers to products that integrated machine learning. Later, in 2016, Google DeepMind&#x0027;s AlphaGo became the first AI system to defeat a world champion Go player in a five-game match [12].</p>
<p class="nonindent1"><b><i>Breakthroughs in natural language processing.</i></b> In 2018, Google researchers introduced the <i>Transformer</i> architecture, which enabled the development of highly effective NLP models. Researchers built the first <i>large language models</i> (LLMs) using this Transformer architecture, many layers of neural networks, and billions of words of data. <i>Generative Pre-trained Transformer</i> (GPT) models have demonstrated impressive and near human-level language processing capabilities [13]. ChatGPT was released in November 2022 and became the first example of a viral AI product, reaching 100 million users in just two months. The success of the GPT models also sparked widespread public discussion on the potential risks of advanced AI systems, including congressional hearings and calls for regulation. In the early 2020s, AI is used for many complex tasks, from image recognition to autonomous vehicles, and continues to evolve and proliferate rapidly.</p>
<h2 class="section" id="sec2-2-2">2.2.2 Types of AI</h2>
<p class="nonindent">The field has developed a set of concepts to describe distinct types or levels of AI systems. However, they often overlap, and definitions are rarely well-formalized, universally agreed upon, or precise. It is important to consider an AI system&#x0027;s particular capabilities rather than simply placing it in one of these broad categories. Labeling a system as a &#x201C;weak AI&#x201D; does not always improve our understanding of it; we need to elaborate further on its abilities and why they are limited.</p>
<p class="nonindent1">This section introduces five widely used conceptual categories for AI systems. We will present these types of AI in roughly their order of intelligence, generality, and potential impact, starting with the least potent AI systems.</p>
<ol class="num">
<li><b>Narrow AI</b> can perform specific tasks, potentially at a level that matches or surpasses human performance.</li>
<li><b>Artificial general intelligence</b> (AGI) can perform many cognitive tasks across multiple domains. It is sometimes interpreted as referring to AI that can perform a wide range of tasks at a human or superhuman level.</li>
<li><b>Human-level AI</b> (HLAI) could perform all tasks that humans can do.</li>
<li><b>Transformative AI</b> (TAI) is a term for AI systems with a dramatic impact on society, at least at the level of the Industrial Revolution.</li>
<li><b>Artificial superintelligence</b> (ASI) refers to systems that surpass human performance on virtually all intellectual tasks [14].</li>
</ol>
<p class="nonindent1"><b><i>Generality and skill level of AI systems.</i></b> The concepts we discuss here do not provide a neat gradation of capabilities as there are at least two different axes along which these can be measured. When considering a system&#x0027;s level of capability, it can be helpful to decompose this into its degree of skill or intelligence and its generality: the range of domains where it can learn to perform tasks well. This helps us explain two key ways AI systems can vary: an AI system can be more or less skillful and more or less general. These two factors are related but distinct: an AI system that can play chess at a grandmaster level is skillful in that domain, but we would not consider it general because it can only play chess. On the other hand, an advanced chatbot may show some forms of general intelligence while not being particularly good at chess. Skill can be further broken down by reference to varying skill levels among humans. An AI system could match the skill of the average adult (50th percentile), or of experts in this skill at varying levels (e.g. 90th of 99th percentile), or surpass all humans in skill.</p>
<h3 class="section"><i>Narrow AI</i></h3>
<p class="nonindent1"><b><i>Narrow AI is specialized in one area.</i></b> Also called <i>weak AI,</i> narrow AI refers to systems designed to perform specific tasks or solve particular problems within a specialized domain of expertise. A narrow AI has a limited domain of competence&#x2014;it can solve individual problems but is not competent at learning new tasks in a wide range of domains. While they often excel in their designated tasks, these limitations mean that a narrow AI does not exhibit high behavioral flexibility. Narrow AI systems struggle to learn new behaviors effectively, perform well outside their specific domain, or generalize to new situations. However, narrow AI is still relevant from the perspective of catastrophic risks, as systems with superhuman capabilities in high-risk domains such as virology or cyber-offense could present serious threats.</p>
<p class="nonindent1"><b><i>Examples of narrow AI.</i></b> One example of narrow AI is a digital personal assistant that can receive voice commands and perform tasks like transcribing and sending text messages but cannot learn how to write an essay or drive a car. Alternatively, image recognition algorithms can identify objects like people, plants, or buildings in photos but do not have other skills or abilities. Another example is a program that excels at summarizing news articles. While it can do this narrow task, it cannot diagnose a medical condition or compose new music, as these are outside its specific domain. More generally, intelligent beings such as humans can learn and perform all these tasks.</p>
<div class="table">
<p class="tcaption">TABLE 2.1. A matrix showing one potential approach to breaking down the skill and generality of existing AI systems [15]. Note that this is just one example and that we do not attempt to apply this exact terminology throughout the book.</p>
<table class="table">
<tr>
<td class="td1"><p class="tbodyleft"><b>Skill</b></p></td>
<td class="td1"><p class="tbodyleft"><b>Narrow</b></p></td>
<td class="td1"><p class="tbodyleft"><b>General</b></p></td>
</tr>
<tr>
<td class="td1"><p class="tbodyleft"><b>No AI</b></p></td>
<td class="td1"><p class="tbodyleft"><b>Narrow Non-AI:</b> calculator software; compiler</p></td>
<td class="td1"><p class="tbodyleft"><b>General Non-AI:</b> human-in-the-loop computing, e.g. Amazon Mechanical Turk</p></td>
</tr>
<tr>
<td class="td1"><p class="tbodyleft"><b>Emerging:</b> equal to or somewhat better than an un-skilled human</p></td>
<td class="td1"><p class="tbodyleft"><b>Emerging Narrow AI:</b> simple rule-based systems</p></td>
<td class="td1"><p class="tbodyleft"><b>Emerging AGI:</b> ChatGPT, Bard, Llama 2, Gemini</p></td>
</tr>
<tr>
<td class="td1"><p class="tbodyleft"><b>Competent:</b> at least 50th percentile of skilled adults</p></td>
<td class="td1"><p class="tbodyleft"><b>Competent Narrow AI:</b> Smart Speakers such as Siri (Apple); VQA systems such as Watson (IBM); SOTA LLMs for some tasks (e.g. short essay writing)</p></td>
<td class="td1"><p class="tbodyleft"><b>Competent AGI:</b> not yet achieved (at least across all tasks)</p></td>
</tr>
<tr>
<td class="td1"><p class="tbodyleft"><b>Expert:</b> at least 90th percentile of skilled adults</p></td>
<td class="td1"><p class="tbodyleft"><b>Expert Narrow AI:</b> spelling &#x0026; grammar checkers such as Grammarly; generative image models such as Dall-E 2</p></td>
<td class="td1"><p class="tbodyleft"><b>Expert AGI:</b> not yet achieved</p></td>
</tr>
<tr>
<td class="td1"><p class="tbodyleft"><b>Virtuoso:</b> at least 99th percentile of skilled adults</p></td>
<td class="td1"><p class="tbodyleft"><b>Virtuoso Narrow AI:</b> DeepBlue; AlphaGo</p></td>
<td class="td1"><p class="tbodyleft"><b>Virtuoso AGI:</b> not yet achieved</p></td>
</tr>
<tr>
<td class="td1"><p class="tbodyleft"><b>Superhuman:</b> outperforms 100% of humans</p></td>
<td class="td1"><p class="tbodyleft"><b>Superhuman Narrow AI:</b> Stockfish; AlphaFold; AlphaZero</p></td>
<td class="td1"><p class="tbodyleft"><b>Artificial Superintelligence (ASI):</b> not yet achieved</p></td>
</tr>
</table>
</div>
<p class="nonindent1"><b><i>Narrow AI vs. general AI.</i></b> Some narrow AI systems have surpassed human performance in specific tasks, such as chess. However, these systems exhibit narrow rather than general intelligence because they cannot learn new tasks and perform well outside their domain. For instance, IBM&#x0027;s Deep Blue famously beat world chess champion Garry Kasparov in 1997. This system was an excellent chess player but was only good at chess. If one tried to use Deep Blue to play a different game, recognize faces in a picture, or translate a sentence, it would fail miserably. Therefore, although narrow AI may be able to do certain things better than any human could, even highly capable ones remain limited to a small range of tasks.</p>
<h3 class="section"><i>Artificial General Intelligence (AGI)</i></h3>
<p class="nonindent1"><b><i>AGI can refer to generality, adaptability or be a shorthand for matching human performance.</i></b> As generally intelligent systems, AGIs can learn and perform various tasks in various areas. On some interpretations, an AGI can reason, learn, and respond well to new situations it has never encountered before. It can even learn to generalize its strong performance to many domains without requiring specialized training for each one: it could initially learn to play chess, then continue to expand its knowledge and abilities by learning video games, diagnosing diseases, or navigating a city. Some would extend this further and define AGIs as systems that can apply their intelligence to nearly any real-world task, matching or surpassing human cognitive abilities across many domains.</p>
<p class="nonindent1"><b><i>There is no single consensus definition of AGI.</i></b> Constructing a precise and detailed definition of AGI is challenging and often creates disagreement among experts; for instance, some argue that an AGI must have a physical embodiment to interact with the world, allowing it to cook a meal, move around, and see and interact with objects. Others contend that a system could be generally intelligent without any ability to physically interact with the world, as intelligence does not require a human-like body. Some would say ChatGPT is an AGI because it is not narrow and is, in many senses, general. Still, an AI that can interact physically may be more general than a non-embodied system. This shows the difficulty of reaching a consensus on the precise meaning of AGI.</p>
<p class="nonindent1"><b><i>Predicting AGI.</i></b> Predicting when distinct AI capabilities will appear (often called &#x201C;AI timelines&#x201D;) can also be challenging. Many once believed that AI systems would master physical tasks before tackling &#x201C;higher-level&#x201D; cognitive tasks such as coding or writing. However, some existing language model systems can write functional code yet cannot perform physical tasks such as moving a ball. While there are many explanations for this observation&#x2014;cognitive tasks bypass the challenge of building robotic bodies; domains like coding and writing benefit from abundant training data&#x2014;this is an example of the difficulties involved in predicting how AI will develop.</p>
<p class="nonindent1"><b><i>Risks and capabilities.</i></b> Rather than debating whether a system meets the criteria for being an AGI, evaluating a specific AI system&#x0027;s capabilities is often more helpful. Historical evidence and the unpredictability of AI development suggest that AIs may be able to perform complicated tasks such as scientific research, hacking, or synthesizing bioweapons before they can reliably automate all domestic chores. Some highly relevant and dangerous capabilities may arrive long before others. Moreover, we could have narrow AI systems that can teach anyone how to enrich uranium and build nuclear weapons but cannot learn other tasks. These dangers show how AIs can pose risks at many different levels of capabilities. With this in mind, instead of simply asking about AGI (&#x201C;When will AGI arrive?&#x201D;), it might be more relevant and productive to consider when AIs will be able to do particularly concerning tasks (&#x201C;When will this specific capability arrive?&#x201D;).</p>
<h3 class="section"><i>Human-Level Artificial Intelligence (HLAI)</i></h3>
<p class="nonindent1"><b><i>Human-level artificial intelligence (HLAI) can do everything humans can do.</i></b> HLAIs exist when machines can perform approximately every task as well as human workers. Some definitions of HLAI emphasize three conditions: first, that these systems can perform every task humans can; second, they can do it at least as well as humans can; and third, they can do it at a lower cost. If a smart AI is highly expensive, it may make economic sense to continue to use human labor. If a smart AI took several minutes to think before doing a task a human could do, its usefulness would have limitations. Like humans, an HLAI system could hypothetically master a wide range of tasks, from cooking and driving to advanced mathematics and creative writing. Unlike AGI, which on some interpretations can perform some&#x2014;but not all&#x2014;the tasks humans can, an HLAI can complete any conceivable human task. Notably, some reserve the term HLAI to describe only cognitive tasks. Furthermore, evaluating whether a system is &#x201C;human level&#x2019; is fraught with biases. We are often biased to dismiss or underrate unfamiliar forms of intelligence simply because they do not look or act like human intelligence.</p>
<h3 class="section"><i>Transformative AI (TAI)</i></h3>
<p class="nonindent1"><b><i>Transformative AI (TAI) refers to AI with societal impacts comparable to the Industrial Revolution.</i></b> The Industrial Revolution fundamentally altered the fabric of human life globally, heralding an era of tremendous economic growth, increased life expectancy, expanded energy generation, a surge in technological innovation, and monumental social changes. Similarly, a transformative AI could catalyze dramatic changes in our world. The focus here is not on the specific design or built-in capabilities of the AI itself but on the consequences of the AI system for humans, our societies, and our economies.</p>
<p class="nonindent1"><b><i>Many kinds of AI systems could be transformative.</i></b> It is conceivable that some systems could be transformative while performing at capabilities below human level. To bring about dramatic change, AI does not need to mimic the powerful systems of science fiction that behave indistinguishably from humans or surpass human reasoning. Computer systems that can perform tasks traditionally handled by people (narrow AIs) could also be transformative by enabling inexpensive, scalable, and clean energy production. Advanced AI systems could transform society without reaching or exceeding human-level cognitive abilities, such as by allowing a wide array of fundamental tasks to be performed at virtually zero cost. Conversely, some systems might only have transformative impacts long after reaching performance above the human level. Even when some forms of AGI, HLAI, or ASI are available, the technology might take time to diffuse widely, and its economic impacts may come years afterward, creating a <i>diffusion lag.</i></p>
<h3 class="section"><i>Superintelligence (ASI)</i></h3>
<p class="nonindent1"><b><i>Superintelligence (ASI) refers to AI that surpasses human performance in virtually all domains of interest [14].</i></b> A system with this set of capabilities could have immense practical applications, including advanced problem-solving, automation of complex tasks, and scientific discovery. However, it should be noted that surpassing humans on only some capabilities does not make an AI superintelligent&#x2014;a calculator is superhuman at arithmetic, but not a superintelligence.</p>
<p class="nonindent1"><b><i>Risks of superintelligence.</i></b> The risks associated with superintelligence are substantial. ASIs could be harder to control and even pose existential threats&#x2014;risks to the survival of humanity. That said, an AI system must not be superintelligent to be dangerous. An AGI, human-level AI, or narrow AI could all pose severe risks to humanity. These systems may vary in intelligence across different tasks and domains, but they can be dangerous at many levels of intelligence and generality. If a narrow AI is superhuman at a specific dangerous task like synthesizing viruses, it could be an extraordinary hazard for humanity.</p>
<p class="nonindent1"><b><i>Superintelligence is not omnipotence.</i></b> Separately, we should not assume that superintelligence must be omnipotent or omniscient. Superintelligence does not mean that an AI can instantly predict how events worldwide will unfold in the far future, nor that the system can completely predict the actions of all other agents with perfect accuracy. Likewise, it does not mean that the ASI could instantly overpower humanity. Moreover, many problems cannot be solved by intelligence or contemplation alone; research and development require real-world experimentation, which involves physical-world processes that take a long time, presenting a key constraint to AIs&#x2019; ability to influence the world. However, we know very little about what a system that is significantly smarter than humans could do. Therefore, it is difficult to make confident claims about superintelligence.</p>
<p class="nonindent1"><b><i>Superhuman performance in narrow areas is not the same as superintelligence.</i></b> A superintelligent AI would significantly outstrip human capabilities, potentially solving problems and making discoveries beyond our comprehension. Of course, this is not exclusive to superintelligence: even narrow AIs solve problems humans find difficult to understand. AlphaFold, for instance, astonished scientists by predicting the 3D structure of proteins&#x2014;a complex problem that stumped biochemists for decades. Ultimately, a superintelligence exceeds these other types of AI because of the breadth of the cognitive tasks in which it achieves superhuman performance.</p>
<p class="nonindent1"><b><i>Risks of superintelligence.</i></b> The risks associated with superintelligence are substantial. ASIs could be harder to control and even pose existential threats&#x2014;risks to the survival of humanity. That said, an AI system must not be superintelligent to be dangerous. An AGI, human-level AI, or narrow AI could all pose severe risks to humanity. These systems may vary in intelligence across different tasks and domains, but they can be dangerous at many levels of intelligence and generality. If a narrow AI is superhuman at a specific dangerous task like synthesizing viruses, it could be an extraordinary hazard for humanity.</p>
<h3 class="section"><i>Summary</i></h3>
<p class="nonindent">This section provided an introduction to artificial intelligence (AI), the broad umbrella that encompasses the area of computer science focused on creating machines that perform tasks typically associated with human intelligence. First, we discussed the nuances and difficulties of defining AI and detailed its history. Then, we explored AI systems in more detail and how they are often categorized into different types. Of these, we surveyed five commonly used terms &#x2014;narrow AI, human-level AI, artificial general intelligence, transformative AI, and superintelligence&#x2014;and highlighted some of their ambiguities. Considering specific capabilities and individual systems rather than broad categories or abstractions is often more informative.</p>
<p class="nonindent1">Next, we will narrow our focus to machine learning (ML), an approach within AI that emphasizes the development of systems that can learn from data. Whereas many classical approaches to AI relied on logical rules and formal, structured knowledge, ML systems use pattern recognition to extract information from data.</p>
<h2 class="section" id="sec2-2-3">2.2.3 Machine Learning</h2>
<h3 class="section0"><i>Overview and Definition</i></h3>
<p class="nonindent">Machine learning (ML) is a subfield of AI that focuses on developing computer systems that can learn directly from data without following explicit pre-set instructions [16, 17]. It accomplishes this by creating computational models that discern patterns and correlations within data. The knowledge encoded in these models allows them to inform decision-making or to reason about and act in the world. For instance, an email spam filter uses ML to improve its ability to distinguish spam from legitimate emails as it sees more examples. ML is the engine behind most modern AI applications, from personalized recommendations on streaming services to autonomous vehicles. One of the most popular and influential algorithmic techniques for ML applications is deep learning (DL), which uses deep neural networks to process data.</p>
<p class="nonindent1"><b><i>Machine learning algorithms.</i></b> An <i>algorithm</i> is a recipe for getting something done&#x2014;a procedure for solving a problem or accomplishing a task, often expressed in a precise programming language. Machine learning (ML) models are algorithms designed to learn from data by identifying patterns and relationships, which enables them to make predictions or decisions as they process new inputs. They often learn from information called training data. What makes ML models different from other algorithms is that they automatically learn patterns in data without explicit task-specific instructions. Instead, they identify correlations, dependencies, or relationships in the data and use this information to make predictions or decisions about new data; for instance, a content curation application may use ML algorithms to refine its recommendations.</p>
<p class="nonindent1"><b><i>Benefits of ML.</i></b> One of the key benefits of ML is its ability to automate complicated tasks, enabling humans to focus on other activities. Developers use ML for applications from medical diagnosis and autonomous vehicles to financial forecasting and writing. ML is becoming increasingly important for businesses, governments, and other organizations to stay competitive and make empirically informed decisions.</p>
<p class="nonindent1"><b><i>Guidelines for understanding ML models.</i></b> ML models can be intricate and varied, making understanding their characteristics and distinctions a challenge. It can be helpful to focus on key high-level aspects that almost all ML systems have:</p>
<ul class="bull"><li><b>General Task: What is the primary goal of the ML model?</b> We design models to achieve objectives. Some example tasks are predicting housing prices, generating images or text, or devising strategies to win a game.</li>
<li><b>Inputs: What data does the ML system receive?</b> This is the information that the model processes to deliver its results.</li>
<li><b>Outputs: What does the ML system produce?</b> The model generates these results, predictions, or decisions based on the input data.</li>
<li><b>Type of Machine Learning: What technique is used to accomplish the task?</b> This describes how the model converts its inputs into outputs (called inference), and learns the best way to convert its inputs into outputs (a learning process called training). An ML system can be categorized by how it uses training data, what type of output it generates, and how it reaches results.</li>
</ul>
<p class="nonindent1">The rest of this section delves deeper into these aspects of ML systems.</p>
<h3 class="section"><i>Key ML Tasks</i></h3>
<p class="nonindent1">In this section, we will explore four fundamental ML tasks&#x2014;classification, regression, anomaly detection, and sequence modeling&#x2014;that describe different problems or types of problems that ML models are designed to solve.</p>
<h4 class="section"><b><i>Classification</i></b></h4>
<p class="nonindent"><b><i>Classification is predicting categories or classes.</i></b> In classification tasks, models use characteristics or <i>features</i> of an input data point (example) to determine which specific category the data point belongs to. In medical diagnostics, a classification model might predict whether a tumor is cancerous or benign based on features such as a patient&#x0027;s age, tumor size, and tobacco use. This is an example of <i>binary classification</i>&#x2014;the special case in which models predict one of two categories. <i>Multi-class classification,</i> on the other hand, involves predicting one of multiple categories. An image classification model might classify an image as belonging to one of multiple different classes such as dog, cat, hat, or ice cream. <i>Computer vision</i> often applies these methods to enable computers to interpret and understand visual data from the world. Classification is categorization: it involves putting data points into buckets.</p>
<figure id="fig:swiss_cheese">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/binary_classification_v2.png" class="tb-img-full"/>
<p class="tb-caption">Figure 2.1 ML models can classify data into different categories.</p>
</figure>
<p class="nonindent1"><b><i>The sigmoid function produces probabilistic outputs.</i></b> A sigmoid is one of several mathematical functions used in classification to transform general real numbers into values between 0 and 1. Suppose we wanted to predict the likelihood that a student will pass an exam or that a prospective borrower will default on a loan. The sigmoid function is instrumental in settings like these&#x2014;problems that rely on computing probabilities. As a further example, in binary classification, one might use a function like the sigmoid to estimate the likelihood that a customer makes a purchase or clicks on an advertisement. However, it is important to note that other widely used models can provide similar probabilistic outputs without employing a sigmoid function.</p>
<h4 class="section"><b><i>Regression</i></b></h4>
<p class="nonindent"><b><i>Regression is predicting numbers.</i></b> In regression tasks, models use features of input data to predict numerical outputs. A real estate company might use a regression model to predict house prices from a dataset with features such as location, square footage, and number of bedrooms. While classification models produce <i>discrete</i> outputs that place inputs into a finite set of categories, regression models produce <i>continuous</i> outputs that can assume any value within a range. Therefore, regression is predicting a continuous output variable based on one or more input variables. Regression is estimation: it involves guessing what a feature of a data point will be given the rest of its characteristics.</p>
<p class="nonindent1"><b><i>Linear regression.</i></b> One type of regression, linear regression, assumes a linear relationship between features and predicted values for a target variable. A linear relationship means that the output changes at a constant rate with respect to the input variables, such that plotting the input-output relationship on a graph forms a straight line. Linear regression models are often helpful but have many limitations; for instance, their assumption that the features and the target variable are linearly related is often false. In general, linear regression can struggle with modeling complicated data patterns in the real world since they are roughly only as complex as their input variables and struggle to add additional structures themselves.</p>
<figure id="fig:swiss_cheese">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/logistic_regression_v2.png" class="tb-img-full"/>
<p class="tb-caption">Figure 2.2 Binary classification can use a sigmoid function to turn real numbers (such as hours of studying) into probabilities between zero and one (such as the probability of passing).</p>
</figure>
<h4 class="section"><b><i>Anomaly Detection</i></b></h4>
<p class="nonindent"><b><i>Anomaly detection is the identification of outliers or abnormal data points [18].</i></b> Anomaly detection is vital in identifying hazards, including unexpected inputs, attempted cyberattacks, sudden behavioral shifts, and unanticipated failures. Early detection of anomalies can substantially improve the performance of models in real-world situations.</p>
<p class="nonindent1"><b><i>Black swan detection is an essential problem within anomaly detection.</i></b> Black swans are unpredictable and rare events with a significant impact on the broader world. These events are difficult to predict because they may not have happened before, so they are not represented in the training data that ML models use to extrapolate the future. Due to their extreme and uncommon nature, such events make anomaly detection challenging. In section 4.7 in the Safety Engineering chapter, we discuss these ideas in more detail.</p>
<h4 class="section"><b><i>Sequence Modeling</i></b></h4>
<p class="nonindent"><b><i>Sequence modeling is analyzing and predicting patterns in sequential data.</i></b> Sequence modeling is a broadly defined task that involves processing or predicting data where temporal or sequential order matters. It may be applied to time-series data or natural language text to capture dependencies between items in the sequence to forecast future elements. An integral part of this process is <i>representation learning,</i> where models learn to convert raw data into more informative formats for the task at hand. Language models use these techniques to predict subsequent words in a sequence, transforming previous words into meaningful representations to detect patterns and make predictions. There are several major subtypes of sequence modeling. Here, we will discuss two: <i>generative modeling</i> and <i>sequential decision-making. Generative modeling.</i> Generative modeling is a subtype of sequence modeling that creates new data that resembles the input data, thereby drawing from the same distribution of features (conditioned on specific inputs). It can generate new outputs from many input types, such as text, code, images, and protein sequences.</p>
<figure id="fig:swiss_cheese">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/linear_regression_v2.png" class="tb-img-full"/>
<p class="tb-caption">Figure 2.3 This linear regression model is the best linear predictor of an output(umbrellas sold) using only information from the input (precipitation).</p>
</figure>
<p class="nonindent1"><i>Sequential decision-making (SDM).</i> SDM equips a model with the capability to make informed choices over time, considering the dynamic and uncertain nature of real-world environments. An essential feature of SDM is that prior decisions can shape later ones. Related to SDM is <i>reinforcement learning (RL),</i> where a model learns to make decisions by interacting with its environment and receiving feedback through rewards or penalties. An example of SDM in complex, real-world tasks is a robot performing a sequence of actions based on its current understanding of the environment.</p>
<figure>
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/anomaly_scatter.png" class="tb-img-full" style="width: 75%"/>
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/anomaly_time_v2.png" class="tb-img-full" style="width: 75%"/>
<p class="tb-caption">FIGURE 2.4. The first graph shows the detection of atypical user activity. The second graph shows the detection of unusually high energy usage. In both cases, the model detects anomalies [19].</p>
</figure>
<h3 class="section"><i>Types of Input Data</i></h3>
<p class="nonindent">In machine learning, a <i>modality</i> refers to how data is collected or represented&#x2014;the type of input data. Some models, such as image recognition models, use only one type of input data. In contrast, <i>multimodal</i> systems integrate information from multiple modalities (such as images and text) to improve the performance of learning-based approaches. Humans are naturally multimodal, as we experience the world by seeing objects, hearing sounds, feeling textures, smelling odors, tasting flavors, and more.</p>
<p class="nonindent1">Below, we briefly describe the significant modalities in ML. However, this list is not exhaustive. Many specific types of inputs, such as data from physical sensors, fMRI scans, topographic maps, and so on, do not fit easily into this categorization.</p>
<ul class="bull"><li><b>Tabular data:</b> Structured data is stored in rows and columns, usually with each row corresponding to an observation and each column representing a variable in the dataset. An example is a spreadsheet of customer purchase histories.</li></ul>
<ul class="bull"><li><b>Text data:</b> Unstructured textual data in natural language, code, or other formats. An example is a collection of posts and comments from an online forum.</li></ul>
<ul class="bull"><li><b>Image data:</b> Digital representations of visual information that can train ML models to classify images, segment images, or perform other tasks. An example is a database of plant leaf images for identifying species of plants.</li></ul>
<ul class="bull"><li><b>Video data:</b> A sequence of visual information over time that can train ML models to recognize actions, gestures, or objects in the footage. An example is a collection of sports videos for analyzing player movements.</li></ul>
<ul class="bull"><li><b>Audio data:</b> Sound recordings, such as speech or music. An example is a set of voice recordings for training speech recognition models.</li></ul>
<ul class="bull"><li><b>Time-series data:</b> Data collected over time that represents a sequence of observations or events. An example is historical stock price data.</li></ul>
<ul class="bull"><li><b>Graph data:</b> Data representing a network or graph structure, such as social networks or road networks. An example is a graph that represents user connections in a social network.</li></ul>
<ul class="bull"><li><b>Set-valued data:</b> Unstructured data in the form of collections of features or input vectors. An example is point clouds obtained from LiDAR sensors.</li></ul>
<h3 class="section"><i>Components of the ML Pipeline</i></h3>
<p class="nonindent">An <i>ML pipeline</i> is a series of interconnected steps in developing a machine learning model, from training it on data to deploying it in the real world. Next, we will examine these steps in turn.</p>
<p class="nonindent1"><b><i>Data collection.</i></b> The first step in building an ML model is data collection. Data can be collected in various ways, such as by purchasing datasets from owners of data or scraping data from the web. The foundation of any ML model is the dataset used to train it: the quality and quantity of data are essential for accurate predictions and performance.</p>
<p class="nonindent1"><b><i>Selecting features and labels.</i></b> After the data is collected, developers of ML models must choose what they want the model to do and what information to use. In ML, a <i>feature</i> is a specific and measurable part of the data used to make predictions or classifications. Most ML models focus on prediction. When predicting the price of a house, features might include the number of bedrooms, square footage, and the age of the house. Part of creating an ML model is selecting, transforming, or creating the most relevant features for the problem. The quality and type of features can significantly impact the model&#x0027;s performance, making it more or less accurate and efficient.</p>
<p class="nonindent1"><b><i>ML aims to predict labels.</i></b> A <i>label</i> (or a <i>target</i>) is the value we want to predict or estimate using the features. Labels in training data are only present in supervised ML tasks, discussed later in this section. Some models use a sample with correct labels to teach the model the output for a given set of input features: a model could use historical data on housing prices to learn how prices are related to features like square footage. However, other (unsupervised) ML models learn to make predictions using unlabelled input data&#x2014;without knowing the correct answers&#x2014;by identifying patterns instead.</p>
<p class="nonindent1"><b><i>Choosing an ML architecture.</i></b> After ML model developers have collected the data and chosen a task, they can process it. An ML <i>architecture</i> refers to a model&#x0027;s overall structure and design. It can include the type and configuration of the algorithm used and the arrangement of input and output layers. The architecture of an ML model shapes how it learns from data, identifies patterns, and makes predictions or decisions.</p>
<p class="nonindent1"><b><i>ML models have parameters.</i></b> Within an architecture, <i>parameters</i> are adjustable values within the model that influence its performance. In the house pricing example, parameters might include the weights assigned to different features of a house, like its size or location. During training, the model adjusts these weights, or parameters, to minimize the difference between its predicted house prices and the actual prices. The optimal set of parameters enables the model to make the best possible predictions for unseen data, generalizing from the training dataset.</p>
<p class="nonindent1"><b><i>Training and using the ML model.</i></b> Once developers have built the model and collected all necessary data, they can begin training and applying it. ML model <i>training</i> is adjusting a model&#x0027;s parameters based on a dataset, enabling it to recognize patterns and make predictions. During training, the model learns from the provided data and modifies its parameters to minimize errors.</p>
<p class="nonindent1"><b><i>Model performance can be evaluated</i></b> Model <i>evaluation</i> measures the performance of the trained model by testing it on data the model has never encountered before. Evaluating the model on unseen data helps assess its generalizability and suitability for the intended problem. We may try to predict housing prices for a new country beyond the original ML model&#x0027;s original training data.</p>
<p class="nonindent1"><b><i>Once ready, models are deployed.</i></b> Finally, once the model is trained and evaluated, it can be deployed in real-world applications. ML <i>deployment</i> involves integrating the model into a larger system, using it, and then maintaining or updating it as needed.</p>
<h3 class="section"><i>Evaluating ML Models</i></h3>
<p class="nonindent1">Evaluation is a crucial step in model development. When developing a machine learning model, it is essential to understand its performance. Evaluation&#x2014;the process of measuring the performance of a trained model on new, unseen data&#x2014;provides insight into how well the model has learned. We can use different metrics to understand a model&#x0027;s strengths, weaknesses, and potential for real-world applications. These quantitative performance measures are part of a broader context of goals and values that inform how we can assess the quality of a model.</p>
<h4 class="section"><b><i>Metrics</i></b></h4>
<p class="nonindent"><b><i>Accuracy is a measure of the overall performance of a classification model.</i></b> Accuracy is defined as the percentage of correct predictions:</p>
<p class="eq"><span class="math display"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>Accuracy</mtext><mo>=</mo><mfrac><mrow><mi mathvariant="normal">#</mi><mtext>&#xA0;of correct predictions</mtext></mrow><mrow><mi mathvariant="normal">#</mi><mtext>&#xA0;of total predictions</mtext></mrow></mfrac><mo>.</mo></math></span></p>
<p class="nonindent1">Accuracy can be misleading if there is an imbalance in the number of examples of each class. For instance, if 95% of emails received are not spam, a classifier assigning all emails to the &#x201C;not spam&#x201D; category could achieve 95% accuracy. Accuracy applies when there is a well-defined sense of right and wrong. Regression models focus on minimizing the error in their predictions.</p>
<p class="nonindent1"><b><i>Confusion matrices summarize the performance of classification algorithms.</i></b> A confusion matrix is an evaluative tool for displaying different prediction errors. It is a table that compares a model&#x0027;s predicted values with the actual values. For example, the performance of a binary classifier can be represented by a 2 x 2 confusion matrix, as shown in Figure 2.5. In this context, when making predictions, there are four possible outcomes:</p>
<ol class="num">
<li><b>True positive (TP):</b> A true positive is a correct prediction of the positive class.</li>
<li><b>False positive (FP):</b> A false positive is an incorrect prediction of the positive class, predicting positive instead of negative.</li>
<li><b>True negative (TN):</b> A true negative is a correct prediction of the negative class.</li>
<li><b>False negative (FN):</b> A false negative is an incorrect prediction of the negative class, predicting negative instead of positive.</li>
</ol>
<figure id="fig:swiss_cheese">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/predicted-actual-green-purple.png" class="tb-img-full"/>
<p class="tb-caption">Figure 2.5 A confusion matrix shows the four possible outcomes from a prediction: true positive, false positive, false negative, and true negative.</p>
</figure>
<p class="nonindent1">Since each prediction must be in one of these categories, the number of total predictions will be the sum of the number of predictions in each category. The number of correct predictions will be the sum of true positives and true negatives. Therefore,</p>
<p class="eq"><span class="math display"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>Accuracy</mtext><mo>=</mo><mfrac><mrow><mtext>TP</mtext><mo>+</mo><mtext>TN</mtext></mrow><mrow><mtext>TP</mtext><mo>+</mo><mtext>TN</mtext><mo>+</mo><mtext>FP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></math></span></p>
<p class="nonindent1"><b><i>False positives vs. false negatives.</i></b> The impact of false positives and false negatives can vary greatly depending on the setting. Which metric to choose depends on the specific context and the error types one most wants to avoid. In cancer detection, while a false positive (incorrectly identifying cancer in a cancer-free patient) may cause emotional distress, unnecessary further testing, and potentially invasive procedures for the patient, a false negative can be much more dangerous: it may delay diagnosis and treatment that allows cancer to progress, reducing the patient&#x0027;s chances of survival. By contrast, an autonomous vehicle with a water sensor that senses roads are wet when they are dry (predicting false positives) might slow down and drive more cautiously, causing delays and inconvenience, but one that senses the road is dry when it is wet (false negatives) might end up in serious road accidents and cause fatalities.</p>
<p class="nonindent1">While accuracy assigns equal cost to false positives and false negatives, other metrics isolate one or weigh the two differently and might be more appropriate in some settings. <i>Precision</i> and <i>recall</i> are two standard metrics that measure the extent of the error attributable to false positives and false negatives, respectively.</p>
<p class="nonindent1"><i>Precision measures the correctness of a model&#x0027;s positive predictions.</i> This metric represents the fraction of positive predictions that are actually correct. It is calculated as <span class="inline"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mtext>TP</mtext><mrow><mtext>TP</mtext><mo>+</mo><mtext>FP</mtext></mrow></mfrac></math></span>, dividing true positives (hits) by the sum of true positives and false positives. High precision implies that when a model predicts a positive class, it is usually correct&#x2014;but it might incorrectly classify many positives as negatives as well. Precision is like the model&#x0027;s aim: when the system says it hit, how often is it right?</p>
<figure id="fig:swiss_cheese">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/true-false-positives-green-purple.png" class="tb-img-full"/>
<p class="tb-caption">Figure 2.6 Precision measures the correctness of positive predictions and penalizes false positives, while recall measures how many positives are detected and penalizes false negatives [20].</p>
</figure>
<p class="nonindent1"><i>Recall measures a model&#x0027;s breadth.</i> On the other hand, recall measures how good a model is at finding all of the positive examples available. It is like the model&#x0027;s net: how many real positives does it catch? It is calculated as <span class="inline"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mtext>TP</mtext><mrow><mtext>TP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></math></span>, signifying the fraction of real positives that the model successfully detected. High recall means a model is good at recognizing or &#x201C;recalling&#x201D; positive instances, but not necessarily that these predictions are accurate. Therefore, a model with high recall may incorrectly classify many negatives as positives.</p>
<p class="nonindent1">In simple terms, precision is about a model being right when it makes a guess, and recall is about the model finding as many of the right answers as possible. Together, these two metrics provide a way to quantify how accurately and effectively a model can detect positive examples. Moreover, there is a trade-off between precision and recall: for a given model, increasing precision will necessarily decrease recall and vice versa.</p>
<p class="nonindent1"><b><i>AUROC scores measure a model&#x0027;s discernment.</i></b> The AUROC (Area Under the Receiver Operating Characteristic) score measures how well a classification model can distinguish between different classes. The ROC curve shows the performance of a classification model by plotting the rate of true positives against false positives as thresholds in a model are changed. AUROC scores range from zero to one, where a score of 50% indicates random-chance performance and 100% indicates perfect performance. To determine whether examples are positive (belong to a certain class) or negative (do not belong to a certain class), a classification model will assign a score to each example and compare that score to a threshold or benchmark value. We can interpret the AUROC as the probability that a positive example scores higher than a negative example.</p>
<figure id="fig:swiss_cheese">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/roc_curve_v2.png" class="tb-img-full"/>
<p class="tb-caption">Figure 2.7 The area under the ROC curve (AUROC) increases as it moves in the upper left direction, with more true positives and fewer false positives [21].</p>
</figure>
<p class="nonindent1">Since it considers performance at all possible decision thresholds, the AUROC is useful for comparing the performance of different classifiers. The AUROC is also helpful in cases of imbalanced data, as it does not depend on the ratio of positive to negative examples.</p>
<p class="nonindent1"><b><i>Mean squared error (MSE) quantifies how &#x201C;wrong&#x201D; a model&#x0027;s predictions are.</i></b> Mean squared error is a valuable and popular metric of prediction error. It is found by taking the average of the squared differences between the model&#x0027;s predictions and the labels, thereby ensuring that positive and negative deviations from the truth are penalized the same and that larger mistakes are penalized heavily. The MSE is the most popular loss function for regression problems.</p>
<p class="nonindent1"><b><i>Reasonably vs. reliably solved.</i></b> The distinction between <i>reasonable</i> and <i>reliable</i> solutions can be instrumental in developing a machine learning model, evaluating its performance, and thinking about tradeoffs between goals. A task is reasonably solved if a model performs well enough to be helpful in practice, but it may still have consistent limitations or make errors. A task is reliably solved if a model achieves sufficiently high accuracy and consistency for safety-critical applications. While models that reasonably solve problems may be sufficient in some settings, they may cause harm in others. Chatbots currently give reasonable results, which is frustrating but essentially harmless. However, if autonomous vehicles show reasonable but not reliable results, people&#x0027;s lives are at stake.</p>
<p class="nonindent1"><b><i>Goals and Tradeoffs.</i></b> Above and beyond quantitative performance measures are multiple goals and values that influence how we can assess the quality of a machine learning model. These goals&#x2014;and the tradeoffs that often arise between them&#x2014;shape how models are judged and developed. One such goal is <i>predictive power,</i> which measures the amount of error in predictions. Inference <i>time</i> (or <i>latency</i>) measures how quickly a machine learning model can produce results from input data&#x2014;in many applications, such as self-driving cars, prediction speed is crucial. <i>Transparency</i> refers to the interpretability of a machine learning model&#x0027;s inner workings and how well humans can understand its decision-making process. <i>Reliability</i> assesses the consistency of a model&#x0027;s performance over time and in varying conditions. <i>Scalability</i> is the capacity of a model to maintain or improve its performance as a key variable&#x2014;compute, parameter count, dataset size, and so on&#x2014;scales.</p>
<p class="nonindent1">Sometimes, these goals are in opposition, and improvements in one area can come at the cost of declines in others. Therefore, developing a machine learning model requires careful consideration of multiple competing goals.</p>
<h2 class="section" id="sec2-2-4">2.2.4 Types of Machine Learning</h2>
<p class="nonindent1"><b><i>One key dimension along which ML approaches vary is the degree of supervision.</i></b> We can divide ML approaches into groups based on how they use the training data and what they produce as an output. In ML, <i>supervision</i> is the process of guiding a model&#x0027;s learning with some kind of label. The model uses this label as a kind of <i>ground truth</i> or <i>gold standard</i> : a signal that can be trusted as accurate and used to supervise the model to achieve the intended results better. Labels can allow the model to capture relationships between inputs and their corresponding outputs more effectively. Supervision is often vital to help models learn patterns and predict new, unseen data accurately.</p>
<p class="nonindent1">There are distinct approaches in machine learning for dealing with different amounts of supervision. Here, we will explore three key approaches: supervised, unsupervised, and reinforcement learning. We will also discuss deep learning, a set of techniques that can be applied in any of these settings.</p>
<figure id="fig:swiss_cheese">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/learning_paradigms.png" class="tb-img-full"/>
<p class="tb-caption">Figure 2.8 The three main types of learning paradigms in machine learning are supervised learning, unsupervised learning, and reinforcement learning.</p>
</figure>
<h4 class="section"><b><i>Supervised Learning</i></b></h4>
<p class="nonindent"><b><i>Supervised learning is learning from labeled data.</i></b> Supervised learning is a type of machine learning that uses a labeled dataset to learn the relationship between input data and output labels. These labels are almost always human-generated: people will go through examples in a dataset and give each one a label. They might be shown pictures of dogs and asked to label the breed. The training process involves iteratively adjusting the model&#x0027;s parameters to minimize the difference between predicted outputs and the true output labels in the training data. Once trained, the model can predict new, unlabeled data.</p>
<p class="nonindent1"><b><i>Examples of supervised learning.</i></b> Some examples of these labeled inputs and outputs include mapping a photo of a plant to its species, a song to its genre, or an email to either &#x201C;spam&#x201D; or &#x201C;not spam.&#x201D; A computer can use a set of dog pictures labeled by humans to predict the breed of any dog in any given image. Supervised learning is analogous to a practice book, which offers a student a series of questions (inputs) and then provides the answers (outputs) at the end of the book. This book can help the student (like an ML model) find the correct answers when given new questions. Without instruction or guidance, the student must learn to answer questions correctly by reviewing the problems and checking their answers. Over time, they learn and improve through this checking process.</p>
<p class="nonindent1"><b><i>Advantages and disadvantages.</i></b> Supervised learning can excel in classification and regression tasks. Furthermore, it can result in high accuracy and reliable predictions when given large, labeled datasets with well-defined features and target variables. However, this method performs poorly on more loosely defined tasks, such as generating poems or new images. Supervision may also require manual labeling for the training process, which can be prohibitively time-consuming and costly. Critically, supervised learning is bottlenecked by the amount of labels, which can often result in less data available than when using unsupervised learning.</p>
<h4 class="section"><b><i>Unsupervised Learning</i></b></h4>
<p class="nonindent"><b><i>Unsupervised learning is learning from unlabeled data.</i></b> Unsupervised learning involves training a model on a dataset without specific output labels. Instead of matching its inputs to the correct labels, the model must identify patterns within the data to help it understand the underlying relationships between the variables. As no labels are provided, a model is left to its own devices to discover valuable patterns in the data. In some cases, a model leverages these patterns to generate supervisory signals, guiding its own training. For this reason, unsupervised learning can also be called self-supervised learning.</p>
<p class="nonindent1"><b><i>Examples of unsupervised learning.</i></b> Language models use unsupervised learning to learn patterns in language using large datasets of unlabeled text. LLMs often learn to predict the next word in a sentence, which enables the models to understand context and language structure without explicit labels like word definitions and grammar instructions. After a model trains on this task, it can apply what it learned to downstream tasks like answering questions or summarizing texts.</p>
<figure id="fig:swiss_cheese">
<img src="https://raw.githubusercontent.com/WilliamHodgkins/AISES/main/images/information-13-00071-g001.png" class="tb-img-full"/>
<p class="tb-caption">Figure 2.9 In image inpainting, models are trained to predict hidden parts of images, causing them to learn relationships between pixels [22].</p>
</figure>
<p class="nonindent1"><b><i>ML models exist on a spectrum of supervision.</i></b> Unsupervised and supervised learning are valuable concepts for thinking about ML models, not a dichotomy with a clear dividing line. Therefore, ML models are on a continuum of supervision, from datasets with clear labels for every data point on one extreme to datasets with no labels on the other. In between lies partial or weak supervision, which provides incomplete or noisy labels such as hashtags loosely describing features of images. This is analogous to a practice book where some solution pages are excessively brief, have errors, or are omitted entirely.</p>
<p class="nonindent1"><b><i>We can reframe many tasks into different types of ML.</i></b> Anomaly detection is typically framed as an unsupervised task that identifies unusual data points without labels. However, it can be refashioned as a supervised classification problem, such as labeling financial transactions as &#x201C;fraudulent&#x201D; or &#x201C;not fraudulent.&#x201D; Similarly, while stock price prediction is usually approached as a supervised regression task, it could be reframed as a classification task in which a model predicts whether a stock price will increase or decrease. The choice in framing depends on the task&#x0027;s specific requirements, the data available, and which frame gives a more useful model. Ultimately, this flexibility allows us to better cater to our goals and problems.</p>
<h4 class="section"><b><i>Reinforcement Learning</i></b></h4>
<p class="nonindent"><b><i>Reinforcement learning (RL) is learning from agent-gathered data.</i></b> Reinforcement learning focuses on training artificial agents to make decisions and improve their performance based on responses from their environment. It assumes that tasks can be modeled as goals to be achieved by an agent maximizing rewards from its environment. RL is distinctive since it does not require pre-collected data, as the agent can begin with no information and interact with its environment to learn new things and acquire new data.</p>
<p class="nonindent1"><b><i>Examples of RL.</i></b> RL can help robots learn to navigate an unknown environment by taking actions and receiving feedback in the form of rewards or penalties based on performance. Through trial and error, agents learn to make better decisions and maximize rewards by adjusting their actions or improving their model of the environment. It refines its strategy based on the consequences of its activities. RL enables agents to learn techniques and decision-making skills through interaction with their environment, which can adapt to dynamic and uncertain situations. However, it requires a well-designed reward function and can be computationally expensive, especially for complex environments with many possibilities for states and actions.</p>
<h4 class="section"><b><i>Deep Learning</i></b></h4>
<p class="nonindent"><b><i>Deep learning (DL) is a set of techniques that can be used in many learning settings.</i></b> Deep learning uses neural networks with many layers to create models that can learn from large datasets. <i>Neural networks</i> are the building blocks of deep learning models and use layers of interconnected nodes to transform inputs into outputs. The structure and function of biological neurons loosely inspired their design. Deep learning is not a new distinct learning type but rather a computational approach that can accomplish any of the three types of learning discussed above. It is most applicable to unsupervised learning tasks as it can perform well without any labels; for instance, a deep neural network trained for object recognition in images can learn to identify patterns in the raw pixel data.</p>
<p class="nonindent1"><b><i>Advantages and challenges in DL.</i></b> Deep learning excels in handling high-dimensional and complex data, providing critical capabilities in image recognition, natural language processing, and generative modeling. In ML, <i>dimensionality</i> denotes the number of features or variables in the data, each representing a unique dimension. High-dimensional data has many features, as in image recognition, where each pixel can be a feature. However, deep learning also requires vast data and substantial computational power. Moreover, the models can be challenging to interpret.</p>
<h3 class="section"><i>Conclusion</i></h3>
<p class="nonindent1">AI is one of the most impactful and rapidly developing fields of computer science. Artificial intelligence involves developing computer systems that can perform tasks that typically require human intelligence, from visual perception to decision-making.</p>
<p class="nonindent1">Machine learning is an approach to AI that involves developing models that can learn from data to perform tasks without being explicitly programmed. A robust approach to understanding any machine learning model is breaking it down into its fundamental components: the task, the input data, the output, and the type of machine learning it uses. Different approaches to ML offer various ways to tackle complex tasks and solve real-world problems. Deep learning is a powerful and popular method that uses many-layered neural networks to identify intricate patterns in large datasets. The following section will delve deeper into deep learning and its applications in artificial intelligence.</p>
<h2 class="section">References</h2>
<p class="ref">[1] Stuart J Russell and Peter Norvig. <i>Artificial Intelligence: A Modern Approach.</i> Pearson, 2020.</p>
<p class="ref">[2] Shane Legg and Marcus Hutter. &#x201C;Universal intelligence: A definition of machine intelligence&#x201D;. In: <i>Minds and Machines</i> 17 (2007), pp. 391&#x2013;444.</p>
<p class="ref">[3] Alan M. Turing. &#x201C;Computing Machinery and Intelligence&#x201D;. In: <i>Mind</i> 59 (236 1950), pp. 433&#x2013;460.</p>
<p class="ref">[4] F. Rosenblatt. &#x201C;The perceptron: A probabilistic model for information storage and organization in the brain&#x201D;. In: <i>Psychological Review</i> 65.6 (1958), pp. 386&#x2013; 408. issn: 0033-295X. doi: 10.1037/h0042519. url: <a href="http://dx.doi.org/10.1037/h0042519">http://dx.doi.org/10.1037/h0042519</a>.</p>
<p class="ref">[5] Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu. &#x201C;Deep blue&#x201D;. In: <i>Artificial intelligence</i> 134.1-2 (2002), pp. 57&#x2013;83.</p>
<p class="ref">[6] Daphne Koller and Nir Friedman. <i>Probabilistic graphical models: principles and techniques</i>. MIT Press, 2009.</p>
<p class="ref">[7] Leo Breiman. &#x201C;Random Forests&#x201D;. In: <i>Machine Learning</i> 20 (3 1995), pp. 273&#x2013; 297. url: <a href="https://doi.org/10.1023/a">https://doi.org/10.1023/a</a>:1010933404324.</p>
<p class="ref">[8] Jerome H. Friedman. &#x201C;Greedy Function Approximation: A Gradient Boosting Machine&#x201D;. In: <i>The Annals of Statistics</i> 29 (5 2001), pp. 1189&#x2013;232. url: <a href="http://www.jstor.org/stable/2699986">http://www.jstor.org/stable/2699986</a>.</p>
<p class="ref">[9] Corinna Cortes and Vladimir Vapnik. &#x201C;Support-Vector Networks&#x201D;. In: <i>Machine learning</i> 20 (3).</p>
<p class="ref">[10] Jia Deng et al. &#x201C;ImageNet: A large-scale hierarchical image database&#x201D;. In: <i>2009 IEEE Conference on Computer Vision and Pattern Recognition</i>. 2009, pp. 248&#x2013;255. doi: 10.1109/CVPR.2009.5206848.</p>
<p class="ref">[11] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. &#x201C;ImageNet Classification with Deep Convolutional Neural Networks&#x201D;. In: <i>Advances in Neural Information Processing Systems</i>. Ed. by F. Pereira et al. Vol. 25. Curran Associates, Inc., 2012. url: <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>.</p>
<p class="ref">[12] David Silver et al. &#x201C;Mastering the game of Go with deep neural networks and tree search&#x201D;. In: <i>Nature</i> 529 (Jan. 2016), pp. 484&#x2013;489. doi: 10.1038/nature16 961.</p>
<p class="ref">[13] Alec Radford et al. &#x201C;Language Models are Unsupervised Multitask Learners&#x201D;. In: 2019. url: <a href="https://api.semanticscholar.org/CorpusID">https://api.semanticscholar.org/CorpusID</a>:160025533.</p>
<p class="ref">[14] Nick Bostrom. &#x201C;Superintelligence: Paths, Dangers, Strategies&#x201D;. In: <i>Oxford University Press</i> (2014).</p>
<p class="ref">[15] Meredith Ringel Morris et al. <i>Levels of AGI for Operationalizing Progress on the Path to AGI</i>. 2024. arXiv: 2311.02462 [cs.AI].</p>
<p class="ref">[16] Christopher M. Bishop. &#x201C;Pattern Recognition and Machine Learning&#x201D;. In: (2016).</p>
<p class="ref">[17] Kevin P. Murphy. <i>Probabilistic Machine Learning: An Introduction</i>. MIT Press, 2022.</p>
<p class="ref">[18] Dan Hendrycks and Kevin Gimpel. <i>A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</i>. 2018. arXiv: 1610.02136 [cs.NE].</p>
<p class="ref">[19] Dan Hendrycks. <i>Anomaly and Out-of-Distribution Detection</i>. url: <a href="https://docs.google.com/presentation/d/1WEzSFUbcl1Rp4kQq1K4uONMJHBAUWhCZTzWVHnLcSV8/edit#slide=id.g60c1429d79_0_0">https://docs.google.com/presentation/d/1WEzSFUbcl1Rp4kQq1K4uONMJHBAUWhCZTzWVHnLcSV8/edit#slide=id.g60c1429d79_0_0</a>.</p>
<p class="ref">[20] Walber. <i>Precision Recall</i>. Creative Commons Attribution-Share Alike 4.0 International license. The layout of the figure has been changed. 2014. url: <a href="https://en.wikipedia.org/wiki/File">https://en.wikipedia.org/wiki/File</a>:Precisionrecall.svg.</p>
<p class="ref">[21] cmglee and Martin Thoma. Creative Commons Attribution-Share Alike 4.0 International license. The colours of the figure have been changed. 2018. url: <a href="https://commons.wikimedia.org/wiki/File">https://commons.wikimedia.org/wiki/File</a>:Roc_curve.svg.</p>
<p class="ref">[22] Haiyin Luo and Yuhui Zheng. &#x201C;Semantic Residual Pyramid Network for Image Inpainting&#x201D;. In: <i>Information</i> 13.2 (Jan. 2022). Creative Commons Attribution 4.0 International license. url: <a href="https://doi.org/10.3390/info13020071">https://doi.org/10.3390/info13020071</a>.</p>
</div>
</body>
</html>
