<h1 id="motivation">Motivation</h1>
<h3 id="introduction">Introduction</h3>
<p>In the chapters “Complex Systems” and “Safety Engineering,” we
considered AI risks that arise not only from the technologies themselves
but from the broader social contexts in which they are embedded. In this
chapter, we extend our exploration of these systemic risks by exploring
how the collective behavior of a <em>multi-agent system</em> may not
reflect the interests of the agents that comprise it. The agents may
produce conditions that none of them wants, even when every one of them
has the same goals and priorities. In the words of economics Nobel
laureate Thomas Schelling, “Micromotives do not equal macrobehavior.”
<span class="citation" data-cites="Schelling1978MicromotivesAM"></span>.
Let us explore this idea using some examples.</p>
<p><strong>Traffic jams.</strong> Consider a traffic jam, where the only
obstacle to each motorist is the car in front. Everyone has the same
goal, which is to reach their destination quickly. Since nobody wants to
be stuck waiting, the solution might appear obvious to someone
unfamiliar with traffic: everyone should simply drive forward, starting
at the same time and accelerating at the same rate. And yet, without
external synchronization, achieving this preferable state is impossible.
All anyone can do is start and stop in response to each others’ starting
and stopping, inching towards their destination slowly and
haltingly.</p>
<p><strong>Tall forests.</strong> In the Rockefeller forest of Northern
California, the trees are more than 350 feet tall, on average. We can
model these trees as agents competing for sunlight access. The taller a
tree is, the more sunlight it can access, as its leaves are above its
neighbors’. However, there is no benefit to being tall other than
avoiding being overshadowed by other trees. In fact, growing so tall
costs each tree valuable resources and risks their structural integrity
failing. If all the trees were 200 feet shorter, each tree would occupy
the same position in the competition as before and each would get the
same amount of sunlight as before, but with greatly reduced growing
costs. All the trees would profit from this arrangement. However, as
there is no way to impose such an agreement between the trees, each
races its neighbor ever higher, and all pay the large costs of growing
so tall.</p>
<p><strong>Excessive working hours.</strong> Many people work far longer
hours than they might ideally like to, at the expense of their health
and their other interests. The usual motivation for this behavior is
competition for resources that require a high level of income in order
to access. For example, where higher-quality housing is limited, people
might compete for the best-paying jobs by working long hours, or they
might supplement their income by working multiple jobs. In theory, if
everyone competing for these resources were to halve their work hours,
they could free up time and increase their quality of life while
maintaining their relative position in the competition: each person
would get the housing quality they would have otherwise, and everyone
would benefit from this freed-up time and improved health. Yet no one
does this, because if they alone were to decrease their work efforts,
they would be out-competed by others who did not.</p>
<p><strong>Military arms races.</strong> Like tree height and work
hours, the major benefit of military power is not intrinsic, but
relative: being less militarily capable than their neighbors makes a
nation vulnerable to invasion. This competitive pressure drives nations
to expend vast sums of money on their military budgets each year,
reducing each nation’s budgets for other areas, such as healthcare and
education. Some forms of military investment, such as nuclear weaponry
and military AI applications, also exacerbate the risks of large-scale
catastrophes. If every nation were to decrease its military investment
by the same amount, everyone would benefit from the reduced expenses and
risks without anyone losing their relative power. However, this
arrangement is not stable, since each nation could improve its security
by ensuring its military power exceeds that of its competitors, and each
risks becoming vulnerable if it alone fails to do this. Military
expenditure therefore remains high in spite of these seemingly avoidable
costs.</p>
<p><strong>Competitive and evolutionary pressures.</strong> The same
basic structure underlies most of these examples, as follows<span
class="citation" data-cites="alexander2016meditations"></span>. A group
of agents is engaged in a competition over a valuable and limited item
(sunlight access, housing quality, military security). One way an agent
can gain more of this valuable item is by sacrificing some of their
other values (energy for growth, social life, an education budget).
Agents who do not make these sacrifices are outcompeted by those who do.
Natural selection weeds out those who do not sacrifice their other
values sufficiently, replacing them with agents who sacrifice more,
until the competition is dominated by those agents who sacrificed the
most. These agents gain no more of the valued item they are competing
for than did the original group, yet are worse off for the losses of
their other values.<br />
We can imagine this process as a never-ending race between 100 runners.
The runners are competing for rank order (their position relative to the
other runners - 1st, 2nd, 3rd etc). One day, one of the runners has an
idea: she realizes that she could improve her ranking by dropping her
water bottle to reduce her weight. She does this, and so manages to
overtake some of those ahead of her. Once these runners realize what she
did, they follow suit, soon overtaking her as well as some of those
previously ahead of them. The process continues until the original rank
order is reestablished, but now all the runners must continue running
without the ability to rehydrate. Later, another runner realizes that he
can improve his ranking by discarding his clothes to become more
aerodynamic. Though this is embarrassing and uncomfortable in the cold
air, he does this and thus moves forward up through the ranks. Other
runners then follow suit, and ultimately the original order is restored,
but now all the runners are cold and embarrassed, as well as dehydrated.
Each time this process occurs, the agents all ultimately end up with the
same rankings relative to each other, but their absolute wellbeing has
reduced.</p>
<p><strong>Steering each agent <span class="math inline">≠</span>
steering the system.</strong> These phenomena hint at the distinct
challenges of ensuring safety in multi-agent systems. The danger posed
by a collective of agents is greater than the sum of its parts. AI risk
cannot be eradicated by merely ensuring that each individual AI agent is
loyal and each individual human operator is well-intentioned. Even if
all agents, both human and AI, share a common set of goals, this does
not guarantee macrobehavior in line with these goals. The agents’
<em>interactions</em> can produce undesirable outcomes.</p>
<p><strong>Chapter focus.</strong> In this chapter, we use abstract
models to understand how intelligent agents can, despite acting
rationally and in accordance with their own self-interest, collectively
produce outcomes that none of them wants, even when they could seemingly
have achieved preferable alternative outcomes. We can characterize these
risks by crudely differentiating them into the following two sets:</p>
<ul>
<li><p><strong>Multi-human dynamics.</strong> These risks are generated
by interactions between the human agencies involved in AI development
and adoption, particularly corporations and nations. The central concern
here is that competitive and evolutionary pressures could drive humanity
to hand over increasing amounts of power to AIs, thereby becoming a
“second-class species.” The frameworks we explore in this chapter are
highly abstract and can be useful in thinking more generally about the
current AI landscape.</p>
<p>Of particular importance are “AI races.” We see these in the
corporate world, where AI developers are cutting corners on safety in
order to avoid being outcompeted by one another. We also see these in
international relations, where nations are racing each other to adopt
dangerous military AI applications. By observing AI races, we can
anticipate that merely persuading these parties that their actions are
high-risk may not be sufficient for ensuring that they act more
cautiously, because they may be willing to tolerate high risk levels in
order to “stay in the race.” For example, nations may chose to continue
investing in military AI technologies that could fail in catastrophic
ways, if abstaining from doing so risks losing international
conflict.</p></li>
<li><p><strong>Multi-AI dynamics.</strong> These risks are generated by
interactions with and between AI agents. In the future, we expect that
AIs will increasingly be granted autonomy in their behavior, and will
therefore interact with others under progressively less human oversight.
This poses risks in at least three ways. First, evolutionary pressures
may promote selfish and deceptive behavior, as well as generating
various forms of intrasystem conflict that could subvert our goals.
Second, many of the mechanisms by which AI agents may cooperate with one
another could promote undesirable behaviors, such as nepotism, outgroup
hostility, and the development of ruthless reputations. Third, AIs may
engage in conflict, using threats of extreme scale in order to extort
others, or even promoting all-out warfare, with devastating
consequences.</p></li>
</ul>
<p>We explore both of the above sets of multi-agent risks using
generalizable frameworks from game theory, evolutionary theory, and
bargaining theory. These frameworks help us understand the collective
dynamics that can lead to outcomes that were not intended or desired by
anyone individually. Even if AI systems are fully under human control
and leading actors such as corporations and states are well-intentioned,
humanity could still end up eroding away our power gradually until it
cannot be recovered.</p>
<h3 id="game-theory">Game Theory</h3>
<p><strong>Central theme: Rational agents will not necessarily secure
good outcomes.</strong> Behavior that is individually rational and
self-interested can produce collective outcomes that are suboptimal, or
even catastrophic, for all involved. This section first examines the
Prisoner’s Dilemma, a canonical game theoretic example that illustrates
this theme—though cooperation would produce an outcome that is better
for both agents, for either one to cooperate would be irrational.<br />
We then build on this by introducing three additional levels of
sophistication. The first addition is time. We explore how cooperation
is possible, though not assured, when agents interact repeatedly over
<em>time</em>. The second addition is the introduction of <em>more than
two agents</em>. We explore how collective action problems can generate
and maintain undesirable states. Third, we explore evolutionary game
theory, studying the dynamics produced by agents that can <em>adapt over
time</em>.<br />
Ultimately, we see how these natural dynamics can produce
catastrophically bad outcomes. They perpetuate military arms races and
corporate AI races, increasing the risks from both. They may also foster
immoral AI behaviors, such as deception and even extortion.</p>
<h3 id="evolutionary-pressure">Evolutionary Pressure</h3>
<p><strong>Central theme: Natural selection will promote AIs that behave
selfishly.</strong> In this section, we first explore generalized
Darwinism, which is the idea that evolution by natural selection can
take place outside of the realm of biology. We explore examples in
linguistics, music, philosophy and sociology. We formalize generalized
Darwinism using Lewontin’s conditions for evolution by natural selection
and the Price equation for evolutionary change. Using both, we show that
AIs are likely to be subject to evolution by natural selection: they
will vary in ways that produce differential fitness and so influence
which traits persist through time and between “generations” of
AIs.<br />
Next, we explore two AI risks generated by evolutionary pressures. The
first is that correctly-specified goals may be subverted or distorted by
“intrasystem goal conflict.” The interests of propagating information
(such as genes, departments, or sub-agents) can sometimes clash with
those of the larger entity that contains it (such as an organism,
government, or AI system), undermining unity of purpose. The second risk
we consider is that natural selection tends to favor selfish traits over
altruistic ones. A future shaped by evolutionary pressures is,
therefore, likely to be dominated by selfish behavior, both in the
institutions that produce and use AI systems, and in the AIs
themselves.<br />
The conclusions of this section are simple. Natural selection will by
default be a strong force in determining the state of the world. Its
influence on AI development carries the risk of intrasystem goal
conflict and the promotion of selfish behavior. Both risks could have
catastrophic effects. Intrasystem goal conflict could prevent our goals
from being carried out and generate unexpected actions. AI agents could
develop selfish tendencies, increasing the risk that they might employ
harmful strategies (including those covered earlier in the chapter, such
as deception or extortion).</p>
<h3 id="conflict-and-cooperation">Conflict and Cooperation</h3>
<p><strong>Central theme: Ensuring cooperation, rather than conflict, is
vital for AI safety.</strong> This section explores the nature of
conflict and cooperation between agents. We start by looking at the
nature of conflict, beginning with an overview of bargaining theory.
Then we explore 5 factors that drive conflict.</p>
<ol>
<li><p><em>Power shifts</em>: a shift in political power triggers
preventative conflict.</p></li>
<li><p><em>First strike advantage</em>: time-sensitive offensive
advantages motivate a party to initiate conflict preemptively.</p></li>
<li><p><em>Information problems</em>: mis- and dis-information kindle
defensive or offensive action over cooperation.</p></li>
<li><p><em>Issue indivisibility</em>: wherever the entity over which
parties are contesting is indivisible, it is harder to avoid resorting
to conflict.</p></li>
<li><p><em>Inequality</em>: inequality may increase the probability of
conflict, due to factors such as relative deprivation and social
envy.</p></li>
</ol>
<p><br />
Next, we turn to assessing how cooperation can help with addressing the
challenges outlined above and what problems it may pose itself. We
explore seven mechanisms that can promote or maintain cooperation:<br />
</p>
<ul>
<li><p><em>Direct reciprocity</em>: the chance of a future meeting
incentivizes cooperative behavior in the present.</p></li>
<li><p><em>Indirect reciprocity</em>: cooperative reputations are
rewarded.</p></li>
<li><p><em>Group selection</em>: inter-group competition promotes
intra-group unity.</p></li>
<li><p><em>Kin selection</em>: indirect benefits of cooperation outweigh
direct costs, motivating altruism towards genetic kin.</p></li>
<li><p><em>Individual stakes to common stakes</em>: the alignment of
interests between group members and the group as a whole.</p></li>
<li><p><em>Simon’s selection</em>: limited knowledge necessitates
reliance on others.</p></li>
<li><p><em>Institutions</em>: large-scale external forces motivate
cooperation through enforcement.</p></li>
</ul>
<h1 id="game-theory-1">Game Theory</h1>
<h2 id="overview">Overview</h2>
<p>This chapter explores the dynamics that may arise when AI and human
agents interact. These interactions create risks distinct from those
generated by any individual AI agent acting in isolation. One way we can
study the strategic interdependence of agents is with the framework of
<em>game theory</em>. Using game theory, we can examine formal models of
how agents interact with each other under varying conditions and predict
the outcomes of these interactions.<br />
Here, we use game theory to present natural dynamics in biological and
social systems that involve multiple agents. In particular, we explore
what might cause agents to come into conflict with one another, rather
than cooperate. We show how these multi-agent dynamics can generate
undesirable outcomes, sometimes for all the agents involved. We consider
risks created by interactions within and between human and AI agents,
from human-directed companies and militaries engaging in perilous races
to autonomous AIs using threats for extortion. The game theoretic
frameworks we present are most relevant where conditions are anarchic
and competitive. Thus, many of these risks can be reduced if incentives
are put in place that ensure human agencies and AI agents are able to
cooperate with one another.</p>
<p><strong>First, we provide an overview of the fundamentals of game
theory.</strong> We begin this section by setting out the
characteristics of game theoretic agents. We also categorize the
different kinds of games we are exploring.</p>
<p><strong>Second, we focus on the Prisoner’s Dilemma.</strong> The
Prisoner’s Dilemma is a simple example of how an interaction between two
agents can generate an equilibrium state that is bad for both, even when
each acts rationally and in their own self-interest. We explore how
agents may arrive at the outcome where neither chooses to cooperate. We
use this to model real-world phenomena, such as negative political
campaigns. Finally, we examine ways we might foster rational cooperation
between self-interested AI agents, such as by altering the values in the
underlying payoff matrices. The key upshot is that intelligent and
rational agents do not always achieve good outcomes.</p>
<p><strong>Third, we add in the element of time by examining the
Iterated Prisoner’s Dilemma.</strong> AI agents are unlikely to interact
with others only once. When agents engage with each other multiple
times, this creates its own hazards. We begin by examining how iterating
the Prisoner’s Dilemma alters the agents’ incentives–—when an agent’s
behavior in the present can influence that of their partner in the
future, this creates an opportunity for rational cooperation. We study
the effects of altering some of the variables in this basic model:
uncertainty about future engagement and the necessity to switch between
multiple different partners. We look at why the cooperative strategy
<em>tit-for-tat</em> is usually so successful, and in what circumstances
it is less so. Finally, we explore iterated multi-agent social dynamics
amongst humans, such as corporate AI races and military AI arms races.
The key upshot is that cooperation cannot be ensured merely by iterating
interactions through time.</p>
<p><strong>Fourth, we consider group-level interactions.</strong> AI
agents might not interact with others in a neat, pairwise fashion, as
assumed by the models previously explored. In the real world, social
behavior is rarely so straightforward. Interactions can take place
between more than two agents at the same time. A group of agents creates
an environmental structure that may alter the incentives directing
individual behavior. Human societies are rife with dynamics generated by
group-level interactions that result in undesirable outcomes. We begin
by formalizing “collective action problems.” We consider real-world
examples such as anthropogenic climate change and fishery depletion.
Multi-agent dynamics such as these generate AI risk in several ways.
Races between human agents and agencies could trigger flash wars between
AI agents or the automation of economies to the point of human
enfeeblement. The key upshot is that achieving cooperation and ensuring
collectively good outcomes is even more difficult in interactions
involving more than two agents.</p>
<p><strong>Finally, we explore how AI agents may adapt their behavior to
their environments.</strong> We use evolutionary game theory to
understand how social environments might shape multi-agent dynamics
involving humans and AIs. We start by considering how evolutionary
forces alter the dynamics in the Iterated Prisoner’s Dilemma: when
agents can adapt their strategies, the results are more complicated. As
we explore in a later section, they include the emergence of “cyclical
dynamics” and some undesirable strategies such as extortion. We use this
to examine the risk of AI extortion in the real world. We also consider
the “Hawk-Dove game” as an example of how agents may reach an
equilibrium state in which they employ a variety of different
strategies, even when they themselves are identical to one another. We
examine how some strategies might be selected over others in AI
populations. In particular, evolutionary pressures could promote
dangerous behaviors in AIs, such as concealing information. The key
upshot is that evolutionary dynamics might drive AIs to employ harmful
social strategies, such as deception, rather than cooperating.</p>
<h2 id="game-theory-fundamentals">Game Theory Fundamentals</h2>
<p>In this section, we briefly run through some of the fundamental
principles in game theory. Game theory is the branch of mathematics
concerned with agents’ choices and strategies in multi-agent
interactions. Game theory is so-called because we reduce complex
situations to abstract games where agents maximize their payoffs. Using
game theory, we can study how altering incentives influences the
strategies that these agents use.</p>
<p><strong>Agents in game theory.</strong> We usually assume that the
agents in these games are self-interested and rational. Agents are
“self-interested” if they make decisions in view of their own utility,
regardless of the consequences to others. Agents are said to be
“rational” if they act as though they are maximizing their utility.</p>
<p><strong>Games can be “zero sum” or “non-zero sum.”</strong> We can
categorize the games we are studying in different ways. One distinction
(made in Figure 1) is between zero sum and non-zero sum games. A
<strong>zero sum</strong> game is one where, in every outcome, the
agents’ payoffs all sum to zero. An example is “tug of war”: any benefit
to one party from their pull is necessarily a cost to the other.
Therefore, the total value of these wins and losses cancel out. In other
words, there is never any net change in total value. Poker is a zero sum
game if the players’ payoffs are the money they each finish with. The
total amount of money at a poker game’s beginning and end is the same —
it has simply been redistributed between the players.<br />
By contrast, many games are non-zero sum. In <em>non-zero</em> sum
games, the total amount of value is not fixed and may be changed by
playing the game. Thus, one agent’s win does not necessarily require
another’s loss. For instance, in cooperation games such as those where
players must meet at an undetermined location, players only get the
payoff together if they manage to find each other. As we shall see, the
Prisoner’s dilemma is a non-zero sum game, as the sum of payoffs change
across different outcomes.</p>
<p><strong>Non-zero sum games can have “positive sum” or “negative
sum”</strong> outcomes. We can categorize the outcomes of non-zero sum
games as <em>positive sum</em> and <em>negative sum</em>. In a positive
sum outcome, the total gains and losses of the agents sum to greater
than zero. Positive sum outcomes can arise when particular interactions
result in an increase in value. This includes instances of
mutually-beneficial cooperation. For example, if one agent has flour and
another has water and heat, the two together can cooperate to make
bread, which is more valuable than the raw materials. As a real-world
example, many view the stock market as positive sum because the overall
value of the stock market tends to increase over time. Though gains are
unevenly distributed, and some investors lose money, the average
investor becomes richer. This demonstrates an important point: positive
sum outcomes are not necessarily “win-win.” Cooperating does not
guarantee a benefit to all involved. Even if extra total value is
created, its distribution between the agents involved in its creation
can take any shape, including one where some agents have negative
payoffs.<br />
In a negative sum outcome, some amount of value is lost by playing the
game. Many competitive interactions in the real world are negative sum.
For instance, consider “oil wars” –— wars fought over a valuable
hydrocarbon resource. Oil wars are zero-sum with regards to oil since
only the distribution (not the amount) of oil changes. However, the
process of conflict itself incurs costs to both sides, such as loss of
life and infrastructure damage. This reduces the total amount of value.
If AI development has the potential to result in catastrophic outcomes
for humanity, then accelerating development to gain short-term profits
in exchange for long-term losses to everyone involved would be a
negative sum outcome.</p>
<h2 id="the-prisoners-dilemma">The Prisoner’s Dilemma</h2>
<p>Our aim in this section is to investigate how interactions between
rational agents, both human and AI, may negatively impact everyone
involved. To this end, we focus on a simple game: the Prisoner’s
Dilemma. We first explore how the game works, and its different possible
outcomes. We then examine why agents may choose not to cooperate even if
they know this will lead to a collectively suboptimal outcome. We run
through several real-world phenomena which we can model using the
Prisoner’s Dilemma, before exploring ways in which cooperation can be
promoted in these kinds of interactions. We end by briefly discussing
the risk of AI agents tending towards undesirable equilibrium
states.</p>
<h3 id="the-game-fundamentals">The Game Fundamentals</h3>
<p>In the Prisoner’s Dilemma, two agents must each decide whether or not
to cooperate. The costs and benefits are structured such that for each
agent, defection is the best strategy regardless of what their partner
chooses to do. This motivates both agents to defect.</p>
<p><strong>The Prisoner’s Dilemma.</strong> In game theory, the
<strong>Prisoner’s Dilemma</strong> is a classic example of the
decisions of rational agents leading to suboptimal outcomes. The basic
setup is as follows. The police have arrested two would-be thieves. We
will call them Alice and Bob. The suspects were caught breaking into a
house. The police are now detaining them in separate holding cells, so
they cannot communicate with each other. The police suspect that the
pair were planning <em>burglary</em> (which carries a lengthy jail
sentence). But they only have enough evidence to charge them with
<em>trespassing</em> (which carries a shorter jail sentence). However,
the testimony of either one of the suspects would be enough to charge
the other with burglary, so the police offer each suspect the following
deal. If only one of them rats out their partner by confessing that they
had intended to commit burglary, the confessor will be released with
<em>no jail time</em> and their partner will spend <em>eight years</em>
in jail. However, if they each attempt to rat out the other by both
confessing, they will both serve a medium prison sentence of <em>three
years</em>. If neither suspect confesses, they will both serve a short
jail sentence of only <em>one year</em>.</p>
<p><strong>The four possible outcomes.</strong> We assume that Alice and
Bob are both rational and self-interested: each only cares about
minimizing their own jail time. We define the decision facing each as
follows. They can either “cooperate” with their partner by remaining
silent or “defect” on their partner by confessing to burglary. Each
suspect faces four possible outcomes, which we can split into two
possible scenarios. Let’s term these “World 1” and “World 2”; see Figure
2. In World 1, their partner chooses to cooperate with them; in World 2,
their partner chooses to defect. In both scenarios, the suspect decides
whether to cooperate or defect themself. They do not know what their
partner will decide to do.<br />
</p>
<figure id="fig:pris-dillema">
<p><img src="images/multiagent-dynamics/image5.png" alt="image" /> <span
id="fig:pris-dillema" label="fig:pris-dillema"></span></p>
<figcaption>The possible outcomes for Alice in the Prisoner’s
Dilemma</figcaption>
</figure>
<p><em>A) The four possible outcomes facing Alice, divided into “World
1” and “World 2.” In both scenarios, Alice must choose whether to
cooperate with her partner (Bob), or defect. In World 1, her partner
chooses to cooperate; in World 2, her partner chooses to defect. The
jail time for each outcome is shown on the right. B) The four possible
outcomes are shown in a decision tree. In World 1, Alice gets a shorter
jail sentence by defecting, than by cooperating. The same is true in
World 2, by an even greater amount. In both cases, choosing to defect
gets Alice the better outcome.</em></p>
<p><strong>Defection is the dominant strategy.</strong> Alice does not
know whether Bob will choose to cooperate or defect. She does not know
whether she will find herself in World 1 or World 2; see Figure 2. She
can only decide whether to cooperate or defect herself. This means she
is making one of two possible decisions. If she defects, she is…<br />
</p>
<div class="blockquote">
<p>…in World 1: Bob cooperates and she goes free instead of spending a
year in jail.<br />
…in World 2: Bob defects and she gets a 3-year sentence instead of an
8-year one.<br />
</p>
</div>
<p>Alice only cares about minimizing her own jail time, so she can save
herself jail time in either scenario by choosing to defect. She saves
herself one year if her partner cooperates or five years if her partner
defects. A rational agent under these circumstances will do best if they
decide to defect, regardless of what they expect their partner to do. We
call this the <em>dominant strategy</em>: a rational agent playing the
Prisoner’s Dilemma should choose to defect <em>no matter what their
partner does</em>.<br />
One way to think about strategic dominance is through the following
thought experiment. Someone in the Arctic during winter is choosing what
to wear for that day’s excursion. They have only two options: a coat or
a t-shirt. The coat is thick and waterproof; the t-shirt is thin and
absorbent. Though this person cannot control or predict the weather,
they know there are only two possibilities: either rain or cold. If it
rains, the coat will keep them drier than the t-shirt. If it is cold,
the coat will keep them warmer than the t-shirt. Either way, the coat is
the better option, so “wearing the coat” is their dominant strategy.</p>
<p><strong>Defection is the dominant strategy for both agents.</strong>
Importantly, both the suspects face this decision in a symmetric
fashion. Each is deciding between identical outcomes, and each wishes to
minimize their own jail time. Let’s consider the four possible outcomes
now in terms of both the suspects’ jail sentences; see Figure 3A. We can
display this information in a <strong>payoff matrix</strong>, as shown
in Table <a href="#tab:payoff-matrix" data-reference-type="ref"
data-reference="tab:payoff-matrix">1</a>. Payoff matrices are commonly
used to visualize games. They show all the possible outcomes of a game
in terms of the value of that outcome for each of the agents involved.
In the Prisoner’s Dilemma, we show the decision outcomes as the payoffs
to each suspect: note that since more jail time is worse than less,
these payoffs are negative. Each cell of the matrix shows the outcome of
the two suspects’ decisions as the payoff to each suspect.<br />
</p>
<div id="tab:payoff-matrix">
<table>
<caption>A Prisoner’s Dilemma payoff matrix</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><span style="color: blue">Bob
cooperates</span></th>
<th style="text-align: center;"><span style="color: blue">Bob
defects</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span style="color: cyan">Alice
cooperates</span></td>
<td style="text-align: center;"><span style="color: cyan">-1</span>,
<span style="color: blue">-1</span></td>
<td style="text-align: center;"><span style="color: cyan">-8</span>,
<span style="color: blue">0</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span style="color: cyan">Alice
defects</span></td>
<td style="text-align: center;"><span style="color: cyan">0</span>,
<span style="color: blue">-8</span></td>
<td style="text-align: center;"><span style="color: cyan">-3</span>,
<span style="color: blue">-3</span></td>
</tr>
</tbody>
</table>
</div>
<p><em>Each cell of the matrix quantifies the decision outcome in terms
of the payoff to each: the numbers are negatives, because more jail time
represents a worse payoff. The color-coding matches the payoff to the
agent. For example, if Alice cooperates, and Bob defects, the outcome
secured is shown in the top right cell (-8, 0): this means Alice gets 8
years in jail, and Bob gets no jail time.</em></p>
<h3 id="nash-equilibria-and-pareto-efficiency">Nash Equilibria and
Pareto Efficiency</h3>
<p>The stable equilibrium state in the Prisoner’s Dilemma is for both
agents to defect. Neither agent would choose to go back in time and
change their decision (to switch to cooperating) if they could not also
alter their partner’s behavior by doing so. This is often considered
counterintuitive, as the agents would benefit if they were both to
switch to cooperating.</p>
<p><strong>Nash Equilibrium: both agents will choose to defect.</strong>
Defection is the best strategy for Alice, regardless of what Bob opts to
do. The same is true for Bob. Therefore, if both are behaving in a
rational and self-interested fashion, they will both defect. This will
secure the outcome of 3 years of jail time each (the bottom-right
outcome of the payoff matrix above). Neither would wish to change their
decision, even if their partner were to change theirs. This is known as
the <em>Nash equilibrium</em>: the strategy choices from which no agent
can benefit by unilaterally choosing a different strategy. When
interacting with one another, rational agents will tend towards picking
strategies that are part of Nash equilibria.</p>
<p><strong>Pareto improvement: both agents would do better if they
cooperated.</strong> As we can see in the payoff matrix, there is a
possible outcome that is better for both suspects. If both choose the
cooperate strategy, they will secure the top-left outcome of the payoff
matrix. Each would serve 2 years less jail time at no cost to the other.
Yet, as we have seen, selecting this strategy is irrational; the
<em>defect</em> strategy is dominant and so Alice and Bob each want to
defect instead. We call this outcome <strong>Pareto
inefficient</strong>, meaning that it could be altered to make some of
those involved better off without making anyone else worse off. In the
Prisoner’s Dilemma, the <em>both defect</em> outcome is Pareto
inefficient because it is suboptimal for both Alice and Bob, who would
both be better off if they both cooperated instead. Where there is an
outcome that is better for some or all agents involved, and not worse
for any, we call the switch to this more efficient outcome a <em>Pareto
improvement</em>. In the Prisoner’s Dilemma, the <em>both cooperate</em>
outcome is better for both agents than the Nash equilibrium of <em>both
defect</em>; see Figure <a href="#fig:pareto-comparison"
data-reference-type="ref" data-reference="fig:pareto-comparison">2</a>B.
The only Pareto improvement possible in this game is the move from the
<em>both defect</em> to the <em>both cooperate</em> outcome; see Figure
<a href="#fig:pareto-efficiency" data-reference-type="ref"
data-reference="fig:pareto-efficiency">3</a>.<br />
</p>
<figure id="fig:pareto-comparison">
<p><img
src="images/multiagent-dynamics/Alice and Bob choices red green.png"
alt="image" /> <span id="fig:pareto-comparison"
label="fig:pareto-comparison"></span></p>
<figcaption>The possible outcomes for both suspects in the Prisoner’s
Dilemma</figcaption>
</figure>
<p><em>A) Shown is the same decision tree as in Figure 2B, but for both
suspects. Rather than jail sentences, we show payoffs (negative numbers,
rather than positive). B) The outcome where both suspects get the “-3”
payoff is the Nash equilibrium, since defection is the dominant strategy
for both. However, this outcome is Pareto inefficient, as both suspects
would do better if both chose instead to cooperate, securing the outcome
in which both get the “-1” payoff. Both switching to cooperation would
produce a Pareto improvement.</em><br />
</p>
<figure id="fig:pareto-efficiency">
<img
src="images/multiagent-dynamics/Alice-bob-payout-purple-green.png" />
<figcaption>The possible outcomes for both suspects in the Prisoner’s
Dilemma - adapted from <span class="citation"
data-cites="kuhn2019prisoner"></span></figcaption>
</figure>
<p><em>Both suspects’ payoffs, in each of the four decision outcomes.
Movement right through the graphspace represents a better payoff for
Alice; movement up represents a better payoff for Bob. A Pareto
improvement must therefore be a movement both right and up. There is
only one such move possible, shown as a green arrow: from “-3,-3” (both
defect) to “-1,-1” (both cooperate).</em></p>
<h3 id="real-world-examples-of-the-prisoners-dilemma">Real-World
Examples of the Prisoner’s Dilemma</h3>
<p>The Prisoner’s Dilemma has many simplifying assumptions.
Nevertheless, it can be a helpful lens through which to understand
social dynamics in the real world. Rational and self-interested parties
often produce states that are Pareto inefficient. There exist
alternative states that would be better for all involved, but reaching
these requires individually irrational action. To illustrate this, let’s
explore some real-world examples.</p>
<p><strong>Mud-slinging.</strong> Consider the practice of mud-slinging.
Competing political parties often use negative campaign tactics,
producing significant reputational costs. By running negative ads to
attack and undermine the public image of their opponents, all parties
end up with tarnished reputations. If we assume that politicians value
their reputation in an absolute sense, not merely in relation to their
contemporary competitors, then mud-slinging is undesirable for all. A
Pareto improvement to this situation would be switching to the outcome
where they all cooperate. With no one engaging in mud-slinging, all the
parties would have better reputations. The reason this does not happen
is that mud-slinging is the dominant strategy. If a party’s opponent
<em>doesn’t</em> use negative ads, the party will boost their reputation
relative to their opponent’s by using them. If their opponent
<em>does</em> use negative ads, the party will reduce the difference
between their reputations by using them too. Thus, both parties converge
on the Nash equilibrium of mutual mud-slinging, at avoidable detriment
to all.</p>
<p><strong>Shopkeeper price cuts.</strong> Another example is price
racing dynamics between different goods providers. Consider two rival
shopkeepers selling similar produce at similar prices. They are
competing for local customers. Each shopkeeper calculates that lowering
their prices below that of their rival will attract more customers away
from the other shop and result in a higher total profit for themselves.
If their competitor drops their prices and they do not, then the
competitor will gain extra customers, leaving the first shopkeeper with
almost none. Thus, “dropping prices” is the dominant strategy for both.
This leads to a Nash equilibrium in which both shops have low prices,
but the local custom is divided much the same as it would be if they had
both kept their prices high. If they were both to raise their prices,
they would both benefit by increasing their profits: this would be a
Pareto improvement. Note that, just as how the interests of the police
do not count in the Prisoner’s Dilemma, we are only considering the
interests of the shopkeepers in this example. We are ignoring the
interests of the customers and wider society. Real-world Pareto
improvements are not common.</p>
<p><strong>Arms races.</strong> Nations’ expenditure on military arms
development is another example. It would be better for all these
nations’ governments if they were all simultaneously to reduce their
military budgets. No nation would become more vulnerable if they were
all to do this, and each could then redirect these resources to areas
such as education and healthcare. Instead, we have widespread military
arms races. We might prefer for all the nations to turn some military
spending to their other budgets, but for any one nation to do so would
be irrational. Here, the dominant strategy for each nation is to opt for
high military expenditure. So we achieve a Nash equilibrium in which all
nations must decrease spending in other valuable sectors. It would be
more Pareto efficient for all to have lower military spending, freeing
money and resources for different domains. We will consider races in the
context of AI development in the following section.</p>
<h3 id="promoting-cooperation">Promoting Cooperation</h3>
<p>So far we have focused on the sources of undesirable multi-agent
dynamics in games like the Prisoner’s Dilemma. Here, we turn to the
mechanisms by which we can promote cooperation over defection.</p>
<p><strong>Reasons to cooperate.</strong> There are many reasons why
real-world agents might cooperate in situations which resemble the
Prisoner’s Dilemma <span class="citation"
data-cites="parfit1984reasons"></span>, as shown in Figure <a
href="#fig:cooperate" data-reference-type="ref"
data-reference="fig:cooperate">4</a>. These can broadly be categorized
by whether the agents have a choice, or whether defection is impossible.
If the agents do have a choice, we can further divide the possibilities
into those where they act in their own self-interest, and those where
they do not (altruism). Finally, we can differentiate three reasons why
self-interested agents may choose to cooperate: a conscience, a change
to the payoff structure, and future reward/punishment. We will explore
two possibilities in this section — payoff changes and altruistic
dispositions — and then “future reward/punishment” in the next section.
Note that we effectively discuss “Defection is impossible” in the
chapter, and “AI consciences” in the .<br />
</p>
<figure id="fig:cooperate">
<img
src="images/multiagent-dynamics/Agents-may-cooperate-because.png" />
<figcaption>Five reasons agents might cooperate - <span class="citation"
data-cites="parfit1984reasons"></span></figcaption>
</figure>
<p><em>Five possible reasons why agents may cooperate in prisoner’s
Dilemma-like scenarios. As highlighted, this section explores only two:
changes to the payoff matrix and increased agent altruism.</em></p>
<p><strong>External consideration: changing the payoffs to incentivize
cooperation.</strong> By adjusting the values in the payoff matrix, we
may more easily steer agents away from undesirable equilibria. As shown
in Table <a href="#tab:abstract" data-reference-type="ref"
data-reference="tab:abstract">2</a>, incentive structures are important.
A Prisoner’s Dilemma-like scenario may arise wherever an individual
agent will do better to defect whether their partner cooperates (<span
class="math inline"><em>c</em> &gt; <em>a</em></span>) or defects (<span
class="math inline"><em>d</em> &gt; <em>b</em></span>). Avoiding this
situation requires altering these constants where they underlie critical
social interactions in the real world: changing the costs and benefits
associated with different activities so as to encourage cooperative
behavior.<br />
</p>
<div id="tab:abstract">
<table>
<caption>Abstract Prisoner’s Dilemma payoff matrix</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><span style="color: blue">Agent B
cooperates</span></th>
<th style="text-align: center;"><span style="color: blue">Agent B
defects</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span style="color: cyan">Agent A
cooperates</span></td>
<td style="text-align: center;"><span style="color: cyan">a</span>,
<span style="color: blue">a</span></td>
<td style="text-align: center;"><span style="color: cyan">b</span>,
<span style="color: blue">c</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span style="color: cyan">Agent A
defects</span></td>
<td style="text-align: center;"><span style="color: cyan">c</span>,
<span style="color: blue">b</span></td>
<td style="text-align: center;"><span style="color: cyan">d</span>,
<span style="color: blue">d</span></td>
</tr>
</tbody>
</table>
</div>
<p><em>Shown is the payoff matrix for the Prisoner’s Dilemma, in the
abstract. Notice that if <span
class="math inline"><em>c</em> &gt; <em>a</em></span> and <span
class="math inline"><em>d</em> &gt; <em>b</em></span>, the highest
payoff for either agent is to defect, regardless of what their opponent
does: Defection is the dominant strategy. Therefore, fostering
cooperation requires that we avoid structuring incentives such that
<span class="math inline"><em>c</em> &gt; <em>a</em></span> and <span
class="math inline"><em>d</em> &gt; <em>b</em></span>.</em><br />
There are two ways to reduce the expected value of defection: lower the
<em>probability</em> of defection success or lower the <em>benefit</em>
of a successful defection. Consider a strategy commonly used by
organized crime groups: threatening members with extreme punishment if
they ‘snitch’ to the police. In the Prisoner’s Dilemma game, we can
model this by adding a punishment equivalent to three years of jail time
for “snitching,” leading to the altered payoff matrix as shown in Figure
<a href="#fig:snitches" data-reference-type="ref"
data-reference="fig:snitches">5</a>. The Pareto efficient outcome
(-1,-1) is now also a Nash Equilibrium because snitching when the other
player cooperates is worse than mutually cooperating (<span
class="math inline"><em>c</em> &lt; <em>a</em></span>).<br />
</p>
<figure id="fig:snitches">
<figure>
<img src="images/multiagent-dynamics/Alice-payoff-with-graphs.png" />
</figure>
<figure>
<img src="images/multiagent-dynamics/Alice-bob-payoff-graphs2.png" />
</figure>
<figcaption>Altering the payoff matrix to “punish snitches”.
</figcaption>
</figure>
<p><em>A) The Prisoner’s Dilemma payoff matrix, with the single Nash
equilibrium highlighted. B) If we add a punishment of three years jail
time for being a “snitch,” the outcome (-1,-1) becomes a second Nash
Equilibrium. Note that this is known as the “Stag Hunt” in game
theory.</em><br />
Internal consideration: making agents more altruistic to promote
cooperation. A second potential mechanism to foster cooperation is to
make agents more altruistic. If each agent also values the outcome for
their partner, this effectively changes the payoff matrix. Now, the
length of their partner’s jail sentence matters to each of them. In the
Prisoner’s Dilemma payoff matrix, the <em>both cooperate</em> outcome
earns the lowest total jail time, so agents who valued their partners’
payoffs equally to their own would converge on cooperation.</p>
<p><strong>Parallels to AI safety.</strong> One possible example of such
a strategy would be to target the values held by AI companies
themselves. Improving corporate regulation effectively changes the
company’s expected payoffs from pursuing risky strategies. If
successful, it could encourage the company building AI systems to behave
in a less purely self-interested fashion. Rather than caring solely
about maximizing their shareholder’s financial interests, AI companies
might cooperate more with each other to steer away from Pareto
inefficient outcomes, and avoid corporate AI races. We explore this in
more detail in <em><a href="#sec:AI-races" data-reference-type="ref"
data-reference="sec:AI-races">2.4.3</a> “Tail risk: AI races”</em>
below.</p>
<h3 id="summary">Summary</h3>
<p><strong>Cooperation is not always rational, so intelligence alone may
not ensure good outcomes.</strong> We have seen that rational and
self-interested agents may not interact in such a way as to achieve good
results, even for themselves. Under certain conditions, such as in the
Prisoner’s Dilemma, they will converge on a Nash equilibrium of both
defecting. Both agents would be better off if they both cooperated.
However, it is hard to secure this Pareto improvement because
cooperation is not rational when defection is the dominant strategy.</p>
<p><strong>Conflict with or between future AI agents may be extremely
harmful.</strong> One source of concern regarding future AI systems is
inter-agent conflict eroding the value of the future. Rational AI agents
faced with a Prisoner’s Dilemma-type scenario might end up in stable
equilibrium states that are far from optimal, perhaps for all the
parties involved. Possible avenues to reduce these risks include
restructuring the payoff matrices for the interactions in which these
agents may be engaged or altering the agents’ dispositions.<br />
</p>
<h2 id="the-iterated-prisoners-dilemma">The Iterated Prisoner’s
Dilemma</h2>
<p>In our discussion of the Prisoner’s Dilemma, we saw how rational
agents may converge to equilibrium states that are bad for all involved.
In the real world, however, agents rarely interact with one another only
once. Our aim in this section is to understand how cooperative behavior
can be promoted and maintained as multiple agents (both human and AI)
interact with each other over time, when they expect repeated future
interactions. We handle some common misconceptions in this section, such
as the idea that simply getting agents to interact repeatedly is
sufficient to foster cooperation, because “nice” and “forgiving”
strategies always win out. As we shall see, things are not so simple. We
explore how iterated interactions can lead to progressively worse
outcomes for all.<br />
In the real world, we can observe this in “AI races”, where businesses
cut corners on safety due to competitive pressures, and militaries adopt
and deploy potentially unsafe AI technologies, making the world less
safe. These AI races could produce catastrophic consequences, including
more frequent or destructive wars, economic enfeeblement, and the
potential for catastrophic accidents from malfunctioning or misused AI
weapons.</p>
<h3 id="introduction-1">Introduction</h3>
<p>Agents who engage with one another many times do not always coexist
harmoniously. Iterating interactions is not sufficient to ensure
cooperation. To see why, we explore what happens when rational,
self-interested agents play the Prisoner’ Dilemma game against each
other repeatedly. In a single-round Prisoner’s Dilemma, defection is
always the rational move. But understanding the success of different
strategies is more complicated when agents play multiple rounds.</p>
<p><strong>In the Iterated Prisoner’s Dilemma, agents play
repeatedly.</strong> The dominant strategy for a rational agent in a
one-off interaction such as the Prisoner’s Dilemma is to defect. The
seeming paradox is that both agents would prefer the cooperate-cooperate
outcome to the defect-defect one. An agent cannot influence their
partner’s actions in a one-off interaction, but in an iterated scenario,
one agent’s behavior in one round may influence how their partner
responds in the next. We call this the <em>Iterated Prisoner’s
Dilemma</em>; see Figure <a href="#fig:iterated"
data-reference-type="ref" data-reference="fig:iterated">6</a>. This
provides an opportunity for the agents to cooperate with each other.</p>
<p><strong>Iterating the Prisoner’s Dilemma opens the door to rational
cooperation.</strong> In an Iterated Prisoner’s Dilemma, both agents can
achieve higher payoffs by fostering a cooperative relationship with each
other than they would if both were to defect every round; compare Figure
8a and b. There are two basic mechanisms by which iteration can promote
cooperative behavior: punishing defection and rewarding cooperation. To
see why, let us follow an example game of the Iterated Prisoner’s
Dilemma in sequence.</p>
<p><strong>Punishment.</strong> Recall Alice and Bob from the previous
section, the two would-be thieves caught by the police. Alice decides to
defect in the first round of the Prisoner’s Dilemma, while Bob opts to
cooperate. This achieves a good outcome for Alice, and a poor one for
Bob, who punishes this behavior by choosing to defect himself in the
second round. What makes this a punishment is that Alice’s score will
now be lower than it would be if Bob had opted to cooperate instead,
whether Alice chooses to cooperate or defect.</p>
<p><strong>Reward.</strong> Alice, having been punished, decides to
cooperate in the third round. Bob rewards this action by cooperating in
turn in the fourth. What makes this a reward is that Alice’s score will
now be higher than if Bob had instead opted to defect, whether Alice
chooses to cooperate or defect. Thus, the expectation that their
defection will be punished and their cooperation rewarded incentivizes
both agents to cooperate with each other.<br />
</p>
<figure id="fig:iterated">
<img src="images/multiagent-dynamics/tit-for-tat-green-red.png" />
<figcaption>If the agents cooperate more, they can both gain better
payoffs</figcaption>
</figure>
<p><em>Alice and Bob play a six-round Iterated Prisoner’s Dilemma. Red
squares show defection; green squares show cooperation. The number in
the square shows that agent’s payoff. Both agents end up with better
total payoffs (less jail time) when they cooperate more.</em></p>
<p><strong>Defection is still the dominant strategy if agents know how
many times they will interact.</strong> If the agents know when they are
about to play the Prisoner’s Dilemma with each other for the final time,
both will choose to defect in that final round. This is because their
defection is no longer punishable by their partner. If Alice defects in
the last round of the Iterated Prisoner’s Dilemma, Bob cannot punish her
by retaliating, as there are no future rounds in which to do so. The
same is of course true for Bob. Thus, <em>defection is the dominant
strategy for each agent in the final round</em>, just as it is in the
single-round version of the dilemma.<br />
Moreover, if each agent expects their partner to defect in the final
round, <em>then there is no incentive for them to cooperate in the
penultimate round either</em>. This is for the same reason: Defecting in
the penultimate round will not influence their partner’s behavior in the
final round. Whatever an agent decides to do, they expect that their
partner will choose to defect next round, so they might as well defect
now. We can extend this argument by reasoning backwards through all the
iterations. In each round, the certainty that their partner will defect
in the next round regardless of their own behavior in the current round
incentivizes each agent to defect. The reward for cooperation and
punishment of defection have been removed. Ultimately, this removal
pushes the agents to defect in every round of the Iterated Prisoner’s
Dilemma.</p>
<p><strong>Uncertainty about future engagement enables rational
cooperation.</strong> In the real world, an agent can rarely be sure
that they will never again engage with a given partner. Wherever there
is sufficient uncertainty about the future of their relationship,
rational agents may be more cooperative. This is for the simple reason
that uncooperative behavior may yield less valuable outcomes in the long
term, because others may retaliate in kind in the future. This tells us
that AIs interacting with each other repeatedly may cooperate, but only
if they are sufficiently uncertain about whether their interactions are
about to end.<br />
Other forms of uncertainty can also create opportunities for rational
cooperation, such as uncertainty about what strategies others will use.
These are most important where the Iterated Prisoner’s Dilemma involves
a population of more than two agents, in which each agent interacts
sequentially with multiple partners. We turn to examining the dynamics
of these more complicated games next.</p>
<h3 id="sec:tournaments">Tournaments</h3>
<p>So far, we have considered the Iterated Prisoner’s Dilemma between
only two agents: each plays repeatedly against a single partner.
However, in the real world, we expect AIs will engage with multiple
other agents. In this section, we consider interactions of this kind,
where each agent not only interacts with their partner repeatedly, but
also switches partners over time. Understanding the success of a
strategy is more complicated in repeated rounds against many partners.
Note that in this section, we define a “match” to mean repeated rounds
of the Prisoner’s Dilemma between the same two agents; see Figure <a
href="#fig:iterated" data-reference-type="ref"
data-reference="fig:iterated">6</a>. We define a “tournament” to mean a
population of more than two agents engaged in a set of pairwise
matches.</p>
<p><strong>In Iterated Prisoner Dilemma tournaments, each agent
interacts with multiple partners.</strong> In the 1970s, the political
scientist Robert Axelrod held a series of tournaments to pit different
agents against one another in the Iterated Prisoner’s Dilemma. The
tournament winner was whichever agent had the highest total payoff after
completing all matches. Each agent in an Iterated Prisoner’s Dilemma
tournament plays multiple rounds against multiple partners. These agents
employed a range of different strategies. For example, an agent using
the strategy named <em>random</em> would randomly determine whether to
cooperate or defect in each round, entirely independently of previous
interactions with a given partner. By contrast, an agent using the
<em>grudger</em> strategy would start out cooperating, but switch to
defecting for all future interactions if its partner defected even once.
See Table <a href="#tab:strategies" data-reference-type="ref"
data-reference="tab:strategies">3</a> for examples of these
strategies.<br />
</p>
<div id="tab:strategies">
<table>
<caption>Strategy Descriptions</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Strategy</strong></th>
<th style="text-align: left;">Characteristics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><em>Random</em></td>
<td style="text-align: left;">Randomly defect or cooperate, regardless
of your partner’s strategy</td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>Always defect</em></td>
<td style="text-align: left;">Always choose to defect, regardless of
your partner’s strategy</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>Always cooperate</em></td>
<td style="text-align: left;">Always choose to defect, regardless of
your partner’s strategy</td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>Grudger</em></td>
<td style="text-align: left;">Start by cooperating, but if your partner
defects, defect in every subsequent round, regardless of your partner’s
subsequent behavior</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>Tit-for-tat</em></td>
<td style="text-align: left;">Start cooperating; then always do whatever
your partner did last</td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>Generous tit-for-tat</em></td>
<td style="text-align: left;">Same as <em>tit-for-tat</em>, but
occasionally cooperate in response to your partner’s defection</td>
</tr>
</tbody>
</table>
</div>
<p><strong>The strategy “<em>Tit-for-tat</em>” frequently won Axelrod’s
tournaments <span class="citation"
data-cites="axelrod1980effective"></span>.</strong> The most famous
strategy used in Axelrod’s tournaments was <em>Tit-for-tat</em>. This
was the strategy of starting by cooperating, then repeating the
partner’s most recent move: if they cooperated, <em>Tit-for-tat</em>
cooperated too; if they defected, <em>Tit-for-tat</em> did likewise.
Despite its simplicity, this strategy was extremely successful, and very
frequently won tournaments. An agent playing <em>Tit-for-tat</em>
exemplified the two mechanisms for promoting cooperation, rewarding
cooperation, yet also punishing defection. Importantly,
<em>Tit-for-tat</em> did not hold a grudge —– it forgave each defection
after it retaliated by defecting in return, only once. This process of
one defection for one defection is captured in the famous idiom “an eye
for an eye.” The <em>Tit-for-tat</em> strategy became emblematic as
being one way to escape the muck of defection.</p>
<p><strong>The success of <em>Tit-for-tat</em> is
counterintuitive.</strong> In any given match, an agent playing
<em>Tit-for-tat</em> will tend to score slightly worse than or the same
as their partner; see Figure <a href="#fig:tit-for-tat"
data-reference-type="ref" data-reference="fig:tit-for-tat">7</a>a. By
contrast, an agent who employs an uncooperative strategy such as
<em>always defect</em> usually scores the same as or better than its
partner; see Figure <a href="#fig:tit-for-tat" data-reference-type="ref"
data-reference="fig:tit-for-tat">7</a>b. In a match between a
cooperative agent and an uncooperative one, the uncooperative agent
tends to end up with the better score.<br />
However, it is an agent’s <em>average</em> score which dictates its
success in a tournament, not its score in any particular match or with
any particular partner. Two uncooperative partners will score worse on
average than cooperative ones. Thus, the success of cooperative
strategies such as <a href="#fig:tit-for-tat" data-reference-type="ref"
data-reference="fig:tit-for-tat">7</a> depends on the population
strategy composition (the assortment of strategies used by the agents in
the population). If there are enough cooperative partners, cooperative
agents may be more successful than uncooperative ones. We discuss this
phenomenon in more detail in <em>Section <a
href="#sec:evolutionary-stable-strategies" data-reference-type="ref"
data-reference="sec:evolutionary-stable-strategies">2.6.2</a>
Evolutionarily stable strategies and frequency-dependent
selection.</em><br />
</p>
<figure id="fig:tit-for-tat">
<img src="images/multiagent-dynamics/Tit-for-tat.png" />
<figcaption>Tit-for-tat tends to score worse than any given partner (in
a match), but better than all on average (in a tournament)</figcaption>
</figure>
<p><em>As with Figure 8: each panel shows a six-round Iterated
Prisoner’s Dilemma, with red squares for defection and green for
cooperation. On the left is <em>Tit-for-tat</em>: An agent using this
strategy tends to score the same as or worse than its partners in each
match. On the right, <em>always defect</em> tends to score the same as
or better than its partner in each match. The average payoff attained by
using either strategy are shown at the bottom: <em>Tit-for-tat</em>
attains a better payoff (lower jail sentence) on average —– and so is
more successful in a tournament —– than <em>always defect</em>.</em></p>
<h3 id="sec:AI-races">Tail Risk: AI Races</h3>
<p>Iterated interactions can generate “AI races.” We discuss two kinds
of races concerning AI development: corporate AI races and military AI
arms races. Both kinds center around competing parties participating in
races for individual, short-term gains at a collective, long-term
detriment. Where individual incentives clash with collective interests,
the outcome can be bad for all. As we discuss here, in the context of AI
races, these outcomes could even be catastrophic.</p>
<p><strong>AI races are the result of intense competitive
pressures.</strong> During the Cold War, the US and the Soviet Union
were involved in a costly nuclear arms race. The effects of their
competition persist today, leaving the world in a state of heightened
nuclear threat. Competitive races of this kind entail repeated
back-and-forth actions that can result in progressively worse outcomes
for all involved. We can liken this example to the Iterated Prisoner’s
Dilemma, where the nations must decide whether to increase (defect) or
decrease (cooperate) their nuclear spending. Both the US and the Soviet
Union often chose to increase spending. They would have created a safer
and less expensive world for both nations (as well as others) if they
had cooperated to reduce their nuclear stockpiles. We discuss this in
more detail in .</p>
<p><strong>Two kinds of AI races: corporate and military <span
class="citation" data-cites="hendrycks2023overview"></span>.</strong>
Competition between different parties—–nations or corporations–—is
incentivizing each to develop, deploy, and adopt AIs rapidly, at the
expense of other values and safety precautions. Corporate AI races
consist of businesses prioritizing their own survival or power expansion
over ensuring that AIs are developed and released safely. Military AI
arms races consist of nations building and adopting powerful and
dangerous military applications of AI technologies to gain military
power, increasing the risks of more frequent or damaging wars, misuse,
or catastrophic accidents. We can understand these two kinds of AI races
using two game-theoretic models of iterated interactions. First, we use
the <em>Attrition</em> model to understand why AI corporations are
cutting corners on safety. Second, we’ll use the <em>Security
Dilemma</em> model to understand why militaries are escalating the use
of—–and reliance on–—AI in warfare.</p>
<h3 id="corporate-ai-races">Corporate AI Races</h3>
<p>Competition between AI research companies is promoting the creation
and use of more appealing and profitable systems, often at the cost of
safety measures. Consider the public release of large language
model-based chatbots. Some AI companies delayed releasing their chatbots
out of safety concerns, like avoiding the generation of harmful
misinformation. We can view the companies that released their chatbots
first as having switched from cooperating to defecting in an Iterated
Prisoner’s Dilemma. The defectors gained public attention and secured
future investment. This competitive pressure caused other companies to
rush their AI products to market, compromising safety measures in the
process.<br />
Corporate AI races arise because competitors sacrifice their values to
gain an advantage, even if this harms others. As the race heats up,
corporations might increasingly need to prioritize profits by cutting
corners on safety, in order to survive in a world where their
competitors are very likely to do the same. The worst outcome for an
agent in the Prisoner’s Dilemma is the one where only they cooperated
while their partner defected. Competitive pressures motivate AI
companies to avoid this outcome, even at the cost of exacerbating
large-scale risks.<br />
Ultimately, corporate AI races could produce societal-scale harms, such
as mass unemployment and dangerous dependence on AI systems. We consider
one such example in <em></em>. This risk is particularly vivid for
emerging industries like AI which lack the better-established safeguards
such as mature regulation and widespread awareness of the harm that
unsafe products can cause found in other industries like
pharmaceuticals.</p>
<p><strong>Attrition model: a multi-player game of “Chicken.”</strong>
We can model this corporate AI race using an “Attrition” model <span
class="citation" data-cites="smith1974theory"></span>, which frames the
race as a kind of auction in which competitors bid against one another
for a valuable prize. Rather than bidding money, the competitors bid for
the risk level they are willing to tolerate. This is similar to the game
“Chicken,” in which two competitors drive headlong at each other.
Assuming one swerves out of the way, the winner is the one who does not
(demonstrating that they can tolerate a higher level of risk than the
loser). Similarly, in the Attrition model, each competitor bids the
level of risk —– the probability of bringing about a catastrophic
outcome —– they are willing to tolerate. Whichever competitor is willing
to tolerate the most risk will win the entire prize, as long as the
catastrophe they are risking does not actually happen. We can consider
this to be an “all pay” auction: both competitors must pay what they
bid, whether they win or not. This is because all of those involved must
bear the risk they are leveraging, and once they have made their bid
they cannot retract it.</p>
<p><strong>The Attrition model shows why AI corporations may cut corners
on safety.</strong> Let us assume that there are only two competitors
and that both of them have the same understanding of the state of their
competition. In this case, the Attrition model predicts that they will
race each other up to a loss of one-third in expected value <span
class="citation" data-cites="nisan2007algorithmic"></span>. If the value
of the prize to one competitor is “X”, they will be willing to risk a
33% chance of bringing about an outcome equally disvaluable (of value
“-X”) in order to win the race <span class="citation"
data-cites="dafoe2022governance"></span>.<br />
As we have discussed previously, market pressures may motivate
corporations to behave as though they value what they are competing for
almost as highly as survival itself. According to this toy model, we
might then expect AI stakeholders engaged in a corporate race to risk a
33% chance of existential catastrophe in order to “win the prize” of
their continued existence. With multiple AI races, long time horizons,
and ever-increasing risks, the repeated erosion of safety assurances
down to only 66% generates a vast potential for catastrophe.</p>
<p><strong>Real-world actors may mistakenly erode safety precautions
even further.</strong> Moreover, real-world AI races could produce even
worse outcomes than the one predicted by the Attrition model <span
class="citation" data-cites="dafoe2022governance"></span>. One reason
for this is that competing corporations may not have a correct
understanding of the state of the race. Precisely predicting these kinds
of risks can be extremely challenging: high-risk situations are
inherently difficult to predict accurately, even in fields far more
well-understood than AI. Incorrect risk calibration could cause the
competitors to take actions that accidentally exceed even the 33% risk
level. Like newcomers to an ’all pay’ auction who often overbid, uneven
comprehension or misinformation could motivate the competitors to take
even greater risks of bringing about catastrophic outcomes. In fact, we
might even expect selection for competitors who tend to underestimate
the risks of these races. All these factors may further erode safety
assurances.</p>
<h3 id="military-ai-arms-races">Military AI Arms Races</h3>
<p>Global interest in military applications for AI technologies is
increasing. Some hail this as the “third revolution in warfare” <span
class="citation" data-cites="lee2021visions"></span>, predicting impact
at the scale of the historical development of gunpowder and nuclear
weapons. There are many causes for concern about the adoption of AI
technologies in military contexts. These include increased rates of
weapon development, lethal autonomous weapons usage, advanced
cyberattacks execution, and automation of decision-making. These could
in turn produce more frequent and destructive wars, acts of terrorism,
and catastrophic accidents. Perhaps even more important than the
immediate dangers from military deployment of AI is the possibility that
nations will continue to race each other along a path towards ever
increased risks of catastrophe. In this section, we explore this
possibility using another game theoretic model.<br />
First, let us consider a few different sources of risk from military AI
<span class="citation" data-cites="hendrycks2023overview"></span>:</p>
<ol>
<li><p><strong>AI-developed weapons.</strong> AI technologies could be
used to engineer weapons. Military research and development offers many
opportunities for acceleration using AI tools. For instance, AI could be
used to expedite processes in dual-use biological and chemical research,
furthering the development of programs to build weapons of mass
destruction.</p></li>
<li><p><strong>AI-controlled weapons.</strong> AI might also be used to
control weapons directly. “Lethal autonomous weapons” have been in use
since March 2020, when a self-directing and armed drone “hunted down”
soldiers in Libya without human supervision. Autonomous weapons may be
faster or more reliable than human soldiers for certain tasks, as well
as being far more expendable. Autonomous weapons systems thus
effectively motivate militaries to reduce human oversight. In a context
as morally salient as warfare, the ethical implications of this could be
severe. Increasing AI weapon development may also impact international
warfare dynamics. The ability to deploy lethal autonomous weapons in
place of human soldiers could drastically lower the threshold for
nations to engage in war, by reducing the expected body count–—of the
nation’s own citizens, at least. These altered warfare dynamics could
usher in a future with more frequent and destructive wars than has yet
been seen in human history.</p></li>
<li><p><strong>AI cyberwarfare.</strong> Another military application is
the use of AI in cyberwarfare. AI systems might be used to defend
against cyberattacks. However, we do not yet know whether this will
outweigh the offensive potential of AI in this context. Cyberattacks can
be used to wreak enormous harm, such as by damaging crucial systems and
infrastructure to disrupt supply chains. AIs could make cyberattacks
more effective in a number of ways, motivating more frequent attempts
and more destructive successes. For example, AIs could directly aid in
writing or improving offensive programs. They could also execute
cyberattacks at superhuman scales by implementing vast numbers of
offensive programs simultaneously. By democratizing the power to execute
large-scale cyberattacks, AIs would also increase the difficulty of
verification. With many more actors capable of carrying out attacks at
such scales, attributing attacks to perpetrators would be much more
challenging.</p></li>
<li><p><strong>Automated executive decision-making.</strong> Executive
control might be delegated to AIs at higher levels of military
procedures. The development of AIs with superhuman strategic
capabilities may incentivize nations to adopt these systems and
increasingly automate military processes. One example of this is
“automated retaliation.” AI systems that are granted the ability to
respond to offensive threats they identify with counterattacks, without
human supervision. Examples of this include the NSA cyber defense
program known as “MonsterMind.” When this program identified an
attempted cyberattack, it interrupted it and prevented its execution.
However, it would then launch an offensive cyberattack of its own in
return. It could take this retaliatory action without consulting human
supervisors. More powerful AI systems, more destructive weapons, and
greater automation or delegation of military control to AI systems,
would all deplete our ability to intervene.</p></li>
<li><p><strong>Catastrophic accidents.</strong> Lethal Autonomous
Weapons and automated decision-making systems both carry risks of
resulting in catastrophic accidents. If a nation were to lose control of
powerful military AI technologies, the outcome could be calamitous.
Outsourcing executive command of military procedures to AI –— such as by
automating retaliatory action –— would put powerful arsenals on
hair-trigger alert. If one of these AI systems were to make even a small
error, such as incorrectly identifying an offensive strike from another
nation, it might automatically “retaliate” to this non-existent threat.
This could in turn trigger automated retaliations from the AI systems of
other nations that detect this action. Thus, a small error could be
exacerbated into an increasingly escalated war. We consider how a “flash
war” such as this might come about in more detail in Section <a
href="#sec:AI-races-Multiple" data-reference-type="ref"
data-reference="sec:AI-races-Multiple">2.5.4</a>. Note that we can also
use the “Attrition” model in the case of military AI arms races to model
how military competitive pressures can motivate nations to cut corners
on safety.</p></li>
<li><p><strong>Co-option of military AI technologies.</strong> Military
AI arms races could also have catastrophic effects outside of
international warfare. New and more lethal weapons could be used
maliciously in other contexts. For instance, biological weapons were
originally created for military purposes. Even though we have since
halted the military use of these weapons, their existence has enabled
many acts of bioterrorism. Examples include the 2001 deployment of
anthrax letters to kill US senators and media executives. The creation
of knowledge of how to make and use these weapons is irreversible. Thus,
their existence, and the risk they pose, is permanent.</p></li>
<li><p><strong>Military AI risks may interact.</strong> Importantly, the
risks posed by military AI applications are not entirely independent of
one another. The increased potential for anonymity when executing
cyberattacks could increase the probability of wars. Where it is harder
to identify the perpetrators, misattribution could trigger conflict
between the target of the attack and an innocent party. The potential
for destructive cyberattacks might be increased by the scaled-up use of
autonomous weapons, as these could be co-opted by such attacks.
Similarly, the danger posed by a rogue AI with executive decision-making
power might be all the more serious if it has control over fleets of
autonomous weapons.</p></li>
</ol>
<p><strong>Security Dilemma model: mutual defensive concerns motivate
nations to increase risks.</strong> We can better understand military AI
arms races using the “Security Dilemma” model <span class="citation"
data-cites="herz1950idealist"></span>. Consider the relationship between
two peaceful nations. Though they are not currently at war with one
another, each is sufficiently concerned about the possibility of
conflict to pay close attention to the other’s state of military
ability. One day, one of the two nations perceives that the other is
more militarily capable than they are due to their having stockpiled
more advanced weaponry. This incentivizes the first nation to build up
their own military capabilities until they match or exceed those of the
other nation. The second nation, perceiving this increase in military
investment and development, feels pressure to follow suit, once again
increasing their weapon capabilities. Neither wishes to be outmatched by
the other. This competitive pressure drives both to escalate the
situation. The ensuing arms race generates increasingly high risks for
both sides, such as increasing the probability or severity of accidents
and misuse.</p>
<p><strong>Example: the Cold War nuclear arms race.</strong> As
previously discussed, the Cold War nuclear arms race typifies this
process. Neither the US nor the Soviet Union wanted to risk being less
militarily capable than their rival, so each escalated their own
weaponized nuclear ability in an attempt to deter the other using the
threat of retaliation. Just as in the Iterated Prisoner’s Dilemma,
neither nation could afford to risk being the lone cooperator while
their rival defected. Thus, they achieve a Pareto inefficient outcome of
both defecting. Competitive pressure drove them to continue to worsen
this situation over time, resulting in today’s enormously heightened
state of nuclear vulnerability.</p>
<p><strong>Increased automation of warfare by one nation puts pressure
on others to follow suit.</strong> Just as with nuclear weapons, so with
military AI: the Security Dilemma model illustrates how defensive
concerns can force nations to go down a route which is against the long
term interests of all involved. This route leads to the competing
nations continually heightening the risks posed by military AI
applications, including more frequent and severe wars, and worse
accidents.</p>
<p>There are many incentives for nations to increase their development,
adoption, and deployment of military AI applications. With more AI
involvement, warfare can take place at an accelerated pace, and at a
more destructive scale. Nations that do not adopt and use military AI
technologies may therefore risk not being able to compete with nations
that do. As with nuclear mutually assured destruction, nations may also
employ automated retaliation as a signal of commitment, hoping to deter
attacks by demonstrating a plausible resolution to respond swiftly and
in kind. This process of automation and AI delegation would thus
perpetuate, despite it being increasingly against the collective
good.<br />
Ultimately, as with economic automation, military AI arms races could
result in humans being unable to keep up. The pace and complexity of
warfare could ascend out of human reach to where we are no longer able
to comprehend or intervene. This could be an irreversible step putting
us at high risk of catastrophic outcomes.</p>
<h3 id="summary-1">Summary</h3>
<p>The Iterated Prisoner’s Dilemma involves repeated rounds of the
Prisoner’s Dilemma game. This iteration offers a chance for agent
cooperation but doesn’t ensure it. There are different strategies by
which agents can attempt to maximize their overall payoffs. These
strategies can be studied by competing agents against one another in
tournaments, where each agent competes against others in multiple rounds
before switching partners.<br />
This provides cause for concern about multi-AI agent futures. The
“races” governing the production, deployment and adoption of AI
technologies, in both corporate and military settings, have the
potential to exacerbate many of the intrinsic risks from AI. The
dynamics we have explored in this section might cause competing agencies
to cut corners on safety, and to escalate weaponized AI applications and
automate warfare. These are two examples of how competitive pressures,
modeled as iterated interactions between agents, can generate races
which increase the risk of catastrophe for everyone. Fostering
cooperation between different parties—–human individuals, corporations,
nations, and AI agents—–is vital for ensuring our collective safety.</p>
<h2 id="collective-action-problems">Collective Action Problems</h2>
<p>We began our exploration of game theory by looking at a very simple
game, the Prisoner’s Dilemma. We have so far considered two ways to
model real-world social scenarios in more detail. First, we explored
what happens when two agents interact <em>multiple times</em> (such as
an Iterated Prisoner’s Dilemma match). Second, we introduced a
population of <em>more than two</em> agents, where each agent switches
partners over time (such as an Iterated Prisoner’s Dilemma tournament).
Now we move beyond pairwise interactions, to interactions that
simultaneously involve more than two agents. We consider what happens
when an agent engages in repeated rounds of the Prisoner’s Dilemma
against multiple opponents at the same time.<br />
One class of scenarios that can be described by such a model is
<em>collective action problems</em>. Throughout this section, we first
discuss the core characteristics of collective action problems. Then, we
introduce a series of real-world examples to highlight the ubiquity of
these problems in human society, as well as showing how AI races can be
modeled in this way. Following this, we transition to a brief discussion
of common pool resource problems to further illustrate the difficulty
with which rational agents, especially AI agents, may secure
collectively good outcomes. Finally, we conclude with a detailed
discussion of flash wars and autonomous economies to show how in a
multi-agent setting, AIs might pursue behaviors or tactics that result
in catastrophic or existential risks to humans.</p>
<h3 id="introduction-2">Introduction</h3>
<p>This first section explores the nature of collective action problems.
We begin with a simple example of a collaborative group project. Through
this, we explore how individual incentives can sometimes clash with what
is in the best interests of the group as a whole. These situations can
motivate individuals to act in ways that negatively impact all of the
population.</p>
<p><strong>A collective action problem is like a group-level Iterated
Prisoner’s Dilemma.</strong> In the Iterated Prisoner’s Dilemma, we saw
how a pair of rational agents can tend towards outcomes that are
undesirable for both. Now let us consider social interactions between
more than two agents. When an individual engages with multiple partners
simultaneously, they may still converge on Pareto inefficient Nash
equilibria. In fact, with more than two agents, cooperation can be even
harder to secure. We can therefore model collective action problems as
an Iterated Prisoner’s Dilemma in which more than two prisoners have
been arrested: If enough of them decide to defect on their partners, all
of them will suffer the consequences.</p>
<p><strong>Example: group projects.</strong> A typical example of a
collective action problem is that of a collaborative project. A group
working together towards a shared goal often encounters a problem: not
everyone pitches in. Some group members take advantage of the rest,
benefiting from the work others are doing without committing as much
effort themselves. The implicit reasoning behind the behavior of these
“slackers” is as follows. They want the group’s goal to be achieved, but
they would prefer this to happen without costing them much personal
effort. Just as with the Prisoner’s Dilemma, “slacking” is their
dominant strategy. If the others work hard and the project is completed,
they get to enjoy the benefits of this success without expending too
much effort themselves. If the others fail to work hard and the project
is not completed, they at least saved themselves the effort they might
otherwise have wasted.<br />
As groups increase in size and heterogeneity, complexity increases
accordingly. Agents in a population may have a diverse set of goals.
Even if the population can agree on a common goal, aligning diverse
agents with this goal can be difficult. For example, even when the
public expresses strong and widespread support for a political measure,
their representatives often fail to carry it out.</p>
<h3 id="sec:formalization">Formalization</h3>
<p>Here, we formalize our model of collective action problems. We look
more closely at the incentives governing individual choices, and the
effects these have at the group level. We examine how the behavior of
others in the group can alter the incentives facing any individual, and
how we can (and do) use these mechanisms to promote cooperative behavior
in our societies.</p>
<p><strong>Each agent must choose whether to contribute to the common
good.</strong> As in the Prisoner’s Dilemma, each agent must choose
which of two actions to take. An agent can choose to
<strong>contribute</strong> to the common good, at some cost to
themselves. The alternative is for the agent to choose to <strong>free
ride</strong>, benefiting from others’ contributions at no personal
cost. Free riders impose <strong>negative
externalities</strong>–—collateral damage for others in pursuit of
private benefit—–on the group as a whole by choosing not to pitch
in.</p>
<p><strong>Free riding is the dominant strategy.</strong> For now, let
us assume that free riding increases an agent’s own personal benefit,
regardless of whether the others contribute or free ride: it is the
dominant strategy. If an agent’s contribution to the common good is
small, then choosing <em>not</em> to contribute does not significantly
diminish the collective good, meaning that an agent’s decision to free
ride has essentially no negative consequences for the agent themself.
Thus, the agent is choosing between two outcomes. The first outcome is
where they gain their portion of the collective benefit, and pay the
small cost of being a contributor. The other outcome is where they gain
this same benefit, but save themselves the cost of contributing.</p>
<p><strong>Free riding can produce Pareto inefficient outcomes.</strong>
Just as how both agents defecting in the Prisoner’s Dilemma produces
Pareto inefficiency, free riding in a collective action problem can
result in an outcome that is bad for all. In many cases, some agents can
free ride without imposing significant externalities on everyone else.
However, if sufficiently many agents free ride, this diminishes the
collective good by leading to no provision of a public good, for
instance. With sufficient losses, the agents will all end up worse than
if they had each paid the small individual cost of contributing and
received their share of the public benefit. Importantly, however, even
in this Pareto inefficient state, free riding might still be the
dominant strategy for each individual, since the cost of contributing
outweighs the trivial increase in collective good they would contribute
by contributing. Thus, escaping undesirable equilibria in a collective
action problem can be exceedingly difficult; see Figure <a
href="#fig:collective" data-reference-type="ref"
data-reference="fig:collective">8</a>.<br />
</p>
<figure id="fig:collective">
<img src="images/multiagent-dynamics/image6.png" />
<figcaption>An abstract collective action problem</figcaption>
</figure>
<p><em>A) All agents contribute, maximizing the collective good. B) A
minority of agents switch to free riding, gaining higher payoffs at no
cost to the collective good. C) More agents switch to free riding. Now
the collective good begins to diminish, lowering the payoffs for
contributors and free riders alike. D) Almost everyone free rides. All
have lower payoffs than if they were all to contribute
instead.</em><br />
We can illustrate a collective action problem using the simple payoff
matrix below. In the matrix, “b” represents the payoff an agent receives
when everyone else cooperates (the collective good divided between the
number of agents) and “c” represents the personal cost of cooperation.
As the matrix illustrates, the dominant strategy for a rational agent
(“you”) here is to free ride whether everyone else contributes or free
rides.<br />
</p>
<table>
<caption>Free Riding in a Group Environment</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: left;"><strong>The rest of the group
contributes</strong></th>
<th style="text-align: left;"><strong>The rest of the group free
rides</strong></th>
<th style="text-align: left;"><strong>Some contribute; others free
ride</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>You contribute</strong></td>
<td style="text-align: left;">b - c</td>
<td style="text-align: left;">-c</td>
<td style="text-align: left;">&lt; b - c</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>You free ride</strong></td>
<td style="text-align: left;">b</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">&lt;b</td>
</tr>
</tbody>
</table>
<p><strong>Agents’ incentives depend on the behavior of other
agents.</strong> Agents in collective action problems can be aware of
the choices other agents make, which can affect their strategies and
behavior over time. For example, the ratio of defectors to cooperators
in a population can affect the degree to which cooperation is achieved.
When rational agents interact with each other, they may be inclined to
shift their strategies to more favorable ones with higher individual
payoffs: they may realize that other agents are utilizing more
successful strategies, and thus choose to adopt them. If defectors
dominate the population initially, and the initial individual costs of
cooperation outweigh the collective benefits of cooperation, then the
population may tend towards an uncooperative state. In simple terms,
collective action problems cannot be solved without cooperation.</p>
<p><strong>Mutual and external coercion.</strong> We can increase the
probability of cooperation by generating incentives that lower the
individual cost of cooperation and increase the individual cost of
defection. There are two ways we may go about this: mutual or external
coercion. <em>Mutual coercion</em> generates cooperative incentives by
establishing communal, societal, and reputational norms. <em>External
coercion</em> generates cooperative incentives through external
intervention, by developing regulations that incentivize collective
action through mandates, sanctions, and legislature, making cooperation
a necessity in certain cases. Below, we illustrate some real-world
scenarios in further detail.</p>
<h3 id="real-world-examples-of-collective-action-problems">Real-World
Examples of Collective Action Problems</h3>
<p>Many large-scale societal issues can be understood as collective
action problems. This section explores collective action problems in the
real world: climate change, public health, and democratic voting. We end
by briefly looking at AI races through this same lens.</p>
<p><strong>Public health.</strong> We can model some public health
emergencies, such as disease epidemics, as collective action problems.
The COVID-19 pandemic took the lives millions worldwide. Some of these
deaths could have been avoided with stricter compliance with public
health measures such as social distancing, frequent testing, and
vaccination. We can model those adhering to these measures as
“contributing” (by incurring a personal cost for public benefit) and
those violating them as “free riding.”<br />
Assume that everyone wished the pandemic to be controlled and ultimately
eradicated, that complying with the suggested health measures would have
helped hasten this goal, and that the benefits of collectively
shortening the pandemic timespan would have outweighed the personal
costs of compliance with these measures (such as social isolation).
Everyone would prefer the outcome where they all complied with the
health measures over the one where few of them did. Yet, each person
would prefer still better the outcome where <em>everyone else</em>
adhered to the health measures, and <em>they alone</em> were able to
free ride. Violating the health measures was therefore the dominant
strategy, and so many people chose to do this, imposing the negative
externalities of excessive disease burden on the rest of their
community.<br />
We used both mutual and external mechanisms to coerce people to comply
with public health measures in the pandemic. For example, some
communities adjusted their social norms (mutual coercion) such that
non-compliance with public health measures would result in damage to
one’s reputation. We also required proof of vaccination for entry into
desirable social spaces (external coercion), among many other
requirements.</p>
<p><strong>Anthropogenic climate change.</strong> In 2021, a majority of
those surveyed worldwide reported wanting to avert catastrophic
anthropogenic climate change. Most, however, chose not to act in
accordance with what they believed necessary to achieve this goal. The
consumption of animal products typically entails far higher greenhouse
gas emissions and environmental damage than plant-based alternatives.
The use of public over private transport similarly reduces personal
carbon footprints dramatically. To avoid the costs of taking these
actions, such as changing routines and compromising on speed or ease,
most people do not change their diets or transport habits. Various
behaviors that increase pollution can be viewed as “free riding.” Since
this is the dominant strategy for each agent, most choose to do this,
resulting in ever-worsening climate change, imposing risks on the global
population.<br />
We could disincentivize excessive meat eating and private transport
using external and mutual coercion. In this example, external coercion
could include lowering bus and train fares and enhancing existing
infrastructure through government subsidies, as well as implementing
fuel taxes on private vehicles. Mutual coercion could include changing
social norms to consider excessive meat eating or short-haul flying
unacceptable.</p>
<p><strong>Democracy.</strong> We can model the maintenance of a
democracy as a set of collective action problems. There are many
situations in which certain actions might provide an individual with
immediate benefits, but would incur longer-term costs on the larger
group if more people were to take these actions. For example, a voting
population must maintain certain norms in order to keep its democracy
functioning. One of these norms is to vote only for candidates who will
not undermine democratic processes, even if others have desirable
traits.<br />
Choosing whether or not to participate in an election at all can
similarly be viewed as a collective action problem. The outcome of an
election is determined by the votes of individuals, each of which has a
choice to either vote or abstain. The results of the election are
determined by the votes of those who choose to participate, and the
costs of participating in the election are carried by citizens
themselves, such as the time and effort required to register and cast a
vote. When large enough numbers of citizens decide to abstain from
voting, the collective outcome of an election may not accurately reflect
the preferences of the population: by acting in accordance with their
rational self interest, citizens may contribute to a suboptimal
collective outcome.</p>
<p><strong>Common pool resource problem.</strong> Rational agents are
incentivized to take more than a sustainable amount of a shared
resource. This is called a <em>common pool resource problem</em> or
<em>tragedy of the commons problem</em>. We refer to a common pool
resource becoming catastrophically depleted as collapse. Collapse occurs
when rational agents, driven by their incentive to maximize personal
gain, tip the available supply of the shared resource below its
sustainability equilibrium <span class="citation"
data-cites="diamond2011collapse"></span>. Below, we further illustrate
how complicated it is to secure collectively good outcomes, especially
when rational agents act in accordance with their self-interest. Such
problems are prevalent at the societal level, and often bear
catastrophic consequences. Thus, we should not eliminate the possibility
that they may also occur with AI agents in a multi-agent setting.<br />
For example, rainforests around the world have been diminished greatly
by deforestation practices. While these forests still exist as a home to
millions of different species and many local communities, they may reach
a point at which they will no longer be able to rejuvenate themselves.
If these practices are sustained, the entire ecosystem these forests
support could collapse. Common pool resource problems exemplify how
agents may bring about catastrophes even when they behave rationally and
in their self-interest, with perfect knowledge of the looming
catastrophe, and despite the seeming ability to prevent it. They further
illustrate how complicated it can be to secure collectively good
outcomes and how rational agents can act to the detriment of their own
group. As with many other collective action problems, we can’t expect to
solve common pool resource problems by having AIs manage them. If we
simply pass the buck to AI representatives, the AIs will inherit the
same incentive structure that produces the common pool resource problem,
and so the problem will likely remain.</p>
<h3 id="sec:AI-races-Multiple">Tail Risk: AI Races Between More Than Two
Competitors</h3>
<p>In the previous section, we looked at how corporations and militaries
may compete with one another in “AI races.” We used a two-player
“attrition” bidding model to see why AI companies cut corners on safety
when developing and deploying their technologies. We used another
two-player “security dilemma” model to understand how security concerns
motivate nations to escalate their military capabilities, even while
increasing the risks imposed on all by increasingly automating warfare
in this manner.</p>
<p>Here, we extend our models of these races to consider more than two
parties, allowing us to see them as collective action problems. First,
we look at how military AI arms races increase the risk of catastrophic
outcomes such as a <em>flash war</em>: a war that is triggered by
autonomous AI agents that quickly spirals out of human control <span
class="citation" data-cites="hendrycks2023overview"></span>. Second, we
explore how ever-increasing job automation could result in an
<em>autonomous economy</em>: an economy in which humans no longer have
leverage or control.</p>
<p><strong>Military AI arms race outcome: flash war.</strong> The
security dilemma model we explored in the previous section can be
applied to more than two agents. In this context, we can see it as a
collective action problem. Though all nations would be at lower risk if
all were to cooperate with one another (“contribute” to their collective
safety), each will individually do better instead to escalate their own
military capabilities (“free ride” on the contributions of the other
nations). Here, we explore one potentially catastrophic outcome of this
collective action problem: a flash war.<br />
As we saw previously, military AI arms races motivate nations to
automate military procedures. In particular, there are strong incentives
to integrate “automated retaliation” protocols. Consider a scenario in
which several nations have constructed an autonomous AI military defense
system to gain a defensive military advantage. These AIs must be able to
act on perceived threats without human intervention. Additionally, each
is aligned with a common goal: “defend our nation from attack.” Even if
these systems are nearly perfect, a single erroneous detection of a
perceived threat could trigger a decision cascade that launches the
nation into a “flash war.” Once one AI system hallucinates a threat and
issues responses, the AIs of the nations being targeted by these
responses will follow suit, and the situation could escalate rapidly. A
flash war would be catastrophic for humanity, and might prove impossible
to recover from.<br />
A flash war is triggered and amplified by successive interactions
between autonomous AI agents such that humans lose control of weapons of
mass destruction <span class="citation"
data-cites="critch2021multipolar"></span>. Any single military defense
AI could trigger it, and the process could continue without human
intervention and at great speed. Importantly, having humans in the loop
will not necessarily ensure our safety. Even if AIs only provide human
operators with instructions to retaliate, our collective safety would
rest on the chance that soldiers would willfully disobey their
instructions.<br />
Collective action between nations could avoid these and other dire
outcomes. Limiting the capabilities of their military AIs by decreasing
funding and halting or slowing down development would require that each
nation gives up a potential military advantage. In a high stakes
scenario such as this one, rational agents (nations) may be unwilling to
give up such an advantage because it dramatically increases the
vulnerability of their nation to attack. The individual cost of
cooperation is high while the individual cost of defection is low, and
as agents continue to invest in military capabilities, competitive
pressures increase, which further exacerbate costs of cooperation –—
thereby disincentivizing collective action. While the collective
benefits of cooperation would drastically reduce the catastrophic risks
of this scenario in the long-term, they may not outweigh the
self-interest of rational agents in the short-term.</p>
<p><strong>Corporate AI race outcome: autonomous economy.</strong> As
AIs become increasingly effective at carrying out human goals, they may
begin to out-perform the average human at an increasing number and range
of jobs, from personal assistants to executive decision-makers. To reap
the benefits of these faster and more effective workers, companies will
likely continue to automate economically valuable functions by
delegating them to AI agents. Ultimately, this could lead to the global
economy becoming “autonomous,” with humans no longer able to steer or
intervene <span class="citation"
data-cites="alexander2016ascended"></span>.<br />
Such an autonomous economy would be a catastrophe for humanity. Like
passengers in an autonomous vehicle, our safety and destination would
rest with the AI systems now acting without our supervision. Our future
would be determined by the behavior and outputs of this autonomous
economy. If the AI agents engaged in this economy were to have
undesirable goals or evolve selfish traits —– a possibility we examine
in the next section of this chapter –— humanity would be unable to
prevent the harms they cause. Even if the AIs themselves are
well-aligned to our goals, the economic system itself may produce
extremely undesirable outcomes. In this section, we have examined many
examples of how macrobehavior can differ dramatically from micromotives.
A population of individuals can tend towards states that are bad for
everyone and yet be in stable equilibria. This could happen just the
same with AI representatives acting on humanity’s behalf in an
autonomous economy.<br />
Just as with the military AI arms race, we can model how an autonomous
economy might be brought about using the security dilemma model. As in
the previous example, if we expand this model to more than two agents,
we can see it as a collective action problem in which competitive
pressures drive different parties to automate economic functions out of
the need to “keep up” with their competitors. Under this model, we can
see how companies must choose whether to maintain human labor
(“contributing”) or automate these jobs using AI (“free riding”).
Although all would prefer the outcome in which the calamity of an
autonomous economy is avoided, each would individually prefer to have a
competitive advantage and not risk being outperformed by rivals who reap
the short-term benefit of using AIs. Thus, economic automation is the
dominant strategy for each competitor. Repeated rounds of this game in
which a sufficient number of agents free ride would drive us towards
this disaster. In each successive round, it would become progressively
more difficult to turn back, as we come to rely increasingly on more
capable AI agents.</p>
<p><strong>Increasing AI autonomy increases the risk of catastrophic
outcomes.</strong> As AIs become more autonomous, humans may delegate
more decision-making power to them. If AIs are able to successfully and
consistently attain the high-level objectives given to them by humans,
we may be more inclined to begin providing them with open-ended goals.
If AIs achieve these goals, humans might not be privy to the process
they follow and may overlook potential harms, as we saw in both the
autonomous economy and flash war examples. Moreover, adaptive
AIs–—systems that actively adjust their computational design,
architecture and behavior in response to new information or changes in
the environment—–could adapt at a much faster rate than humans. The
possibility of self-improvement among such AIs would further exacerbate
this problem. Adaptive AIs could develop unanticipated emergent
behaviors and strategies, making them deeply unpredictable. Humans could
be inclined to accept these negative behaviors in order to maintain a
competitive advantage in the short-term.</p>
<p><strong>Reducing competitive pressures could foster collective
action.</strong> The security dilemma model shows how nations can be
motivated to escalate their offensive capabilities out of the perception
that their competitors are doing the same. However, by signaling the
opposite, we might be able to produce the reverse effect, such as
military de-escalation or an increase in AI safety standards. For
instance, whether different nations will acquiesce to a shared
international standard for AI regulation may depend on whether the
nations are individually signaling their willingness to regulate in
their own jurisdiction in the first place. If one nation perceives that
others are engaging in strict domestic regulation, they might see this
as a credible signal of commitment to an international standard. By
easing the competitive pressures, we might be able to foster collective
action to avoid driving up the collective risk level.</p>
<h3 id="summary-2">Summary</h3>
<p>We observe important and intractable collective action problems in
many domains of life, such as environmental degradation, pandemic
responses, maintenance of democracies, and common pool resource
depletion. We can understand these as Iterated Prisoner’s Dilemmas with
many more than two agents interacting simultaneously in each round of
the game. As before, we see that “free riding” can be the dominant
strategy for an individual agent, and this can lead to Pareto
inefficient outcomes for the group as a whole. We can use the mechanisms
of mutual and external coercion to incentivize agents to cooperate with
each other and achieve collectively good outcomes.<br />
If we expand our models of AI races to include more than two agents, we
can understand the races themselves as collective action problems, and
examine how they exacerbate the risk of catastrophe. One example is how
increasingly automating military protocols increases the risk of a
“flash war”. Similar dynamics of automation in the economic sphere could
lead to an “autonomous economy.” Either outcome would be disastrous and
potentially irreversible, yet we can see how competitive pressures can
drive rational and self-interested agents (such as nations or companies)
down a path towards these calamities.</p>
<h2 id="evolutionary-game-theory">Evolutionary Game Theory</h2>
<p>We began our discussion of game theory by exploring the Prisoner’s
Dilemma, a simple game that involves two agents who interact only once.
To model real-world multi-agent dynamics more closely, we first built on
this by iterating the two agents’ interactions. We next introduced more
than two agents into the game. Now, we introduce a third elaboration:
the ability for agents to adapt their strategies. We look at how the
multi-agent dynamics we have explored so far change when the agents
concerned can adapt their behaviors.<br />
This section uses ideas from <em>Evolutionary Game Theory</em>. This
field was born from a merging of economics and biology in the 1970s when
John Maynard-Smith and George Price found a way to describe living
organisms’ traits and behaviors as game strategies. Using evolutionary
game theory, we explore how evolutionary forces alter the multi-agent
dynamics in the Iterated Prisoner’s Dilemma. In particular, we look at
the conditions under which a population may cycle through different
strategy compositions. We then move to a different game, known as the
“Hawk-Dove game,” to illustrate how identical agents may tend towards
Nash equilibria in which they use a range of different strategies.<br />
We use these games to understand why predicting the behavior of many
different AI agents may be very difficult. AI agents may evolve to
employ extremely harmful strategies in repeated social interactions,
including trying to extort or deceive others.By enhancing our
understanding of these factors, we may better anticipate how to address
potential risks stemming from multi-AI agent interactions.</p>
<h3 id="sec:dilemma-tournaments">Evolutionary Iterated Prisoner’s
Dilemma Tournaments</h3>
<p>In this section, we investigate how evolutionary forces change the
dynamics of a population of agents engaged in pairwise interactions. We
explore how these forces can prevent a population from reaching a stable
equilibrium at all. We see why, if AI agents are subject to evolutionary
pressures, they may change how they interact with others in ways that
are difficult to predict.</p>
<p><strong>Evolutionary tournaments: agents can adapt their
strategies.</strong> Following his first round of Iterated Prisoner
Dilemma tournaments, Axelrod later ran a second set which he
re-structured to resemble the dynamics of natural selection <span
class="citation" data-cites="axelrod1981evolution"></span>. We define
these “evolutionary tournaments” as ones where a strategy’s success in
one round determined the number of agents who used that strategy in the
next round. The most numerous strategy at the end was the winner. Though
strategies that have been successful in previous tournaments, such as
<em>tit-for-tat</em>, were often still successful in evolutionary
tournaments, their victory was by no means assured. The outcomes of
these new tournaments were very sensitive to changes in conditions,
which we explore next. Figure <a
href="#wrap-fig:evolutionary-tournament" data-reference-type="ref"
data-reference="wrap-fig:evolutionary-tournament">[wrap-fig:evolutionary-tournament]</a>
shows the difference between these two kinds of Iterated Prisoner’s
Dilemma tournament.</p>
<p><strong>Cyclical dynamics can emerge; uncooperative strategies can
persist.</strong> An interesting aspect of evolutionary tournaments is
that they test how well a strategy performs against itself. As the
success of a particular strategy increases, so does its relative
frequency in the population. Thus, agents using this strategy will play
against each other more often. In this context, strategies like
<em>tit-for-tat</em> are vulnerable to what we call the “echo” problem
<span class="citation" data-cites="nowak1992titfortat"></span>. Consider
a match between two agents both playing <em>tit-for-tat</em>. If one
defects—–either by accident, or by design if they are a less cooperative
<em>tit-for-tat</em> variant—–the other will retaliate by defecting as
well. This will repeat, triggering a sequence of mutual defections. Both
agents are thus locked into playing <em>always defect</em>. This gives
them a lower score than if one were to forgive the other, allowing them
to return to a state of cooperation. Thus, more “forgiving” strategies
can fare better in a population with a high frequency of agents playing
<em>tit-for-tat</em> by cutting short any string of mutual defections.
This phenomenon is related to "frequency dependent selection," which we
explore later in this chapter. More forgiving strategies include
<em>generous tit-for-tat</em>, which occasionally overlooks partner
defection.<br />
In an evolutionary tournament, agents can switch to more successful
strategies. So, if the strategy generous <em>tit-for-tat</em> is more
successful than “normal” <em>tit-for-tat</em>, it may eventually become
more frequent than <em>tit-for-tat</em> in the population <span
class="citation" data-cites="Nowak2006FiveRF"></span>. However, if
generous <em>tit-for-tat</em> becomes a sufficiently common strategy in
turn, agents playing <em>always cooperate</em> can achieve just as high
a score, making this strategy popular too. If <em>always cooperate</em>
reaches a sufficient frequency in the population, this creates an
opportunity to exploit all these cooperators by switching to an
uncooperative strategy such as <em>always defect</em>. If enough agents
take this opportunity and switch to playing <em>always defect</em>,
other agents who play <em>tit-for-tat</em> will once again start to
outcompete these defectors (as outlined previously). Thus, a cycle can
ensue, in which each strategy is replaced by another; see Figure <a
href="#fig:cycle" data-reference-type="ref"
data-reference="fig:cycle">9</a>. Many other examples of cyclical
dynamics have been identified in Iterated Prisoner’s Dilemma tournaments
<span class="citation" data-cites="garcia2018strategy"></span>.<br />
</p>
<figure id="fig:cycle">
<img src="images/multiagent-dynamics/Evolutionary.png" />
<figcaption>The population may cycle through different strategy
compositions.</figcaption>
</figure>
<p>In an evolutionary Iterated Prisoner’s Dilemma tournament, cyclical
dynamics may emerge. Shown here is one such example, wherein each of the
four boxes represents a different population strategy composition, in
which the most frequent strategy is the one displayed in the box. For
example, the bottom right green box represents a population strategy
composition in which the majority of the agents are using the strategy
<em>always cooperate</em>. A population in any of the four strategy
compositions shown may be replaced by another composition (as shown by
the arrows).<br />
</p>
<h3 id="sec:evolutionary-stable-strategies">Evolutionarily Stable
Strategies and Frequency-Dependent Selection</h3>
<p>In this section, we investigate how different strategies may come to
replace one another within an evolving population. We start with the
classic evolutionary game theoretic model known as the “Hawk-Dove game”
to demonstrate how identical agents may reach a Nash equilibrium in
which they employ different strategies to one another. This suggests
that if AI agents are subject to evolutionary forces, it may be
difficult to predict their behavior, because the incentives facing each
may depend on what others in the population are doing.</p>
<p><strong>Example: the Hawk-Dove game.</strong> Let us consider a
simple evolutionary game-theoretic scenario. A group of agents is
engaged in frequent, pairwise contests over an indivisible resource
(such as nesting space). Each agent can choose one of two strategies.
<em>“Hawks”</em> always fight for the prize and only back down when
injured. <em>“Doves”</em> only threaten but never actually fight. We
call this the ‘Hawk-Dove’ game <span class="citation"
data-cites="Smith1982EvolutionAT"></span>.<br />
In a one-on-one game, a <em>Hawk</em> will always beat a <em>Dove</em>
without any cost. When two agents choose the same strategy, each has a
50% chance of winning or losing. But there is a difference in outcomes.
The loser of a <em>Dove-Dove</em> contest suffers no injury costs.
Whereas the loser of a <em>Hawk-Hawk</em> contest not only fails to gain
the valuable resource but also incurs an injury cost from fighting. We
show these two strategies’ average losses and gains in the payoff matrix
below (where b is the benefit of winning the resource, and c is the cost
incurred by fighting). We assume that b&lt;c, all <em>Hawks</em> have
equal fighting ability, and all <em>Doves</em> have equal threatening
ability.<br />
</p>
<div id="tab:hawk-dove">
<table>
<caption>Hawk-Dove payoff matrix</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Hawk <strong>(opponent)</strong></th>
<th style="text-align: center;">Dove (<strong>Opponent</strong>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Hawk <strong>(self)</strong></td>
<td style="text-align: center;"><span
class="math inline">$\frac{1}{2}(b-c)$</span></td>
<td style="text-align: center;"><span
class="math inline"><em>b</em></span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Dove <strong>(self)</strong></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"><span
class="math inline">$\frac{1}{2}b$</span></td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:hawk-dove" label="tab:hawk-dove"></span></p>
<p><strong>Neither <em>Hawk</em> nor <em>Dove</em> is the dominant
strategy.</strong> If all agents were Hawks, everyone would fight each
other for the resource and only stop when hurt. Their average gain would
be <span class="math inline">$\frac{1}{2}(b-c)$</span>. But since the
price of fighting is more than the resource’s value <span
class="math inline">(<em>b</em>&lt;<em>c</em>)</span>, this payoff would
be negative. If one agent switched to being a <em>Dove</em>, they would
avoid the cost of fighting, but they would never win the resource. Their
average payoff would be <span class="math inline">0</span>, which is
better than the payoffs the <em>Hawks</em> would be getting. Also, this
gain would increase as more agents choose Dove, as their average payoff
when meeting <em>Doves</em> would be <span
class="math inline">$\frac{1}{2}b$</span>. Therefore, <em>Doves</em>
could <em>invade</em> a <em>Hawk</em>-only group.<br />
Similarly, if all agents were <em>Doves</em>, an agent who switched to
<em>Hawk</em> would gain more without paying a price for fighting, as
<em>Doves</em> don’t fight back. So, <em>Hawks</em> could invade a
<em>Dove</em>-only group. Hence, neither strategy is always better. The
best strategy for an agent depends on what strategies the other agents
are using. So the population strategy composition will include both
<em>Doves</em> and <em>Hawks</em>.</p>
<p><strong>Evolutionarily stable strategies.</strong> When the members
of an evolving population adopt a strategy that makes it uninvadable by
any rare novel mutants that may arise, we call this state an
<em>evolutionarily stable strategy (ESS)</em> <span class="citation"
data-cites="Smith1982EvolutionAT"></span>. An ESS is a kind of Nash
Equilibrium: when agents are at this equilibrium, they cannot improve
their success by switching to any other strategy. In the Nash equilibria
we have considered so far in this chapter, the agents have tended to use
the same strategy as one another –— if an agent used a different
strategy, they would gain a lower payoff. However, a population can
sometimes stably contain more than one strategy. Agents can use
different strategies that are equally successful. For example, in the
Hawk-Dove game, where fighting is more costly than the prize <span
class="math inline">(<em>b</em>&lt;<em>c</em>)</span>, the population
will reach a stable equilibrium in which there are both <em>Hawks</em>
and <em>Doves</em>.</p>
<p><strong>Frequency-dependent selection.</strong> This kind of
equilibrium, where agents use different strategies, is stabilized by the
success of either strategy decreasing as it becomes more common, and
increasing as it becomes rarer. When the success of a strategy depends
on how common it is compared to others, we call this
<em>frequency-dependent selection</em>. A strategy may succeed more as
it becomes more common. For example, if more wasps evolve bright colors
to warn predators, the warning works better. Alternatively, a strategy’s
success might drop as it becomes more common. For example, certain
snails have evolved to look like inedible objects. Frequency-dependent
selection has caused them to look very different from one another. The
more often a camouflage strategy is used, the more chances predators
have to see through it. As a result, snails that “choose” (evolve) less
popular camouflage strategies do the best. This results in a dynamic
state in which the no snail color spreads to fixation in the population.
Instead, snails of multiple colors coexist stably; see Figure <a
href="#fig:freq-selection" data-reference-type="ref"
data-reference="fig:freq-selection">10</a>.<br />
The evolutionary biologist Richard Dawkins explains this using the
following fictional scenario <span class="citation"
data-cites="dawkins2015evolutionarily"></span>. A population of seagulls
lives on the coast where their only source of food is the fish in the
sea. Each seagull can choose whether to behave as a “<em>fisher</em>” or
as a “<em>pirate</em>”. <em>Fishers</em> catch fish themselves;
<em>pirates</em> wait for <em>fishers</em> to catch a fish, then steal
it from them. Neither of these is an ESS. In a population composed
entirely of fishers, a seagull who mutates and becomes a <em>pirate</em>
will benefit hugely; piracy is a profitable strategy because
<em>pirates</em> do not expend energy fishing for themselves. Thus, the
<em>pirate</em> strategy can invade the <em>fisher</em> strategy. On the
other hand, a population composed entirely of <em>pirates</em> is
invadable by a <em>fisher</em>. Without any fishers to prey on, the
<em>pirates</em> have no fish to eat. A seagull who mutates and becomes
a <em>fisher</em> can access food for themself, giving them an
advantage. Thus, neither strategy is an ESS by itself. Some proportion
of the population opt for one strategy, and another proportion opts for
the other. Thus, this process of frequency-dependent selection can
produce populations that stably contain more than two strategies.<br />
</p>
<figure id="fig:freq-selection">
<img src="images/multiagent-dynamics/Snails-graph.png" />
<figcaption>Frequency-dependent selection.</figcaption>
</figure>
<p><em>In each population, snails “choose to” evolve one of three colors
as camouflage: red, green, or blue. The frequency graph shows the
proportion of snails of each color. The black arrows indicate the
passage of time. The most successful color is indicated both by a gray
border and a gray arrow. Whichever color is rarest in one iteration
bestows the highest fitness, as predators are least used to spotting
snails of that color. Thus, more snails use this color in the following
generation. In this way, the population cycles through different color
compositions.</em><br />
Frequency-dependent selection in game theory. We have encountered
frequency-dependent selection already in this section. In Section <a
href="#sec:formalization" data-reference-type="ref"
data-reference="sec:formalization">2.5.2</a>, we briefly discussed how
the ratio of contributors to free-riders in the population can alter the
incentives for the agents engaged in a collective action problem. In
Section <a href="#sec:dilemma-tournaments" data-reference-type="ref"
data-reference="sec:dilemma-tournaments">2.6.1</a>, we used evolutionary
Iterated Prisoner’s Dilemma tournaments to model the dynamics of natural
selection, and found that there may be no stable equilibrium state.
Instead, the best strategy for an agent to employ may depend on what
others in the population are doing. In these cases, the population may
cycle or undulate through different strategy compositions. Whichever
strategy is most common may become less successful than a rarer
alternative.</p>
<p><strong>Identical AI agents may tend towards different
strategies.</strong> A Nash equilibrium, or ESS, doesn’t always mean
that all agents will use the same strategy. Frequency-dependent
selection might cause AI agents to adopt different but equally
successful strategies. Under certain conditions, like the size and
variety of a population, harmful strategies like <em>extortion</em> can
persist by stably cycling through different proportions of agents as the
population evolves.</p>
<p><strong>Evolutionarily stable strategies can be Pareto
inefficient.</strong> Note that in the Hawk-Dove game, all agents would
be better off if they were <em>Doves</em>, as every agent that plays
<em>Hawk</em> lowers the average payoff for them all. Just as with the
Prisoner’s Dilemma, the Nash equilibrium is Pareto inefficient. Hence,
the ESS is not necessarily ‘good’ for the population, and what is good
for the population is not necessarily evolutionary stable. <em>A
population of altruistic AIs, for example, is not evolutionarily
stable</em>, as that population is easily invadable; we explore this
point in greater detail in “Evolutionary Pressures - Section 3.3.” This
makes it likely that evolution is incompatible with achieving ideal
outcomes.</p>
<h3 id="sec:extortion">Extortion</h3>
<p>We have seen how frequency-dependent selection can generate a
population of agents using different strategies to one another. This can
include strategies which we would consider harmful, even immoral. In
this section, we examine one such possibility: extortion.</p>
<p><strong><em>Extortion</em> strategies in the Iterated Prisoner’s
Dilemma.</strong> In the real world, we describe the use of threats to
force a victim to take an action they would otherwise not want to take
(such as to relinquish something valuable) as “extortion.” Examples
include criminal organizations ransoming those they have kidnapped to
extort their families for money in exchange for their safe return.<br />
In the Iterated Prisoner’s Dilemma, there is a set of <em>extortion</em>
strategies that bear similarity to this real-world phenomenon. An agent
playing the game can use an <em>extortion</em> strategy to ensure that
their payoff in any match is higher than their partner’s <span
class="citation" data-cites="press2012iterated"></span>. The
extortionist achieves this by acting similarly to an agent using
<em>tit-for-tat</em>, responding to like with like. However, the
extortionist will occasionally defect even when their partner has been
cooperative. Extortionists effectively calculate the maximum number of
defections they can get away with without annihilating the motivation of
their partner to continue cooperating with them. They decide whether to
cooperate or defect using a set of probabilities. The most recent
interaction with their partner determines which probability they select.
An example strategy is shown in Figure <a href="#fig:extort-2"
data-reference-type="ref" data-reference="fig:extort-2">11</a>. An
extortionist’s partner is incentivized to acquiesce to the extortion
since deviating in any way will yield them a lower payoff. However, in
maximizing their own score, they attain an even higher score for the
extortionist. An extortionist thus scores higher than most of its
partners in Iterated Prisoner’s Dilemma matches.<br />
</p>
<figure id="fig:extort-2">
<img src="images/multiagent-dynamics/The-extort-strategy.png" />
<figcaption>The <em>Extort-2</em> strategy - adapted from <span
class="citation" data-cites="stewart2012extortion"></span></figcaption>
</figure>
<p>Shown is an extortion strategy called <em>Extort-2</em>, from the
point of view of the extortionist. “You” are the agent using the
<em>Extort-2</em> strategy, and “they” are your partner. As with all
<em>extortion</em> strategies, <em>Extort-2</em> involves reacting
probabilistically to the most recent interaction had with a partner. As
an example, in the previous round, if the extortionist defected, but
their partner cooperated, the extortionist will cooperate with a
probability of <span class="math inline">$\frac{1}{3}$</span> in this
round.</p>
<p><strong>Extortion strategies rarely win tournaments but seldom die
out altogether.</strong> As we saw in Section <a href="#sec:tournaments"
data-reference-type="ref" data-reference="sec:tournaments">2.4.2</a>,
many uncooperative strategies may gain a higher score than most of their
partners in head-to-head matches, and yet still lose in tournaments. By
contrast, extortionists can be somewhat successful in tournaments under
certain conditions. Extortionists are vulnerable to the same problem as
many other uncooperative strategies: they gain low payoffs in matches
against other extortionists. Each will therefore perform less well as
the frequency of extortionists in the population increases. (This again
relates to "frequency dependent selection," which we discuss later in
this chapter.) Thus, <em>extortion</em> may arise as part of a cycle in
large populations of interacting agents, but it is rarely stable.
However, extortionists can persist for longer if they are sufficiently
unlikely to meet one another. For instance, where a sufficiently small
population of agents is engaged in a tournament, a single extortionist
can achieve very high payoffs by exploiting cooperative strategies.</p>
<p><strong>Extortion strategies perform even better in evolutionary
tournaments <span class="citation"
data-cites="hilbe2013evolution"></span>.</strong> Extortionists can be
highly successful in evolutionary tournaments. This is especially true
in tournaments with more detailed replications of evolutionary dynamics.
For instance, in some evolutionary tournaments, agents can produce
offspring who inherit both their strategy and score. With the addition
of noise, in the form of random mutations, extortionists enjoy great
success in this context. In even more sophisticated models that
replicate coevolutionary arms races, they are yet more successful.
Moreover, simulating a dramatic environmental change in these models
often produces an opportunity for exceptionally profitable extortion
tactics.</p>
<p><strong>AI agents may use extortion: evidence from the Iterated
Prisoner’s Dilemma.</strong> AI agents could use extortion in order to
gain resources or power. As we have seen, agents can succeed in the
Iterated Prisoner’s Dilemma by using <em>extortion</em> strategies. This
is particularly true if the extortionist is part of a small group, if
the social dynamics mirror evolution, or after major environmental
changes. These findings are extremely worrying as they could describe
future AI scenarios. Relationships might form among a small number of
powerful AI agents. These agents may undulate through desirable and
undesirable behaviors, or they might switch opportunistically to using
extortion tactics in the wake of changes to their environment. However,
since there are some fragile assumptions in these simple models, we must
also consider evidence from real-world agents.</p>
<p><strong>AI agents may use extortion: evidence from the real
world.</strong> The widespread use of extortion among humans outside the
world of game theoretic models suggests there is still a major cause for
concern. Real-world extortion can still yield results even when the
target is perfectly aware that it is taking place. The use of ransomware
schemes to extort private individuals and companies is increasing
rapidly. In fact, cybersecurity experts estimate that the annual
economic cost of ransomware activity is in the billions of US dollars.
Terrorist organizations such as ISIS rely on extortion through
hostage-taking for a large portion of their total income. The ubiquity
of successful extortion in so many contexts sets a powerful historical
precedent for its efficacy.</p>
<h3 id="tail-risk-extortion-with-digital-minds">Tail Risk: Extortion
With Digital Minds</h3>
<p>Here we examine the possibility of AI agents engaging in extortion to
pursue their goals. Though the probability of AI extortion may be low,
the impact could be immense. As an example, we consider the potential
for extortionist AIs to simulate and torture sentient digital minds as
leverage.</p>
<p><strong>Real-world extortion is a form of optimized
disvalue.</strong> An extortionist chooses to threaten their target
using a personalized source of concern. They optimize their extortion to
be prioritized over their target’s other concerns. Often, the worse the
outcome being threatened, the more likely the target is to acquiesce.
This incentivizes extortionists to threaten to bring about extremely
disvaluable scenarios. In order to be effective, extortionist AIs might
therefore leverage the threat of huge amounts of harm —– far more than
would likely come about incidentally, without design. If the disvaluable
outcome the extortionist has designed for their target is also
disvaluable to wider society, then we will share the potentially
enormous costs of any executed threats.</p>
<p><strong>AI extortion could entail torturing vast numbers of sentient
digital minds.</strong> Human extortionists often threaten to inflict
excruciating pain or death on those their victim cares about. AI
extortionists might engage in similar behaviors, threatening to induce
extreme levels of suffering, but on a vastly larger scale. This scale
could potentially exceed any in human history. This is because
extortionist AIs with greater-than-human technological capabilities
might be able to simulate sentient digital minds. The potential for
optimized disvalue in these simulations suggests near-unimaginable
horrors. Vast numbers of digital people in these simulated environments
could be subjected to immeasurably agonizing experiences.</p>
<p><strong>Simulated torture at this scale could make the future more
disvaluable than valuable.</strong> Simulations designed for the purpose
of extortion would likely be far more disvaluable than simulations which
contain disvalue unintentionally. The simulation’s designer would likely
be able to choose what kinds of objects to simulate, so they could avoid
wasting energy simulating non-sentient entities such as inanimate
objects. Moreover, the designer could ensure that these sentient
entities experience the greatest amount of suffering possible for the
timespan of the simulation. They might even be able to simulate minds
capable of more disvaluable experiences than have ever existed
previously, deliberately designing the digital entities to be able to
suffer as greatly as possible. Put together, a simulation optimized for
disvalue could produce several orders of magnitude more disvalue than
anything in history. This would be unprecedented in humanity’s history,
and could make a horrifying–—even net negative—–future.</p>
<p><strong>AI agents may be superhumanly proficient at wielding
extortion.</strong> Future AI agents may far exceed humans in their
ability to wield threats. One reason for this could be that they have
superhuman tactical capabilities, as some do already in competitive
games. Superior strategic intelligence could allow AI agents to conceive
and execute far more advanced programs of extortion than that of which
humans are generally capable. A second reason why AI agents may be
especially adept at employing threats is if they have superhuman
longevity or goal-preservation capabilities. With greater timespans
available, the action space for extorting targets is larger. Finally,
AIs may have technological capabilities that exceed those of current and
historical humans. This could widen the option space for AI extortion
still further.</p>
<p><strong>Extortion may be exceptionally effective against
AIs.</strong> Two goals of machine ethics are: 1) to foster in AI an
intrinsic value for humanity (and humanity’s values); 2) to make AI
agents that are unbiased. Both goals could result in AI agents being
more vulnerable to extortion than humans tend to be. Let us examine an
example of this for each goal.<br />
<em>Goal 1: Foster in AI an intrinsic value for humanity (and humanity’s
values).</em><br />
AI agents that value individual humans highly may be less prone to
“scope insensitivity.” This is the human bias of failing to “feel”
changes in the size of some value appropriately. Very small or very
large numbers often appear to us to be of similar size to other very
small or very large numbers, even when they actually differ by orders of
magnitude. Human scope insensitivity may provide some protection against
larger-scale extortion, as it lowers the motivation of extortionists to
increase the scale of their threats. It is possible that AI agents may
prioritize outcomes more accurately in accordance with their expected
value. If this is the case, they would likely be more responsive to high
stakes, and more vulnerable to large-scale extortion attempts.<br />
<em>Goal 2: Make AI agents that are unbiased.</em><br />
Unbiased AI agents may have far more altruistic values than any human or
institution. These agents may be extremely vulnerable to extortion in
the form of threats against their impartial moralities. Extortionist AI
agents could leverage the threat of extreme torture of countless digital
sentients in simulated environments to extort more morally impartial AI
targets. The execution of any such threat could immensely degrade the
value of the future.</p>
<p><strong>AI extortionists may execute higher-stakes threats more
frequently than humans.</strong> A successful act of extortion is the
deliberate creation of a state in which both the extortionists and their
targets prefer the outcome the extortionist demands. In some sense, both
parties therefore <em>want</em> the target to acquiesce to the extortion
and the extortionist not to follow through on their threat. In this way,
both usually have some incentive to avoid the threat being executed.
However, out of a desire to signal credibility in future interactions,
extortionists must follow through on threats occasionally. Consider
examples such as hostage ransoming or criminal syndicate protection
rackets. Successful future extortion requires a signal of commitment,
such as destroying the property of those who defy the
extortionists.<br />
AIs may carry out more frequent and more severe threats than humans tend
to. One reason for this is that they may have different value systems
which tolerate higher risks, reducing their motivation to acquiesce to
extortion. For example, an AI agent that sufficiently values the far
future may prefer to demotivate future extortionists from trying to
extort them. They may therefore defy a current extortion attempt,
tolerating even very large costs to them and others, for the long-term
benefit of credibly signaling that future extortion attempts will not
work either.<br />
More generally, with a greater variety of value systems, a greater
number of agents, and a greater action space size, miscalibrated
extortion attempts are more likely. Where the threat is insufficient to
force compliance, the aforementioned need to signal credibility
incentivizes the extortionist to execute their threat as punishment for
their target’s refusal to submit.</p>
<p><strong>AI agents extorting humans.</strong> AI agents might also
extort human targets. One example scenario would be an AI developing
both a weaponized biological pathogen, and an effective cure. If the
pathogen is slow-acting, the AI agent could then extort humans by
deploying the bioweapon, and leveraging the promise of its cure to force
those infected into complying with this demands. Pathogens that are
sufficiently fast to spread and difficult to detect could infect a very
large number of human targets, so this tactic could enable extremely
large-scale threats to be wielded effectively.</p>
<h3 id="concealment-of-information">Concealment of information</h3>
<p>Honesty is sometimes the best policy in an evolutionary game, but not
always. Evolutionary equilibria may arise in which individual organisms
use a mix of deceptive tactics to maximize their fitness. In other
words, an ESS can entail the concealment of information. This section
examines how evolutionary dynamics may pressure AI agents to evolve
deceptive behaviors in contexts where these are more evolutionarily
stable than honesty. Note that we examine other risks of deceptive AI in
the section of the chapter.</p>
<p><strong>Concealment of information can be an ESS.</strong> In an
evolutionary setting, a certain frequency or degree of dishonest
communication can be selected for <span class="citation"
data-cites="hamblin2009evolution"></span>. Non-venomous king snakes
closely resemble venomous coral snakes to ward off predators <span
class="citation" data-cites="greene1981coral"></span>. Cannibalistic
Portia spiders pluck the webs of other spiders to mimic the vibrations
of struggling prey, thus luring their victim out into the open to be
killed and eaten <span class="citation"
data-cites="jackson1998spider"></span>. Several flower species mimic the
appearance of certain female insects in order to better attract males
for pollination <span class="citation"
data-cites="schiestl2005success"></span>. Some bacteria produce specific
molecules that act as misdirection signals to other competing bacteria,
which reduce the probability of attack and detection, allowing the
bacteria to maintain its competitive advantage <span class="citation"
data-cites="mokkonen2015evolutionary"></span>. Evolutionarily stable
dishonesty is common in the natural world.</p>
<p><strong>Natural selection may favor AIs that conceal
information.</strong> Consider the following scenario: a game of Poker.
Agents will be better off if they have a very good ‘poker face’ because
other agents will not be able to deduce tell-tale signs associated with
bluffing or a good hand. The incentive to have a good “poker face” is
not malicious: it does not seek to exploit other agents and their
strategies, but rather, to maximize one’s own payoff in a competitive
game. We might extend this reasoning to AI. If multiple AIs operate
within a competitive landscape where available resources are limited,
they may be incentivized to conceal information from other AIs or humans
in order to preserve a competitive advantage. In other words, an AI
might develop a good ‘poker face’ since revealing certain kinds of
information might affect their abilities to maximize payoff. Thus,
strategies involving the concealment of information could be favored by
natural selection among AIs, giving rise to immoral behaviors that
emerge from amoral (morally neutral) grounds.</p>
<p><strong>AI agents may develop immoral behaviors from amoral
conditions.</strong> We’ve shown that dishonest communication can be an
ESS. The effects of inter-agent engagement can incentivize behavior we
view as immoral that nonetheless evolves from morally neutral
circumstances.</p>
<h3 id="summary-3">Summary</h3>
<p>In this section, we began by exploring how evolutionary forces change
the multi-agent dynamics of a simple game like the Iterated Prisoner’s
Dilemma. One example was the potential for cyclical dynamics in place of
a stable equilibrium state. We then moved to another effect of
evolutionary dynamics: the potential for a Nash equilibrium in which
agents behave very differently to one another, even when the agents
themselves are identical and have identical incentives. As an example,
we explored the “Hawk-Dove game,” finding that neither strategy is
dominant over the other. Through this, we uncovered the force underlying
this effect: “frequency-dependent selection.”</p>
<p>Using our understanding of how strategies may vary in terms of their
“evolutionary stability,” we explored why AI agents might develop
immoral behaviors even from amoral contexts. We examined why a set of
strategies known as <em>extortion</em> strategies are especially
successful in these evolutionary tournaments. Extortion not only works
among unsophisticated agents involved in sequential, symmetrical
competitions. We also see extortion in real-world scenarios between more
sophisticated entities like individual humans or groups. This suggests
that AI extortion could be a risk despite additional complications like
asymmetries in capabilities, knowledge, or disposition. Certain traits
likely in future AI agents, such as heightened sensitivity to scope,
strategic planning ability, and goal-preservation, could make AI
extortion especially effective and destructive. Together, this suggests
that AI agents may engage in extortion in order to pursue their goals.
This could involve optimized disvalue at extreme scales. We discussed
one such possibility: the torture of digital minds.<br />
</p>
<h1 id="sec:evo-pressures">Evolutionary Pressures</h1>
<h2 id="overview-1">Overview</h2>
<p>This central focus of this chapter concerns the dynamics we expect in
a multi-AI agent future. We must consider the risks that emerge from the
interactions between these agents, and between humans and AI
agents.<br />
In the previous section, we began our investigation into multiagent
dynamics using the mathematical framework of game theory. We found that
rational agents do not always secure collectively good outcomes. We
concluded with a brief exploration of evolutionary game theory. Using
the concepts of “evolutionarily stable strategies” and
“frequency-dependent selection,” we saw how multi-agent dynamics change
when agents can alter and adapt their behavior in response to what
others are doing.<br />
We found that identical agents under initially identical conditions can
evolve to select radically different strategies to one another.
Sometimes, there may be no stable equilibrium at all, and a population
of agents will instead undulate through different strategy compositions
perpetually. These more complicated multi-agent dynamics provide the
opportunity for behaviors we consider immoral, such as deception or
extortion, to arise from entirely amoral starting conditions.<br />
In this second part of the <em>Multi-Agent Dynamics</em> chapter, we use
evolutionary theory to build on these ideas and flesh out the risks
posed by the influence of natural selection on AI development. Our
ultimate conclusions are that AI development is likely to be subject to
evolutionary forces, and that we should expect the default outcome of
this influence to be the promotion of selfish and undesirable AI
behavior.</p>
<p><strong>First, we examine the idea of generalized Darwinism in AI
populations.</strong> We begin this section by looking at how evolution
by natural selection can operate in non-biological domains, an idea
known as “generalized Darwinism.” We formalize this idea using the
conditions set out by Lewinton as necessary and sufficient for natural
selection, and Price’s equation for describing evolutionary change over
time. Finally, we set out the case that evolutionary pressures are
influencing AIs. We turn to the ramifications of this claim in the
second section.</p>
<p><strong>Second, we explore why evolutionary pressures may promote
selfish AI behavior.</strong> We next consider what traits and
strategies natural selection tends to favor. We begin by setting out the
“information’s eye view” of evolution as a generalized Darwinian
extrapolation of the “gene’s eye view” of biological evolution. Using
this framing, we examine how conflict can arise within a system when the
interests of propagating information clash with those of the entity that
contains the information. Internal conflict of this kind could arise
within AI systems, distorting or subverting goals even when they are
specified and understood correctly. Finally, we explore why natural
selection tends to favor selfish strategies over altruistic ones. Our
upshot is that AI development is likely to be subject to evolutionary
pressures. These pressures may distort the goals we specify if the
interests of internal components of the AI system clash, and could also
generate a trend towards increasingly selfish AI behavior.</p>
<h2 id="generalized-darwinism">Generalized Darwinism </h2>
<p>Our aim in this section is to understand <em>generalized
Darwinism</em> –— the idea that Darwinian mechanisms are a useful way to
explain many phenomena outside of biology <span class="citation"
data-cites="dawkins1983universal"></span> –— and how we can use this as
a helpful model for modeling AI development. Using examples ranging from
science to music, we examine how evolution by natural selection can
operate in non-biological systems. We formalize this process using the
conditions for natural selection and consider how AI development meets
these criteria and is therefore subject to evolutionary pressures.</p>
<h3 id="conceptual-framework-for-generalized-darwinism">Conceptual
Framework for Generalized Darwinism</h3>
<p>Evolution by natural selection is not confined to the domain of
biological organisms. We can model many other phenomena using Darwinian
mechanisms. In this section, we use a range of examples to elaborate
this idea.</p>
<p><strong>Generalized Darwinism: natural selection can be applied to
non-biological phenomena.</strong> Evolution by natural selection does
not depend on mechanisms particular to biology <span class="citation"
data-cites="Smolin1992DidTU"></span>. Darwin proposed that populations
change over the course of generations when differences among individuals
help some reproduce more than others, so that eventually, the population
is made up of descendants of those that reproduced the most. Darwin
understood that this idea could explain many other phenomena. For
instance, he suggested that natural selection could explain the
evolution of language: “The survival or preservation of certain favored
words in the struggle for existence is natural selection” <span
class="citation" data-cites="Dennett1995DarwinsDI"></span>.<br />
As an example, Richard Dawkins has argued that human culture developed
according to the principles of natural selection <span class="citation"
data-cites="dawkins2006selfish"></span>. A piece of cultural
information, such as a song, is passed down over generations, often with
small changes, and some songs remain very well-known even over very long
time periods. The 18th century French tune “Ah! Vous dirai-je, Maman”
was pleasing and easy to sing, so the young Mozart wrote a version of
it, and it was later used as the tune for the English poem “The Star,”
which was sung over and over, until today, when many people know it as
“Twinkle Twinkle,” “The Alphabet Song,” or “Baa Baa Black Sheep” <span
class="citation" data-cites="blackmore1999meme"></span>. There were many
other 18th century songs that have long since been forgotten, but that
one has spread to many people over centuries, due to it being more
memorable and “catchy” than others of its time.</p>
<p><strong>By applying this Darwinian lens, we can describe many
non-biological phenomena.</strong> In nature, evolution happens when
individuals have a variety of traits, and individuals with some traits
propagate more than others. If a species of insect can be either red or
brown, but the brown ones blend in better and are less likely to be
eaten by birds, then more of the red insects will get eaten before
reproducing, while brown insects will tend to have more descendants.
Over time, the population will consist primarily of brown insects.<br />
We note a similar pattern in other, non-biological domains. For example,
alchemy was once a popular way of explaining the relationships among
different metals. People who believed in alchemy taught it to their
students, who taught it to their own students in turn, often with small
differences. Over time, some of those ideas continued to help them
explain the world, and others didn’t. In this respect, chemistry could
be viewed as a descendant of alchemy. The ideas that define it now were
propagated when they helped us increase our understanding of the natural
world, while others were discarded. In the same vein, after the first
widespread video conferencing services were developed, similar products
proliferated. Users chose the product that best met their needs,
selecting for services that were cheap, easy to use, and reliable. Each
company regularly released new versions of its product that were
slightly adapted from earlier ones, and competitors imitated and thereby
propagated the best features and implemented them into their own. Some
products incorporated the most adaptive features quickly, and the
descendants of those products are the ones we use today—–while others
were quickly outcompeted and fell into obscurity.</p>
<p><strong>Generalized Darwinism does not imply that evolution produces
good outcomes.</strong> Often, things that are the best at propagating
are not “good” in any meaningful sense. Invasive species arrive in a new
location, propagate quickly, and local ecosystems begin to crumble. The
forms of media that are most successful at propagating in our minds may
be harmful to our happiness and social relationships. For instance, news
articles that get more clicks are likely to have their click-attracting
traits reproduced in the next generation. Clicks thus select for more
sensational, emotionally charged headlines. In the context of AI,
generalized Darwinism poses significant risks. To see why, we first need
to understand how many phenomena tend to develop based on Darwinian
principles, so that we can think about how to predict and mitigate these
risks.<br />
</p>
<h2 id="formalizing-generalized-darwinism">Formalizing Generalized
Darwinism</h2>
<p>In this section, we formalize generalized Darwinism. First, we
overview the criteria necessary and sufficient for evolution by natural
selection to operate on a system. Second, we examine how we might
predict what happens to a system that meets these conditions. Together,
these help us to see why “survival of the fittest” is a poor description
of evolution by natural selection. Instead, this process would be better
described as “propagation of the better-propagated information.”</p>
<p><strong>Lewontin’s three conditions for evolution by natural
selection.</strong> The evolutionary biologist Richard Lewontin
formulated three criteria necessary and sufficient for evolution by
natural selection <span class="citation"
data-cites="lewontin1970units"></span>:</p>
<ol>
<li><p><strong>Variation</strong>: There is variation in traits among
individuals</p></li>
<li><p><strong>Retention</strong>: Future iterations of individuals tend
to resemble previous iterations</p></li>
<li><p><strong>Differential fitness</strong>: Different variants have
different propagation rates</p></li>
</ol>
<p>The validity of these criteria does not depend on biology. In living
organisms, DNA encodes the variations among individuals. Traits encoded
by DNA are heritable, and subject to selection. But this is not the only
way to fulfill the Lewontin conditions. Video conferencing software has
variation (there are many different options), retention (today’s video
conferencing software is similar to last year’s), and differential
fitness (some products are much more widely used and imitated than
others). Precisely how change occurs depends on the specific
phenomenon’s mechanism of propagation.</p>
<p><strong>The Price Equation describes how a trait changes in frequency
over time.</strong> In the 1970s, the population geneticist George R.
Price derived an equation that provides a mathematical description of
natural selection <span class="citation"
data-cites="Price1970SelectionAC"></span>. One formulation of Price’s
equation is given here:<br />
<span
class="math display"><em>Δ</em><em>z̄</em> = Cov(<em>ω</em>,<em>z</em>) + E<sub><em>w</em></sub>(<em>Δ</em><em>z</em>).</span></p>
<p>In this equation, <span class="math inline"><em>z̄</em></span> denotes
the average value of some trait <span
class="math inline"><em>z</em></span> in a population, and <span
class="math inline"><em>Δ</em><em>z̄</em></span> is the change in the
average value of <span class="math inline"><em>z</em></span> between the
parent generation and the offspring generation. If <span
class="math inline"><em>z</em></span> is height, and the parent
generation is 5 ’5 " on average and the next generation is 5’ 7” on
average, then <span class="math inline"><em>Δ</em><em>z̄</em></span> is 2
inches. <span class="math inline"><em>ω</em></span> is relative fitness:
how many offspring does an individual have relative to the average for
their generation? <span
class="math inline"><em>E</em><sub><em>w</em></sub>(<em>Δ</em><em>z</em>)</span>
is the expected value of <span
class="math inline"><em>Δ</em><em>z</em></span>: that is, the average
change in <span class="math inline"><em>z</em></span> between
generations, weighted by fitness, so that individuals who have more
offspring are counted more heavily.</p>
<p>Price’s Equation shows that the change in the average value of some
trait between parents and offspring is equal to the sum of a) the
covariance of the trait value and the fitness of the parents, and b) the
fitness-weighted average of the change in the trait between a parent and
its offspring. “Covariance” describes the phenomenon of one variable
varying together with another. To see whether a population will get
taller over time, for example, we would need to know the covariance of
fitness with height (do tall individuals have more surviving offspring?)
and the difference between a parent’s height and their average child’s
height.</p>
<p><strong>The Price Equation can be applied to non-biological
systems.</strong> The Price Equation does not require any understanding
of what causes a trait to be passed down to a subsequent generation or
why some individuals have more offspring than others, only of how much
the trait <em>is</em> passed on and how much it covaries with fitness.
The Price Equation would work just as well with car designs or tunes as
with birds or mollusks.</p>
<p><strong>The Price Equation allows us to predict what happens when
Lewontin conditions apply.</strong> The Price equation uses differences
between members of the parent generation with respect to some trait
<span class="math inline"><em>z</em></span> (variation), similarities
between parent and offspring generation with respect to <span
class="math inline"><em>z</em></span> (retention), and differential
fitness (selection). As a result, when we understand the degree to which
each of the Lewontin conditions apply, we can predict how much of some
trait will be present in subsequent generations <span class="citation"
data-cites="Okasha2007EvolutionAT"></span>.</p>
<p><strong>First misunderstanding: “fitness” does not describe physical
power.</strong> The idea of “fitness” often brings to mind a contest of
physical power, in which the strongest or fastest organism wins, but
this is a misunderstanding. Fitness in an evolutionary sense is not
something we gain at the gym. Being fit may not necessarily entail being
exceptionally good at any specific abilities. Sea sponges, for example,
are among the most ancient of animal lineages, and they are not quick,
clever, or good at chasing prey, especially when compared to, say, a
shark. But empirically, sea sponges have been surviving and reproducing
for hundreds of millions of years, much more than many species that
would easily beat them in head-to-head contests at almost any other
challenge.</p>
<p><strong>Second misunderstanding: “fitness” is not the mechanism
driving evolution.</strong> Biologists often talk about fitness when
discussing how well-suited an organism is to its environment. In
particular, they often treat fitness as a short-hand for <em>relative
reproductive success</em>: how much an individual contributes to the
next generation’s gene pool, relative to their competitors. However,
this usage of the word “fitness” seems to present evolution as being
fundamentally tautological. In the idiom “survival of the fittest,” we
appear to be using both “survival” and “fit” to mean <em>relative
reproductive</em> success. This would suggest that evolution is merely
the process in which those who reproduce more successfully, reproduce
more successfully! If true, the theory of evolution would seem to be
using its own conclusion to demonstrate its argument. As we shall see
next, however, this is actually false.</p>
<p><strong>“Fitness” is simply a metric we use to measure propagation
rate.</strong> In fact, evolutionary theory does <em>not</em> rely on
circular logic. This is because an organism’s fitness does not determine
its reproductive success; natural selection does. Instead, “fitness” is
simply the word we use to describe and measure propagation success.
Those who are better at propagating their information (by surviving and
reproducing) don’t have some “being fit” property which causes their
success. Rather, we deem how “fit” they are by measuring how successful
they’ve been at propagating their information. Thus the phrase “survival
of the fittest” should really be “propagation of the better-propagated
information.”<br />
The Price Equation, and natural selection more broadly, simply says that
if a trait helps individuals survive longer or reproduce more, and that
trait is passed on to the offspring, then more of the next generation
will have that trait. It does not tell us why a trait leads to an
individual having more offspring; it is only a way of expressing the
fact that some traits do correlate with having more offspring. The same
is true when natural selection is applied to non-biological systems;
“fitness” is simply a word for the quality of propagating more. The
information that propagates best is, of course, the information that
propagates best. But it need not be, and often is not, “better” in any
other sense. Fitness is a metric that describes how much information
propagates, not an assessment of value.</p>
<h2 id="generalized-darwinism-and-ai-populations">Generalized Darwinism
and AI Populations </h2>
<p>The three Lewontin conditions, of variation, retention, and
differential fitness, are all that is needed for evolution by natural
selection. This means we can assess how natural selection is likely to
affect AI populations by considering how the conditions apply to AIs.
Here, we claim that AIs are likely to meet all three conditions, so we
should expect natural selection forces to influence their traits and
development.</p>
<p><strong>Variation: AIs are designed and trained in a variety of
ways.</strong> As previously noted in "Natural Selection Favors AI over
Humans" <span class="citation"
data-cites="hendrycks2023natural"></span>,</p>
<div class="blockquote">
<p>"When thinking about advanced AI, some have envisioned a single AI
that is nearly omniscient and nearly omnipotent, escaping the lab and
suddenly controlling the world. This scenario tends to assume a rapid,
almost overnight, take-off with no prior proliferation of other AI
agents; we would go from AIs roughly similar to the ones we have now to
an AI that has capabilities we can hardly imagine so quickly that we
barely notice anything is changing. However, there could also be many
useful AIs, as is the case now. It is more reasonable to assume that AI
agents would progressively proliferate and become increasingly competent
at specific tasks, rather than assume one AI agent spontaneously goes
from incompetent to omnicompetent. This is similar to the subdivision of
biological niches. For example, lions and cheetahs developed completely
different and mutually exclusive strategies to catch prey through
strength or speed. Furthermore, if there are multiple AIs, they can work
in parallel rather than waiting for a single model to get around to a
task, making things move much faster." <span class="citation"
data-cites="hendrycks2023natural"></span></p>
</div>
<p>This means that people are likely to continue creating multiple AI
agents, even if there is a single best model. Financial gains would
encourage multiple competitors to challenge the top system <span
class="citation" data-cites="dietterich2000ensemble"></span>.<br />
In addition to the argument that AI populations have variation because
of the history of their development, there are also pragmatic arguments
for this claim. In evolutionary theory, Fisher’s fundamental theorem
states that the rate of adaptation is directly proportional to the
variation (all else equal). In rapidly-changing environments, where
quick adaptation increases a population’s probability of survival,
populations with more variation may persist longer. Consider how
variation in crops reduces the probability of catastrophic crop failure,
and variation in investments reduces the risk of unmanageable financial
losses. And in machine learning, an ensemble of AI systems will often
perform more accurately than a single AI <span class="citation"
data-cites="dietterich2000ensemble"></span>. Variation can help guide
decision making, in the same way that many people’s aggregated
predictions will usually be better than any one expert’s. Because of
these factors, we are more likely to see a powerful and resilient
population of AIs if they have significant variation.</p>
<p><strong>Variation in AI developers.</strong> As well as variation in
the AI systems themselves, we also see variation between the big
technology companies developing and adopting AI technologies. It may
seem simple to prevent the rise of selfish AI behaviors by avoiding
their selection. However, the reality is different. AI companies,
directed more by evolutionary pressures than by safety concerns, are
vying for survival in a fiercely competitive landscape. Consider how
OpenAI, which started as a nonprofit dedicated to benefiting humanity,
shifted to a capped-profit structure in 2019 due to funding needs.
Consequently, some of its safety-centric members branched out and
founded Anthropic, a company intending to prioritize AI safety. However,
even Anthropic couldn’t resist the call of commercialization, succumbing
to evolutionary pressures itself.<br />
Evolutionary pressures are driving safety-minded researchers to adopt
the behaviors of their less safety-minded competitors, because they are
anticipating that they can gain a significant fitness advantage in the
short-term by deprioritizing safety. Note that this evolutionary process
is not based on actual selection events (the researchers will not be
destroyed if they are outcompeted), but rather the researchers’
projections of what might happen if they adopt particular strategies. AI
safety <em>ideas</em> are being selected against, which is driving the
researchers to change their <em>behavior</em> (to behave in a less
safety-conscious manner). Importantly, as the number of competitors
rises, the variation in approaches and values also increases. This
increase in variation escalates the intensity of the evolutionary
pressures and the extent to which these pressures distort the behavior
of big AI companies.</p>
<p><strong>Retention: new AIs are developed under the influence of
earlier generations.</strong> Retention does not require exact copying;
it only requires that there be non-zero similarity among individuals in
subsequent generations. In the short term, AIs are developed by adapting
older models, or by imitating features from competitors’ models. Even
when training AIs from scratch, retention may still occur, as highly
effective architectures, datasets, and training environments are reused
thereby shaping the agent in a way similar to how humans (or other
biological species) are shaped by their environments. Even if AIs change
very rapidly compared to the timescales of biological evolution, they
will still meet the criterion of retention; their generations can be
extremely short, so they can move through many generations in a short
time, but each generation will still be similar to the one before it.
Retention is a very easy standard to meet, and even with many
uncertainties about what AIs may be like, it is very likely that they
meet this broad definition.</p>
<p><strong>Differential Fitness: some AIs are propagated more than
others.</strong> There are many traits which could cause some AI models
or traits to be propagated more than others (increasing their
“fitness”). Some of these traits could be highly undesirable to humans.
For example, <em>being safer</em> than alternatives may confer a fitness
advantage to an AI. However, <em>merely appearing to be safer</em> might
also improve an AI’s fitness. Similarly, <em>being good at automating
human jobs</em> could result in an AI being propagated more. On the
other hand, <em>being easy to deactivate</em> could reduce an AI’s
fitness. Therefore, an AI might increase its fitness by integrating
itself into critical infrastructure or encouraging humans to develop a
dependency for it, making us less keen to deactivate it. As long as some
AIs are at least marginally more attractive than others, AI populations
will meet the condition of differential fitness. There are many possible
points at which natural selection could take effect on AIs. These
include the actions of AI developers, in fine-tuning and customizing
models, or re-designing training processes.</p>
<p><strong>If the Lewontin conditions are satisfied, we must consider
how intense the evolutionary pressures are.</strong> More intense
selection pressure leads to faster change. In a population of birds in a
time with plenty of food, birds with any shape beak may survive.
However, if food becomes scarce, only those with the most efficient
beaks for accessing some specific food may survive, and the next
generation will disproportionately have that beak shape. More variation
also leads to faster adaptation, because variants that will be adaptive
in a new circumstance are more likely to already exist in the
population. The faster rounds of adaptation occur, the more quickly
distinct groups emerge with their own features.<br />
If there is more intense selection pressure on AIs, where only AIs with
certain traits propagate, then we should expect to see the population
optimize around those traits. If there is more variation in the AI
population, that optimization process will be faster. If the rate of
adaptation also accelerates, we would expect trends that lead to greater
differentiation in AI populations that are distinct from the changes in
the traits of individual AI models. In the following section, we will
discuss the evolutionary trends that tend to dominate when selection
pressure is intense and how they might shape AI populations.</p>
<h3 id="summary-4">Summary</h3>
<p>We started this section by exploring how evolution by natural
selection can occur in non-biological contexts. We then formalized this
idea of “generalized Darwinism” using Lewontin’s conditions and the
Price equation. We found that AI development may be subject to
evolutionary pressures by evaluating how it meets the Lewontin
conditions. In the next section, we turn to the ramifications of this
claim.</p>
<h2 id="subsec:selection-and-selfish">Levels of Selection and Selfish
Behavior</h2>
<p>Our aim in this section is to understand which AI characteristics are
favored by natural selection. We explore this by first outlining an
“information’s eye view” of evolution by natural selection. Here, we
find that internal conflict can arise where the interests of the
propagating information (such as a gene) clash with those of the larger
entity that contains it (such as an organism). This phenomenon could
arise in AI systems, distorting or subverting goals even when human
operators have specified them correctly.<br />
We then move to a second risk generated by natural selection operating
at the level of propagating information: Darwinian forces strongly favor
selfish traits over altruistic ones. Although on the level of an
individual organism, individuals may behave altruistically under
specific conditions (such as genetic relatedness), on the level of
information, evolution by natural selection tends to produce
selfishness. We conclude by outlining how a multi-agent AI future shaped
by natural selection will be dominated by selfish behavior.</p>
<h3 id="informations-eye-view">Information’s Eye View</h3>
<p>We often consider individual organisms to be the unit on which
natural selection is operating. However, it is their genes that are
being propagated through time and space, not the organisms themselves.
This section considers the “gene’s eye view” of evolution by natural
selection. We then use generalized Darwinism to build up an extrapolated
version of this perspective we can call the “information’s eye view” of
evolution.</p>
<p><strong>Species succeed when their information propagates, but
sometimes interests diverge.</strong> The information of living
organisms is primarily contained in DNA. Genes contain the instructions
for forming bodies. Most of the time, a gene propagates most
successfully when the organism that contains it propagates successfully.
But sometimes, the best thing for a gene is not the best thing for the
organism. For example, mitochondrial DNA is only passed on from females,
so it propagates most if the organism has only female offspring. In some
organisms, mitochondrial DNA gives rise to genetic mechanisms that
increase the production of female descendants. However, if too many
individuals have this mutation, the population will be
disproportionately female, and the organism will be unable to pass on
the rest of its genes. In this situation, the most effective propagation
mechanisms for the gene in the mitochondria is harmful to the
reproductive success of its host.</p>
<p><strong>The “gene’s eye view” of evolution.</strong> In <em>The
Selfish Gene</em>, Richard Dawkins argues that <em>gene</em> propagation
is a more useful framing than organism propagation <span
class="citation" data-cites="dawkins2006selfish"></span>. In Dawkins’
view, organisms are simply vehicles that allow genes to propagate.
Instead of thinking of birds with long beaks competing with birds with
short beaks, we can think about genes that create long beaks competing
with genes that create short beaks, in a fight for space within the bird
population. This gives us a framework for understanding examples like
the one above: the gene within the mitochondria is competing for space
in the population, and will sometimes take that space even at the
expense of the host’s individual fitness.</p>
<p><strong>Information functions similarly to genes, narrowing the space
of possibilities.</strong> We are humans and not dogs, roundworms, or
redwood trees almost entirely because of our genes. If we do not know
anything about what an organism is, aside from how long its genome is,
then for every base in the genome, there are four possibilities, so
there is an extremely large number of possible combinations. If we learn
that the first base is a G, you have divided the total number by four.
When we decode the entire genome, we have narrowed down an impossibly
large space of possibility to a single one: we can now know not only
that the organism is a cat, but even <em>which</em> cat
specifically.<br />
In non-biological systems, information works in a parallel way. There
are many possible ways to begin a sentence. Each word eliminates
possible endings and decreases the listener’s uncertainty, until they
know the full sentence at the end. Using the framework of information
theory, we can think of information as the resolution or reduction of
uncertainty (though this is not a formal definition). For an idea,
information is just the facts about it that make it different from other
ideas. A textbook’s main information is its text. A song’s information
consists of the pitches and rhythms that distinguish it from other
songs. These larger phenomena (ideas, books, songs) are distinguished by
the information they contain.</p>
<p><strong>Information that propagates occupies a larger volume of both
time and space.</strong> A single music score, written centuries ago and
buried underground ever since, has been propagated across hundreds of
years of time, but very little space. In contrast, a hit tune that is
suddenly everywhere and then quickly forgotten takes up a lot of space,
but very little time. But the best propagated information takes up a
large volume of both. The tune for “Twinkle Twinkle” has been taking up
space in many minds, pieces of paper, and digital formats for hundreds
of years and continues to propagate. The same is true for genetic
information. A gene that flourished briefly hundreds of millions of
years ago, and one that has had a consistent small presence, both take
up much less space-time volume than a gene that long ago became dominant
in many successful branches of the evolutionary tree <span
class="citation" data-cites="Smolin1992DidTU"></span>.</p>
<p><strong>Just as some genes propagate more, the same is true for bits
of information.</strong> In accordance with generalized Darwinism, we
can extend the gene’s eye view to an “information’s eye view.” A living
organism’s basic unit of information is a gene. Everything that evolves
as a consequence of Darwinian forces contains information, some of which
is inherited more than others. Dawkins coined the term “meme” as an
analogue for gene: a meme is the basic unit of cultural inheritance.
Like genes, memes tend to develop variations, and be copied and adapted
into new iterations. The philosopher of science, Karl Popper, wrote that
the growth of knowledge is “the natural selection of hypotheses: our
knowledge consists, at every moment, of those hypotheses which have
shown their (comparative) fitness by surviving so far in their struggle
for existence.” Social phenomena such as copycat crimes can also be
modeled as examples of memetic inheritance. Many types of crimes are
committed daily, some of which inspire imitators, whose subsequent
crimes can themselves be selected for and copied. Selection operates on
the level of individual pieces of information, as well as on the higher
level of organisms and phenomena.</p>
<p><strong>AIs may pass on information in ways analogous to our genetics
and cultural memetics.</strong> AIs are computer programs, made of code
that determines what they are like, in a similar way to how our DNA
determines what we are like. Different code makes the difference between
an agentic AI and Flappy Bird. Their code, or pieces from it, can be
directly copied and adapted for new models. But their information can
also be memetically transmitted, as our cultural memes can. Even today,
AIs are often designed based on hearing about and imitating successful
models, not only on copying code from them. AIs also help create
training data for new AIs and evaluate their learning, which makes the
new AIs tend to have traits similar to earlier models. As AIs continue
to become more autonomous, they may be able to imitate and learn from
one another, self-modifying to adopt traits and behaviors that seem
useful. The AI information that propagates the most will take up more
and more space-time volume, as it is copied into more AIs that multiply
and endure over longer periods.</p>
<h2 id="subsec:intrasys-goal">Intrasystem Goal Conflict</h2>
<p>The interests of an organism and its genetic information are usually
aligned well. However, they can sometimes diverge from one another. In
this section, we identify analogous, non-biological phenomena, where
conflict arises between a system and the sub-systems in which it stores
its information. Evolutionary pressures might generate internal conflict
within AI systems. This could distort or subvert goals set for AIs by
human operators, even when the goal was specified and understood
correctly.</p>
<p><strong>Conflict within genomes.</strong> As we have seen, selection
on the level of genes does not always result in the best outcomes for
the organism. As briefly mentioned above, human mitochondrial DNA is
only transferred to offspring through biological females. A human’s
mitochondrial genome is identical to their biological mother’s, assuming
no change due to mutation. Thus, mitochondrial genes which benefit only
females may be selected for, even where they incur a cost upon males, as
males represent a heritability deadend. These and other “selfish”
genetic elements give rise to intragenomic conflict.<br />
Another example concerns plants in the genus <em>Plantago</em>. These
plants have organelles which are passed on only via females. Some genes
in these organelle genomes have evolved to promote male sterility and
thus favor female offspring, against the evolutionary interests of the
organism itself. Intragenomic conflict can be exacerbated by other
biological agents <span class="citation"
data-cites="leveson2016engineering"></span>. In crustaceans, genetic
conflict of this kind can be exacerbated by a bacterium known as
<em>Wolbachia</em>, which can only be passed on to future generations by
eggs, not sperm. <em>Wolbachia</em> therefore often kills males or
drives them to develop into females <span class="citation"
data-cites="yen1971hypothesis"></span>.</p>
<p><strong>Conflict within organisms.</strong> We observe other kinds of
internal conflict within organisms which do not concern their genomes.
For example, the bacterial species that compose the human gut microbiome
can exist in a mutually-beneficial symbiosis with their host. However,
some bacteria reveal themselves to be “opportunistically pathogenic”: in
the wake of disruptions like the use of antibiotics, many of these
once-mutualists will propagate at accelerated rates, often at the
expense of the host’s health. As the philosopher of evolutionary biology
Samir Okasha notes, “intraorganismic conflict is relatively common among
modern organisms” <span class="citation"
data-cites="Okasha2018AgentsAG"></span>.</p>
<p><strong>Intrasystem goal conflict: between information and the larger
entity that contains it.</strong> Just as in the case of intraorganismic
conflict, the interests of propagating information and the entities that
contain the information can diverge from one another. The basic melody
of “Twinkle Twinkle” has been vastly more successful at propagating than
the complete French song “Ah vous dirai-je, Maman,” because the tune
itself has become a meme, propagated into many languages with different
lyrics. Morphing into “Baa Baa Black Sheep,” is bad for the propagation
of that song, but excellent for the propagation of its tune, which has
changed very little. Intrapsychic conflict is similarly common. For
instance, many people want to improve their diets, but crave unhealthy
foods.<br />
Thus, a great many complex systems may have internal conflict, where
their functioning is distorted by the behavior of their component
subsystems. We call the more general phenomenon that can describe all of
these examples <em>intrasystem goal conflict</em>: the clash of
interests of different agents within a shared system. Intrasystem goal
conflict can arise within complex systems in a range of domains,
including social institutions, human psyches, organisms and genomes.</p>
<p><strong>Intrasystem goal conflict may cause an AI system not to
behave as a unified agent.</strong> One reason why we might expect an AI
system not to pursue a specified goal is because intrasystem goal
conflict has eroded its “unity of purpose.” A system has achieved unity
of purpose if there is alignment at all levels of internal organization
<span class="citation" data-cites="Okasha2018AgentsAG"></span>.
Undermining a system’s <em>unity of purpose</em> reduces its ability to
carry out its system-level goals. A helpful analogy here is to consider
political “coups.” A coup is characterized by a struggle for control
within a political system, in which agents within the system act to
seize power, often eroding the system’s unity of purpose by disrupting
its stability and functionality. When political leaders are overthrown
by their second-in-command, the goals of the political system usually
change. Similarly, if we give an AI agent a goal to pursue, the agent
may in turn assign parts of this goal to sub-agents, who may take over
and subvert the original goal with their own.</p>
<p><strong>Intrasystem goal conflict can arise through
modularization.</strong> There are two mechanisms by which intrasystem
goal conflict may arise. The first is <em>modularization</em>: a goal
may be broken down into components, each of which thus becomes a
sub-goal. For example, consider a multi-agent system, such as a
committee, which wishes to carry out a complicated big-picture goal. In
this case, they may break their goal up into subgoals, and delegate each
of these to “working groups” (different subsets of the committee). Each
working group now pursues solely its delegated sub-goal and the
different working groups compete with one another. Ultimately, this
modularity enables the different working groups to grow and change
independently of one another, and results in an uneven balance between
the different sub-goals. Thus, the big-picture goal is distorted.<br />
It is worth noting that some cases may differ from this story in terms
of their causality. Rather than a single complex goal being modularized,
sometimes individual agents may come together to form a coalition to
pursue a larger-scale goal. Misalignments of interests between these
agents can give rise to intrasystem goal conflict in these cases just as
with modularized systemic goals. Examples include gut bacterial
opportunistic pathogenicity, and some inter-sexual and intragenomic
conflict. For instance, intersexual conflict in animals has produced
species in which the males deliberately cause the females physical
damage when mating. This strategic harming hinders the female’s ability
to re-mate in future, and incentivizes her to allocate more resources
towards raising that particular male’s genetic offspring. Females have
in turn evolved defensive strategies against this, such as armor plating
and spines <span class="citation"
data-cites="morrow2003traumatic"></span>. Intersexual coevolutionary
“arms races” such as these have been documented in a diverse range of
species, from ducks to fruit flies <span class="citation"
data-cites="rowe2002sexually"></span>. Rather than a unified goal being
modularized, the goal conflict here has arisen in the context of a
coalition. The female and male are collaborating through sexual
reproduction, but even slight misalignments of their interests can
result in dramatic goal conflict.</p>
<p><strong>Intrasystem goal conflict can arise through
delegation.</strong> The second mechanism by which intrasystem goal
conflict may arise is in the <em>delegation</em> itself. In the above
example, the working groups each pursue their assigned sub-goal
faithfully. The problem is that the collective outcome of these
agencies, each optimizing for their assigned subgoal is a distorted
version of the original big-picture goal. However, the delegation of a
goal to another agent can also itself generate intrasystem goal
conflict.<br />
In economics, this is called the <strong>principal-agent
problem</strong>. Put simply, this is the problem of conflicting
priorities that arises when one party (the <em>principal</em>) delegates
a goal of theirs to another (the <em>agent</em>), who is not entirely
value-aligned with the <em>principal</em>. The <em>agent</em> has other
interests, such as self-preservation or gain of influence, which compete
with the goal the <em>principal</em> has delegated to them. This can
result in the distortion or subversion of the delegated goal: the agent
pursues their own goals as well as (or instead of) that of the
principal. Principal-agent problems are most common where there is a
knowledge asymmetry, such that the agent knows more about the relevant
matter than the principal. For example, consider a client who has hired
a mechanic to fix their car. The client wishes their car to be fixed as
cost-effectively as possible; the mechanic wishes to maximize their
profits. The principal-agent problem is thus that the client cannot
trust the mechanic to report the cost of the car fix accurately.<br />
Principal-agent problems can generate intrasystem goal conflict wherever
the components of a modularized big-picture goal are delegated to
sub-agents. In particular, the sub-agents are likely to be motivated by
self-preservation. If the goal the principal has delegated to them
clashes with their own goal of self-preservation, this can produce
strong intrasystem goal conflict. To build on the committee example
above, now consider that each working group is incentivized to compete
with the others to avoid being disbanded. Each exaggerates the
importance of their own work, or reconfigures their assigned objective
in order to achieve goals of their own. Ultimately, some even break away
from the committee, to pursue their emergent goals without the
constraints imposed by the committee from which they formed.<br />
Intrasystem goal conflict often arises when a system both modularizes
and delegates a complex goal. For example, the US Food and Drug
Administration (FDA) and the Department of Agriculture (USDA) are
sub-agencies of the same government <span class="citation"
data-cites="moss2010warning"></span>. The US government delegates
sub-goals to each agency to carry out, yet sometimes their interests and
actions conflict with one another. In 2010, the USDA implemented a
marketing campaign to promote eating more cheese, to support the dairy
industry. Simultaneously, the FDA implemented a marketing campaign of
their own to promote eating less cheese, as part of an effort to reduce
the saturated fat content of American diets. Despite the fact that both
agencies were delegated sub-goals from the same government, their
conflicting objectives resulted in their taking actions that hindered
one another.</p>
<p><strong>Intrasystem goal conflict in AI systems.</strong> In the
future, humans and AI agents may interact in many different ways,
including by working together on collaborative projects. This provides
the opportunity for goal distortion or subordination through intrasystem
goal conflict. For instance, when humans enlist AI agents to collaborate
on tasks, principal-agent problems may arise. Just as how human
collaborators may betray or overturn their principals, AI agents may
behave similarly. If an AI collaborator has a goal of self-preservation,
they may try to remove any leverage or power others have over them. In
this way, the system (of principal and agent) that ends up executing
actions based on these conflicting goals will not necessarily be
equivalent to how a system with unity of purpose would pursue the goal
set by the humans. The behavior of this emergent multi-agent system may
thus distort our goals, or even subvert them altogether.</p>
<h2 id="selfishness">Selfishness</h2>
<p>In the previous section, we examined one risk generated by natural
selection favoring the propagation of information: conflict between the
information (such as genes, departments, or sub-agents) and the larger
entity that contains it (such as an organism, government, or AI system).
In this section, we consider a second risk: that natural selection tends
to favor selfish traits and strategies over altruistic ones. We conclude
that the greater the influence of evolutionary pressures on AI
development, the more we should expect a multi-AI agent future to be one
dominated by selfish behavior.</p>
<p><strong>Selfishness: furthering one’s own information propagation at
the expense of others.</strong> In evolutionary theory, “selfishness”
does not imply intent to harm another, or belief that one’s own
interests ought to dominate. Organisms that do not have malicious
intentions often display selfish traits. The lancet liver fluke, for
example, is a small parasite that infects sheep by first infecting ants,
hijacking their brains and making them climb to the top of stalks of
grass, where they get eaten by sheep <span class="citation"
data-cites="martin20183d"></span>. The lancet liver fluke does not wish
ants ill, nor does it have a belief that lancet liver flukes should
thrive while ants should get eaten. It simply has evolved a behavior
that enables it to propagate its own information at the expense of the
ant’s.</p>
<p><strong>Selfishness in AI.</strong> AI systems may exhibit “selfish”
behaviors, expanding the AIs’ influence at the expense of human values.
Note that these AIs may not even understand what a human is and yet
still behave selfishly towards them. For example, AIs may automate human
tasks, necessitating extensive layoffs <span class="citation"
data-cites="hendrycks2023overview"></span>. This could be very
detrimental to humans, by generating rapid or widespread unemployment.
However, it could take place without any malicious intent on the part of
AIs merely behaving in accordance with their pursuit of efficiency. AIs
may also develop newer AIs that are more advanced but less
interpretable, reducing human oversight. Additionally, some AIs may
leverage emotional connections by imitating sentience or emulating the
loved ones of human users. This might generate social resistance to
their deactivation. For instance, AIs that plead not to be deactivated
might stimulate an emotional attachment in some humans. If afforded
legal rights, these AIs might adapt and evolve outside human control,
becoming deeply embedded in society and expanding their influence in
ways that could be irreversible.</p>
<p><strong>Selfish traits are not the opposite of cooperation.</strong>
Many organisms display cooperative behavior at the individual level.
Chimpanzees, for example, regularly groom other members of their group.
They don’t do this to be “nice,” but rather because this behavior is
reciprocated in future, so they are likely to eventually benefit from it
themselves <span class="citation"
data-cites="schino2007grooming"></span>. Cells found in filamentous
bacteria, so named because they form chains, regularly kill themselves
to provide much needed nitrogen for the communal thread of bacterial
life, with every tenth cell or so “committing suicide” <span
class="citation" data-cites="ratzke2017ecological"></span>. But even in
these examples, cooperative behavior ultimately helps the individual’s
information propagate. Chimpanzees who groom others expect to have the
favor returned in future. Filamentous bacteria live in colonies made up
of their clones, so one bacterium sacrificing itself to save copies of
itself still propagates its information. We cover this in more detail in
the next section: <em>Cooperation and Conflict</em>.</p>
<p><strong>Natural selection tends to produce selfish traits.</strong>
Organisms that further their own information propagation will typically
propagate more. A lancet liver fluke that developed the ability to give
ants free choice and allow them not to climb stalks of grass if they
don’t want to would be less likely than the current version to succeed
at getting eaten by sheep and continuing its life cycle. Most biological
selfishness is less dramatic, but nonetheless, the organisms alive today
are necessarily the descendants of those that succeeded at propagating
their own information, and not of those that traded propagation for
other qualities.</p>
<p><strong>Altruism that reduces an individual’s fitness is not an
evolutionarily stable strategy.</strong> Imagine a very altruistic
fictional population of foxes who freely share food with one another,
even at great cost to themselves. When food is abundant, they all
thrive, and when food is scarce, they suffer together. If, during a time
of scarcity, one fox decides to steal food from the communal stores and
take it for herself and her offspring, they may survive while others
starve. As a result, her offspring, who may have inherited her selfish
trait, will make up a higher proportion of the next generation. As this
repeats, the population will be dominated by individuals who take food
for themselves when they can. The population of altruists may get along
quite well on its own, but altruism is unstable, because anyone who
decides to exploit it will do better than the group. Since altruism that
reduces an individual’s overall fitness is not an evolutionarily stable
strategy, we should expect to see selfish behavior being promoted.</p>
<p><strong>The more natural selection acts on a population, the more
selfish behavior we expect.</strong> In the example in the preceding
paragraph, when food is abundant, there is little advantage to
selfishness and there may even be penalties, as the group punishes
selfish behavior. There is plenty of food to go around, so the
descendants of foxes who steal food will not be much more likely to
survive, and the next generation can contain plenty of altruists. But in
times when only a few can propagate, selfishness will confer a greater
advantage, and the population will tend to become selfish more
quickly.</p>
<p><strong>Avoiding extreme AI selfishness: changing the
environment.</strong> AI agents’ fitness could either be influenced more
by natural selection or by the environment. We have sketched out the
default outcome of the former: a landscape of powerful and selfish AI
agents. One way we might prevent this trend towards increasingly selfish
behavior is to ensure that it is the <em>environment</em> which ends up
shaping the fitness of AI agents substantially more than natural
selection. Currently, we are in an environment of extreme competition,
and so AI agents that are better-suited to this competitive environment
will propagate more, and increase the proportion of the population with
their traits (including selfish traits). However, if we altered the
environment such that the actions of AI researchers and AI agents were
not so heavily steered by competitive pressures, we could reduce this
problem.</p>
<p><strong>Avoiding extreme AI selfishness: changing the
selection.</strong> Another possibility is to change what makes AI
agents “fit.” We could establish an ecosystem in which AI agents can be
developed, deployed, and adopted more safely, without the influence of
such extreme competitive pressures. In this ecosystem, we could select
against AIs with the most harmful selfish behaviors, and select for AIs
that faithfully assist humans. As these AIs proliferate through this
ecosystem, they could then counteract the worst excesses of selfish
behavior from other agents.</p>
<h3 id="summary-5">Summary</h3>
<p>In this section, we considered the effects of evolutionary pressures
on AI populations. We started by using the idea of generalized Darwinism
to expand the “gene’s eye view” of biological evolution to an
“information’s eye view.” Using this view, we identified two AI risks
generated by natural selection: intrasystem goal conflict and selfish
behavior. Intrasystem goal conflict could distort or subvert the goals
we set an AI system to pursue. Selfish behavior would likely be favored
by natural selection wherever it promotes the propagation of
information: If AI development is subject to strong Darwinian forces, we
should expect AIs to tend towards selfish behaviors.<br />
</p>
<h1 id="conflict-and-cooperation-1">Conflict and Cooperation</h1>
<h2 id="overview-2">Overview</h2>
<p>In this chapter, we have been exploring the risks that arise from
interactions between multiple agents. So far, we have used game theory
and evolutionary theory to understand how collective behavior can
produce undesirable outcomes. In simple terms, securing morally good
outcomes without cooperation can be extremely difficult, even for
intelligent rational agents. Consequently, the potential for conflict
and the importance of cooperation has emerged as a strong theme in this
chapter. In this third section of this chapter, we examine conflict and
cooperation in more detail.</p>
<h3 id="conflict-overview">Conflict overview</h3>
<p>We begin this section by exploring the drivers of conflict. Here, we
use the term “conflict” loosely, to describe the decision to defect
rather than cooperate in a competitive situation. This may lead to
violence in some cases, but not necessarily all. Our goal is to uncover
how, despite being costly, conflict can sometimes be a rational choice
nevertheless.<br />
Microorganisms, humans, states, and nations all cooperate and get into
conflict. In nature, we can observe cooperation in the form of social
insect behavior, pack hunting, symbiotic relationships, and much more.
In humans, we encounter cooperation in several areas including
coordinated disaster responses, international peace negotiations, and
community service, among many others. By contrast, conflict in both
human and non-human organisms can occur as a consequence of resource
competition, territorial disputes, mating access, the maintenance of
social dominance hierarchies, including several other factors.
Importantly, the mechanisms and factors that motivate cooperation and
conflict are prevalent in various environments. Thus, we have good
reason to suppose that AI agents will be similarly influenced by these
various mechanisms and factors as they decide to cooperate or conflict
with humans and other AIs.<br />
We will begin our discussion of conflict with concepts in bargaining
theory. We then examine some specific "conflict factors": features of
competitive situations that make it harder to reach negotiated
agreements or avoid confrontation. We first explore five conflict
factors from bargaining theory. These can be divided into the following
two groups: <strong>Commitment problems.</strong> According to
bargaining theory, one reason bargains may fail is that some of the
agents making an agreement may have the ability and incentive to break
it. We explore three examples of commitment problems.</p>
<ul>
<li><p><em>Power shifts</em>: when there are imbalances between agents’
capabilities such that one agent becomes stronger than the other,
conflict is more likely to emerge between them.</p></li>
<li><p><em>First-strike advantages</em>: when one agent possesses the
element of surprise, the ability to choose where conflict takes place,
or the ability to quickly defeat their opponent, the probability of
conflict increases.</p></li>
<li><p><em>Issue indivisibility</em>: agents cannot always divide a good
however they please – some goods are “all or nothing” and this increases
the probability of conflict between agents.</p></li>
</ul>
<p><strong>Information problems.</strong> According to bargaining
theory, the other principal cause of a bargaining failure is that some
of the agents may lack good information. Uncertainty regarding a rival’s
capabilities and intentions can increase the probability of conflict. We
explore two information problems.</p>
<ul>
<li><p><em>Misinformation</em>: in the real world, agents frequently
have incorrect information, which can cause them to miscalculate
suitable bargaining ranges.</p></li>
<li><p><em>Disinformation</em>: agents may sometimes have incentives to
misrepresent the truth intentionally. Even the expectation of
disinformation can make it more difficult to reach a negotiated
settlement.</p></li>
</ul>
<p><strong>Conflict factors outside of bargaining theory.</strong>
Bargaining frameworks do not encompass all possible reasons why agents
may decide to conflict with one another. We end by exploring one such
conflict factor:</p>
<ul>
<li><p><em>Inequality</em>: under conditions of inequality, agents may
fight for access to a larger share of available resources or a desired
social standing.</p></li>
</ul>
<h3 id="cooperation-overview">Cooperation overview</h3>
<p>Next, we turn to cooperation. We observe many forms of cooperation in
biological systems: social insect colonies, pack hunting, symbiotic
relationships, and much more. Humans perform community services,
negotiate international peace agreements, and coordinate aid for
disaster responses. Our very societies are built around
cooperation.<br />
Cooperation between AI stakeholders may be vital for counteracting the
competitive and evolutionary pressures of AI races we have explored in
this chapter. For example, the “merge-and-assist” clause of OpenAI’s
charter outlines their commitment to cease competition with—-and provide
assistance to—-any “value-aligned, safety-conscious” AI developer who
appears close to producing AGI, in order to reduce the risk of eroding
safety precautions. Cooperation between AI agents is also necessary for
reducing some of the multi-agent risks we have looked at: we want AIs to
cooperate, rather than defect, in Prisoner’s Dilemma scenarios.<br />
However, ensuring that AIs behave cooperatively may not be a total
solution to the collective action problems we have examined in this
chapter. By more closely examining how cooperative relationships can
come about, it is possible to see how they may backfire with disastrous
consequences for AI safety. Instead, we need a more nuanced view of the
potential benefits and risks of promoting cooperation between AI
systems. To do this, we study seven different mechanisms by which
cooperation may arise in multi-agent systems, considering the
ramifications of each for cooperation between and within human agencies
and AI agents [17]:</p>
<ul>
<li><p><em>Direct reciprocity</em>: when individuals are likely to
encounter others in the future, they are more likely to cooperate with
them.</p></li>
<li><p><em>Indirect reciprocity</em>: when it benefits an individual’s
reputation to cooperate with others, they are more likely to do
so.</p></li>
<li><p><em>Group selection</em>: when there is competition between
groups, cooperative groups may outcompete non-cooperative
groups.</p></li>
<li><p><em>Kin selection</em>: when an individual is closely related to
others, they are more likely to cooperate with them.</p></li>
<li><p><em>Individual stakes to common stakes</em>: when individual
interests become aligned with the collective good of the group,
individuals are more likely to behave in ways that benefit the
whole.</p></li>
<li><p><em>Simon’s selection mechanism</em>: when available information
is limited, individuals may be impelled to rely on social channels that
require cooperation.</p></li>
<li><p><em>Institutional mechanisms</em>: when there are externally
imposed incentives (such as laws) that subsidize cooperation and punish
defection, individuals and groups are more likely to cooperate.</p></li>
</ul>
<h2 id="conflict">Conflict</h2>
<h3 id="introduction-to-conflict">Introduction to Conflict</h3>
<p>Conflict is common in nature. Organisms engage in conflict to
maintain social dominance hierarchies, to hunt, and to defend territory.
People also engage in conflict. Throughout human history, wars are
common, often occurring as a consequence of power-seeking behavior,
which inspired conflict over attempts at aggressive territorial
expansion or resource acquisition.<br />
We begin this section by discussing bargaining theory, which lays the
groundwork for understanding why it may be rational for agents to engage
in conflict. Next, we turn to the specific factors that may motivate
agents to engage in conflict with one another, even when compromise
might be the better option. We explore why AIs may be similarly affected
by these factors, such that they may view conflict as an instrumentally
rational choice in certain contexts.</p>
<p><strong>Conflict can be rational.</strong> Though humans know
conflict can be enormously costly, we often still pursue or instigate
it, even when compromise might be the better option.<br />
Consider the following example: a customer trips in a store and sues the
owner for negligence. There is a 60% probability the lawsuit is
successful. If they win, the owner has to pay them $40,000, and going to
court will cost each of them $10,000 in legal fees. There are three
options: (1) they or the owner concede, (2) they both let the matter go
to court, (3) they both reach an out-of-court settlement.<br />
</p>
<ol>
<li><p>If the owner concedes, the owner loses $40,000, and if the
customer concedes, they gain nothing.</p></li>
<li><p>If both go to court, the owner’s expected payoff is the product
of the payment to the customer and the probability that the lawsuit is
successful minus legal fees. In this case, the owner’s expected payoff
would be <span class="math inline">(−40,000×0.6) − 10, 000</span> while
the customer’s expected payoff would be <span
class="math inline">(40,000×.6) − 10, 000</span>. As a result, the owner
loses $34,000 dollars and the customer gains $14,000 dollars.</p></li>
<li><p>An out-of-court settlement x where <span
class="math inline">14, 000 &lt; <em>x</em> &lt; 34, 000</span> would
enable the customer to get a higher payoff and the owner to pay lower
costs. Therefore, a mutual settlement is the best option for both if
<span class="math inline"><em>x</em></span> is in this range.</p></li>
</ol>
<p>Hence, if the proposed out-of-court settlement would be greater than
$34,000, it would make sense for the owner to opt for conflict rather
than bargaining. Similarly, if the proposed settlement were less than
$14,000, it would be rational for the customer to opt for conflict.</p>
<h3 id="bargaining-theory">Bargaining Theory</h3>
<p>Here, we begin with a general overview of bargaining theory, to
illustrate how pressures to outcompete rivals or preserve power and
resources may make conflict an instrumentally rational choice. Next, we
turn to the unitary actor assumption, highlighting that when agents view
their rivals as unitary actors, they assume that they will act more
coherently, taking whatever steps necessary to maximize their welfare.
Following this, we discuss the notion of commitment problems, which
occur when agents cannot reliably commit to an agreement or have
incentives to break it. Commitment problems increase the probability of
conflict, and are motivated by specific factors, such as power shifts,
first strike advantages, and issue indivisibility–—factors we discuss in
the section. When any one of these factors is present, it may shrink or
destroy the bargaining range, which represents the set of possible
outcomes that both agents involved in a conflict find acceptable through
negotiation. We conclude this section by highlighting that AIs may also
find it instrumentally rational to pursue conflict, for the reasons we
have just explored. Other factors, such as information problems and
incentives to misrepresent, as well as inequality can also increase the
probability of conflict, which we will discuss in the <em>Conflict
Factors</em> section.</p>
<p><strong>Bargaining theory.</strong> Conflict is fundamentally costly
for winners and losers alike. However, it may sometimes be rational for
agents to engage in conflict. <em>Bargaining theory</em> describes why
different agents may engage in conflict. Rational agents, due to
pressures to outcompete rivals or preserve their power and resources,
will sometimes prefer conflict, especially when they cannot reliably
predict the outcomes of conflict scenarios. When rational agents assume
that potential rivals have the same mindset, the probability of conflict
increases.</p>
<p><strong>The unitary actor assumption.</strong> We tend to assume that
a group is a single entity, and that its leader is only interested in
maximizing the overall welfare of the entity. We call this the
<em>unitary actor assumption</em>, which is another name for the “unity
of purpose” assumption discussed previously in this chapter. A nation in
disarray without coherent leadership is not necessarily a unitary actor.
When we view groups and individuals as unitary actors, we can assume
they will act more coherently, so they can be more easily modeled as
taking steps necessary to maximize their welfare. When parties make this
assumption, they may be less likely to cooperate with others since what
is good for one party’s welfare may not necessarily be good for
another’s.</p>
<p><strong>The bargaining range.</strong> Whether or not agents are
likely to cooperate will be influenced by whether their bargaining
ranges overlap. The bargaining range represents the set of possible
outcomes that both agents involved in a conflict find acceptable through
negotiation. Recall the lawsuit example: a bargaining settlement “<span
class="math inline"><em>x</em></span>” is only acceptable if it falls
between $14,000 and $34,000. Any settlement “<span
class="math inline"><em>x</em></span>” below $14,000 will be rejected by
the customer while any settlement “<span
class="math inline"><em>x</em></span>” above $34,000 will be rejected by
the store owner. Thus, the bargaining range is often depicted as a
spectrum with the lowest acceptable outcome for one party at one end and
the highest acceptable outcome for the other party at the opposite end.
Within this range, there is room for negotiation and potential
agreements.</p>
<figure id="fig:overview-barg">
<img src="images/multiagent-dynamics/image18.png" />
<figcaption>Overview of Bargaining ranges</figcaption>
</figure>
<p><strong>Conflict in AIs.</strong> Let us assume that AI agents will
act rationally in the pursuit of their goals (so, at the least, we model
them as unitary actors or as having unity of purpose). In the process of
pursuing and fulfilling their goals, AI agents may encounter potential
conflict scenarios, just as humans do. In certain scenarios, AIs may be
motivated to pursue conflict over a peaceful resolution, for the reasons
we have just explored. In doing so, they may be subject to the same
commitment problems and conflict factors that humans encounter, and use
them to motivate their decisions to instigate conflict. If it were
instrumentally rational for AIs to pursue conflict, they may be
influenced by the commitment problems and factors we discuss in the
following section.</p>
<h3 id="sec:commitment problems">Commitment problems</h3>
<p>Many conflicts occur over resources, which are key to an agent’s
power. Consider a bargaining failure in which two agents bargain over
resources in an effort to avoid war. If agents were to acquire these
resources, they could invest them into military power. As a result,
neither side will be credible if they commit that they will only use
them for peaceful purposes. This is one instance of a <em>commitment
problem</em> <span class="citation"
data-cites="fearon1995rationalist"></span>, which is when agents cannot
reliably commit to an agreement, or when they may even have incentives
to break an agreement. Commitment problems are usually motivated by
specific factors, such as power shifts, first-strike advantages, and
issue indivisibility, which may make conflict a rational choice.
Ultimately, when such factors are present, it becomes more difficult for
agents to establish a common ground over which to bargain, thereby
increasing the probability of conflict. It is important to note that our
discussion of these commitment problems assumes anarchy: we take for
granted that contracts are not enforceable in the absence of a higher
governing authority. <strong>Power shifts overview.</strong> When there
are imbalances between parties’ capabilities such that one party becomes
stronger than the other, <em>power shifts</em> can occur. Such
imbalances can arise as a consequence of several factors including
technological and economic advancements, increases in military
capabilities, as well as changes in governance, political ideology, and
demographics. In the context of AIs, power could shift if AIs become
more intelligent or have a change in resources. Individual parties may
initially be able to avoid conflict by arriving at a peaceful and
mutually beneficial settlement with their rivals. However, if they or
their rival’s power increases after this settlement has been made, the
stronger party may end up benefiting from it more than the weaker party.
Thus, we encounter the following commitment problem: a negotiated
settlement might work today but it remains uncertain whether it will
work in the future.</p>
<p><strong>Example: The US vs China.</strong>n China has been investing
heavily in its military. This has included the acquisition of new
technologies, such as nuclear and supersonic missiles, as well as
drones. The future is uncertain, but if this trend continues, it could
pose a commitment problem. If China were to gain a military advantage
over the US, this would shift the balance of power. This possibility
undermines the stability of bargains struck today between the US and
China, because China’s expected outcome from conflict may increase in
the future if they become more powerful. The US may expect that
agreements made with China about cooperating on AI regulation could lose
enforceability later if there is a significant power shift.</p>
<p>This situation can be modeled using the concept of “Thucydides’
Trap.” The ancient Greek historian Thucydides suggested that the
contemporary conflict between Sparta and Athens might have been the
result of Athens’ increasing military strength, and Sparta’s fear of the
looming power shift. Though this analysis of the Peloponnesian War is
now much-contested, this concept can nevertheless serve to understand
how a rising power threatening the position of an existing superpower in
the global order can increase the potential for conflict rather than
peaceful bargaining.</p>
<p><strong>Effect on the bargaining range.</strong> Consider two agents,
A and B. A is always weaker than B, but relative to the time period, A
is weaker in the future than it is in the present. A will always have a
lower bargaining range, so B will be unlikely to accept any settlements,
especially as B’s power increases. It makes sense for A to prefer
conflict, because if it waits, B’s bargaining range will shift further
and further away, eliminating any overlap between the two. Therefore, A
prefers to gamble on conflict even if the probability that A wins is
lower than B; the costs of war do not outweigh the benefits of a
peaceful but unreasonable settlement. Consider the 1956 Suez Crisis.
Egypt was seen as a rising power in the Middle East, having secured
control over the Suez canal. This threatened the interests of the
British and French governments in the region, who responded by
instigating a preventive war. To safeguard their diminishing influence,
the British and French launched a swift and successful military
intervention.</p>
<p><strong>Power shifts and AI.</strong> AIs could shift power as they
gain greater intelligence and more access to resources. Recall the
chapter on , where we saw that an agent’s power is highly related to the
efficiency with which they can exploit resources for their benefit,
which often depends on their level of intelligence. The power of future
AI systems is largely unpredictable; we do not know how intelligent or
useful they will be. This could give rise to substantial uncertainty
regarding how powerful potential adversaries using AI might become. If
this is the case, there might be reason to engage in conflict to prevent
the possibility of adversaries further increasing their power.</p>
<p><strong>First strike advantage overview.</strong> If an agent has a
<em>first-strike advantage</em>, they will do better to launch an attack
than respond to one. This gives rise to the following commitment
problem: an offensive advantage may be short-lived, so it is best to act
on it before the enemy does instead. Some ways in which an agent may
have a first strike advantage include:</p>
<ol>
<li><p>As explored above, anticipating a future power shift may motivate
an attack on the rising power to prevent it from gaining the upper
hand.</p></li>
<li><p>The costs of conflict might be lower for the attacker than they
are for the defender, so the attacker is better off securing on
offensive advantage while the defender is still in a position of
relative weakness.</p></li>
<li><p>The odds of victory may be higher for whichever agent attacks
first. The attacker might possess the element of surprise, the ability
to choose where conflict takes place, or the potential to quickly defeat
their opponent. For instance, a pre-emptive nuclear strike could be used
to target an enemy’s nuclear arsenal, thus diminishing their ability to
retaliate.</p></li>
</ol>
<p><strong>Examples: IPOs, patent Infringement, and Pearl
Harbor.</strong> When a company goes public, it can release an IPO,
allowing members of the general public to purchase company shares.
However, company insiders, such as executives and early investors, often
have access to valuable information not available to the general public;
this gives insiders a first-strike advantage. Insiders may buy or sell
shares based on this privileged information, leading to potential
regulatory conflicts or disputes with other investors who do not have
access to the same information. Alternatively, when a company develops a
new technology and files a patent application, they gain a first-strike
advantage by ensuring that their product will not be copied or
reproduced by other companies. If a rival company does create a similar
technology and later files a patent application, conflict can emerge
when the original company claims patent infringement. On the
international level, we note similar dynamics, such as in the case of
Pearl Harbor. To reduce the threat posed by the US’s Pacific fleet, the
Japanese initiated a devastating surprise attack on the US naval base at
Pearl Harbor. The attack was successful in securing a first-strike
advantage for Japan, but it also ensured the US’s entry into WWII.<br />
</p>
<table>
<caption>A pay-off matrix for competitors choosing whether to defend or
preemptively attack (first strike)</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;">defend</th>
<th style="text-align: center;">preempt</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">defend</td>
<td style="text-align: center;">2,2</td>
<td style="text-align: center;">0,3</td>
</tr>
<tr class="even">
<td style="text-align: left;">preempt</td>
<td style="text-align: center;">3,0</td>
<td style="text-align: center;">1,1</td>
</tr>
</tbody>
</table>
<p><strong>Effect on the bargaining range.</strong> When the advantages
of striking first outweigh the costs of conflict, it can shrink or
destroy the bargaining range entirely. For any two parties to reach a
mutual settlement through bargaining, each must be willing to freely
communicate information with the other. However, in doing so, each party
might have to reveal offensive advantages, which would increase their
vulnerability to attack. The incentive to preserve and therefore conceal
an offensive advantage from opponents’ pressures agents to defect from
bargaining.</p>
<p><strong>First Strike Advantage and AIs.</strong> One scenario in
which an AI may be motivated to secure a first strike advantage is
cyberwarfare. An AI might hack servers for a variety of reasons to
secure an offensive advantage. AIs may want to disrupt and degrade an
adversary’s capabilities by attacking and destroying critical
infrastructure. Alternatively, an AI might gather sensitive information
regarding a rival’s capabilities, vulnerabilities, and strategic plans
to leverage potential offensive advantages.<br />
AIs may provide first strike advantages in other ways, too. Sudden and
dramatic progress in AI capabilities could motivate one party to take
offensive action. For example, if a nation very rapidly develops a much
more powerful AI system than its military enemies, this could present a
powerful first strike advantage: by attacking immediately, they may hope
to prevent their rivals from catching up with them, which would lose
them their advantage. Similar incentives were likely at work when the US
was considering a nuclear strike on the USSR to prevent them from
developing nuclear weapons themselves.<br />
Reducing the possibility of first-strike advantages is challenging,
especially with AI. However, we can lower the probability that they
arise by ensuring that there is a balance between the offensive and
defensive capabilities of potential rivals. In other words, defense
dominance can facilitate peace because attempted attacks between rivals
are likely to be unsuccessful or result in mutually assured destruction.
Therefore, we might reduce the probability that AIs are motivated to
pursue a first-strike advantage by ensuring that humans maintain defense
dominance, for instance, by requiring that advanced AIs have a built-in
incorruptible fail-safe mechanism, such as a manual “off-switch.”</p>
<p><strong>Issue indivisibility overview.</strong> Settlements that fall
within bargaining range will always be preferable to conflict, but this
assumes that whatever issues agents bargain over are divisible. For
instance, two agents can divide a territory in an infinite amount of
ways insofar as the settlement they arrive at falls within the
bargaining range, satisfying both their interests and outweighing the
individual benefits of engaging in conflict. Unfortunately, however,
some goods are indivisible, which inspires the following commitment
problem: parties cannot always divide a good however they please—–some
goods are “all or nothing.” When parties encounter <em>issue
indivisibility</em> <span class="citation"
data-cites="fearon1995rationalist"></span>, the probability of conflict
increases. Indivisible issues include monarchies, small territories like
islands or holy sites, national religion or pride, and sovereign
entities such as states or human beings, among several others.</p>
<p><strong>Examples: shopping, organ donation, and
co-parenting.</strong> Imagine two friends that go out for a day of
shopping. For lunch, they stop at their favorite deli and find that it
only has one sandwich left: they decide to share this sandwich between
themselves. After lunch, they go to a clothing store, and both come
across a jacket they love, but of which there is only one left. They
begin arguing over who should get the jacket. Simply put, sandwiches can
be shared and jackets can’t. Issue indivisibility can give rise to
conflict, often leaving all parties involved worse off.<br />
The same can be true in more extreme cases, such as organ donation.
Typically, the available organ supply does not meet the transplant needs
of all patients. Decisions as to who gets priority for transplantation
may favor certain groups or individuals and allocation systems may be
unfair, giving rise to conflict between doctors, patients, and
healthcare administrations. Finally, we can also observe issue
indivisibility in co-parenting contexts. Divorced parents sometimes
fight for full custody rights over their children. This can result in
lengthy and costly legal battles that are detrimental to the family as a
whole.</p>
<p><strong>Effect on the bargaining range.</strong> When agents
encounter issue indivisibilities, they cannot arrive at a reasonable
settlement through bargaining. Sometimes, however, issue indivisibility
can be resolved through side payments. One case in which side payments
were effective was during the Spanish-American War of 1898, fought
between Spain and the United States over the territory of the
Philippines. The conflict was resolved when the United States offered to
buy the Philippines from Spain for 20 million dollars. Conversely, the
Munich Agreement at the dawn of WWII represents a major case where side
payments were ineffective. In an attempt to appease Hitler and avoid
war, the British and French governments reached an agreement with
Germany, allowing them to annex certain parts of Czechoslovakia. This
agreement involved side payments in the form of territorial concessions
to Germany, but it ultimately failed, as Hitler’s aggressive
expansionist ambitions were not satisfied, leading to the outbreak of
World War II. Side payments can only resolve issue indivisibility when
the value of the side payments outweighs the value of the good.<br />
</p>
<figure id="fig:first-strike">
<img src="images/multiagent-dynamics/image19.png" />
<figcaption>First strike advantage</figcaption>
</figure>
<p><strong>Issue indivisibility and AIs.</strong> Imagine there is a
very powerful AI training system, and that whoever has access to this
system will eventually be able to dominate the world. In order to reduce
the chance of being dominated, individual parties may compete with one
another to secure access to this system. If parties were to split the
AI’s compute up between themselves, it would no longer be as powerful as
it was previously, perhaps not more powerful than their existing
training systems. Since such an AI cannot be divided up among many
stakeholders easily, it may be rational for parties to conflict over
access to it, since doing so ensures global domination.</p>
<h3 id="information-problems">Information problems</h3>
<p>Misinformation and disinformation both involve the spread of false
information, but they differ in terms of intention. Misinformation is
the dissemination of false information, without the intention to
deceive, due to a lack of knowledge or understanding. Disinformation, on
the other hand, is the deliberate spreading of false or misleading
information with the intent to deceive or manipulate others. Both of
these types of information problem can cause bargains to fail,
generating conflict.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Distinguish</th>
<th style="text-align: center;">Defect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Distinguish</td>
<td style="text-align: center;"><span
class="math inline"><em>b</em> − <em>c</em></span></td>
<td style="text-align: center;"><span
class="math inline"> − <em>c</em>(1−<em>a</em>)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Defect</td>
<td style="text-align: center;"><span
class="math inline"><em>b</em>(1−<em>a</em>)</span></td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>The term <span class="math inline"><em>a</em></span> is the
probability of a player knowing the strategy of its partner. Relevant
for AI since it might reduce uncertainty (though still chaos and
incentives to conceal or misrepresent information or compete).</p>
<p><strong>Misinformation overview.</strong> Uncertainty regarding a
rival’s power or intentions can increase the probability of
conflict<span class="citation"
data-cites="fearon1995rationalist"></span>. Bargaining often requires
placing trust in another not to break an agreement. This harder to
achieve when one agent believes something false about the other’s
preferences, resources, or commitments. This lack of shared, accurate
information can lead to mistrust and a breakdown in negotiations.</p>
<p><strong>Example: Russian invasion of Ukraine.</strong> Incomplete
information may lead overly optimistic parties to make too large
demands, whereas rivals that are tougher than expected reject those
demands and instigate conflict. Examples of misinformation problems
generating conflict may include Russia’s 2022 invasion of Ukraine.
Russian President Putin reportedly miscalculated Ukraine’s willingness
to resist invasion and fight back. With more accurate information
regarding Ukraine’s abilities and determination, Putin may have been
less likely to instigate conflict.</p>
<p><strong>Effect on the bargaining range.</strong> Misinformation can
prevent agents from finding a mutually-agreeable bargaining range, as
shown in Figure 9.14. For example, if each agent believes themself to be
the more powerful party, each may therefore want more than half the
value they are competing for. Thus, each may reject any bargain offer
the other makes, since they expect a better if they opt for conflict
instead.</p>
<figure id="fig:information-problems">
<img src="images/multiagent-dynamics/image20.png" />
<figcaption>Information problems</figcaption>
</figure>
<p><strong>Misinformation and AI.</strong> AI technologies may produce
misinformation directly. Examples of this include large language models
hallucinating false facts. Less directly, AI development could also
exacerbate misinformation problems by increasing uncertainty. For
example, military AI applications may make wars more uncertain, and this
may increase the probability of conflict. AI weaponry innovation
presents an opportunity for states to gain power. However, AI
capabilities advances are often highly uncertain—-it may be unclear how
powerful a model trained on an order of magnitude of compute would be,
or how far behind adversaries are in their effort to create powerful
models. As automated warfare technologies become more widespread and
sophisticated, nations may struggle to predict their probability of
victory in any given conflict accurately. This increased potential for
miscalculation may make warfare more likely.<br />
There are other ways reducing information problems can reduce AI risk.
If there are substantial existential risks from AIs but this is not
widely agreed on, improving understanding of these risks could help make
different actors (such as the US and China) get better estimates of the
payoff matrix. With better understanding of AI risk, they may recognize
that it is in their self-interest to cooperate (slow down AI development
and militarization) instead of defecting (engaging in an AI race).
Similarly, creating information channels such as summits can increase
understanding and coordination; even if countries do not agree on shared
commitments, the discussions on the sidelines can reduce
misunderstandings and the risk of conflict.</p>
<p><strong>Disinformation overview.</strong> TEXT.Importantly, stronger
parties recognize that weaker parties have an incentive to
misrepresent–—they might bluff to get more out of the deal. Sometimes,
it is unclear how strong a rival is and what their intentions may be
when there are information problems.<br />
<strong>Examples: employment and the real estate industry.</strong>
Throughout labor markets, employers and job seekers often encounter
disinformation problems. Employers may intentionally withhold
information about the salary range or offer lower wages than what the
market standard suggests in order to secure lower employment costs. On
the other hand, job seekers might exaggerate their qualifications or
professional experience to increase their chances of getting hired. Such
discrepancies can lead to legal conflicts and high turnover rates.
Alternatively, in the real estate market, disinformation problems can
emerge between sellers and buyers. Sellers sometimes withhold critical
information about the property’s condition to increase the probability
that the property gets purchased. Buyers, on the other hand, may be
incentivized to misrepresent their budget or willingness to pay to
pressure sellers to lower their price. Oftentimes, this can result in
legal battles or disputes as well as the breakdown of property
transactions.</p>
<p><strong>Effect on the bargaining range.</strong> Consider two agents:
A, which is stronger, and B, which is weaker. B demands “X” amount for a
bargaining settlement, but A, as the stronger agent, will not offer this
to avoid being exploited by B. In other words, A thinks B is just trying
to get more for themself to “bait” A or “bluff” by implying that the
bargaining range is lower. But B might not be bluffing and A might not
be as strong as they think they are. Consider the Sino-Indian war in
this respect. At the time, India had perceived military superiority
relative to China. But in 1962, the Chinese launched an attack on the
Himalayan border with India, which demonstrated China’s superior
military capabilities, and triggered the Sino-Indian war. Therefore,
stronger parties prefer conflict when they believe rivals are bluffing,
whereas weaker parties prefer conflict when they believe rivals are not
as powerful as the latter think they are.<br />
<strong>Disinformation and AI.</strong> AIs themselves may have
incentives to misrepresent the facts. For example, the agent “Cicero,”
developed by Meta, is capable of very high performance in the board
wargame “Diplomacy.” Its success requires it to misrepresent certain
information to the other players in strategic fashion. We have seen many
other examples of AIs producing disinformation for a variety of reasons,
such as large language models successfully persuading users that they
are conversing with a human. The ability for AIs to misrepresent
information successfully is only likely to increase in future. This
could exacerbate disinformation problems, and thus contribute to greater
risk of conflict by eroding the potential for peaceful negotiation.</p>
<h3 id="conflict-factors-outside-of-bargaining-theory">Conflict factors
outside of bargaining theory</h3>
<p><strong>Inequality is another factor that is highly predictive of
conflict.</strong> Crime is a form of conflict. Income and educational
inequality are robust predictors of violent crime <span class="citation"
data-cites="kelly2000inequality"></span>, with an elasticity in excess
of 0.5 (elasticity measures how sensitive one variable is to changes in
another variable) even when controlling for variables such as race and
family composition. Similarly, individuals and families with a yearly
income below $15,000 are three times more likely to be the victims of
violent crime than are individuals and families with a yearly income
over $75,000 <span class="citation"
data-cites="victimrates2011"></span>. Moreover, economists from the
World Bank have also highlighted that the effects of inequality on both
violent and property crime are robust between countries, finding that
when economic growth improves in a country, violent crime rates decrease
substantially <span class="citation"
data-cites="fajnzylber2002inequality"></span>. This is consistent with
evidence at the national level; in the US, for example, the Bureau of
Justice reports that households below the federal poverty level have a
rate of violent victimization that is more than twice as high as the
rate for households above the federal poverty level. Moreover, these
effects were largely consistent between both rural and urban areas where
poverty was prevalent, further emphasizing the robust relationship
between inequality and conflict.</p>
<p><strong>Inequality and relative deprivation.</strong> Relative
deprivation is the perception or experience of being deprived or
disadvantaged in comparison to others. It is a subjective measure of
social comparison, not an objective measure of deprivation based on
absolute standards. People may feel relatively deprived when they
perceive that others possess more resources, opportunities, or social
status than they do. This can lead to feelings of resentment. For
example, “Strain theory,” proposed by sociologist Robert K. Merton,
suggests that individuals experience strain or pressure when they are
unable to achieve socially approved goals through legitimate means.
Relative deprivation is a form of strain, which may lead individuals to
resort to various coping mechanisms, one of which is criminal behavior.
For example, communities with a high prevalence of relative deprivation
can evolve a subculture of violence <span class="citation"
data-cites="horne2009effect"></span>. Consider the emergence of gangs,
in which violence becomes a way to establish dominance, protect
territory, and retaliate against rival groups, providing an alternative
path for achieving a desired social standing.</p>
<p><strong>AIs and relative deprivation.</strong> Advanced future AIs
and widespread automation may propel humanity into an age of abundance,
where many forms of scarcity have been largely eliminated on the
national, and perhaps even global scale. Under these circumstances, some
might argue that conflict will no longer be an issue; people would have
all of their needs met, and the incentives to resort to aggression would
be greatly diminished. However, as previously discussed, relative
deprivation is a subjective measure of social comparison, and therefore,
it could persist even under conditions of abundance.<br />
Consider the notion of a “hedonic treadmill,” which notes that
regardless of what good or bad things happen to people, they
consistently return to their baseline level of happiness. For instance,
reuniting with a loved one or winning an important competition might
cultivate feelings of joy and excitement. However, as time passes, these
feelings dissipate, and individuals tend to return to the habitual
course of their lives. Even if individuals were to have access to
everything they could possibly need, the satisfaction they gain from
having their needs fulfilled is only temporary.<br />
Abundance becomes scarcity reliably. Dissatisfied individuals can be
favored by natural selection over highly content and comfortable
individuals. In many circumstances, natural selection could disfavor
individuals who stop caring about acquiring more resources and expanding
their influence; natural selection favors selfish behavior (for more
detail, see <em>section <a href="#subsec:selection-and-selfish"
data-reference-type="ref"
data-reference="subsec:selection-and-selfish">3.5</a></em> of
<em>Evolutionary Pressures</em>). Even under conditions of abundance,
individuals may still compete for resources and influence because they
perceive the situation as a zero-sum game, where resources and power
must be divided among competitors. Individuals that acquire more power
and resources could incur a long-term fitness advantage over those that
are “satisfied” with what they already have. Consequently, even with
many resources, conflict over resources could persist in the evolving
population.<br />
Relatedly, in economics, the law of markets, also known as “Say’s Law,”
proposes that production of goods and services generates demand for
goods and services. In other words, supply creates its own demand.
However, if supply creates demand, the amount of resources required to
sustain supply to meet demand must also increase accordingly. Therefore,
steady increases in demand, even under resource-abundant conditions will
reliably result in resource scarcity.</p>
<p><strong>Conflict over social standing and relative power may
continue.</strong> There will always be scarcity of social status and
relative power, which people will continue to compete over. Social envy
is a fundamental part of life; it may persist because it tracks
differential fitness. Motivated by social envy, humans establish and
identify advantageous traits, such as the ability to network or climb
the social ladder. Scarcity of social status motivates individuals to
compete for social standing when doing so enables access to larger
shares of available resources. Although AIs may produce many forms of
abundance, there would still be dimensions on which to compete.
Moreover, AI development could itself exacerbate various forms of
inequality to extreme levels. We discuss this possibility in Chapter 9
Governance; Section 3 - Distribution.</p>
<h3 id="summary-6">Summary</h3>
<p>Throughout this section, we have discussed some of the major factors
that drive conflict. When any one of these factors is present, agents’
incentives to bargain for a peaceful settlement may shift such that
conflict becomes an instrumentally rational choice. These factors
include power shifts, first strike advantages, issue indivisibility,
information problems and incentives to misrepresent, as well as
inequality.<br />
In our discussion of these factors, we have laid the groundwork for
understanding the conditions under which decisions to instigate conflict
may be considered instrumentally rational. This knowledge base allows us
to better predict the risks and probability of AI-driven conflict
scenarios.<br />
First, we covered how power shifts can incentivize AI agents to pursue
conflict, to maintain strategic advantages or deter potential attacks
from stronger rivals, especially in the context of military AI
use.<br />
Second, we explored how the short-lived nature of offensive advantages
may incentivize AIs to pursue first-strike advantages, to degrade or
identify vulnerabilities in adversaries’ capabilities, as may be the
case in cyberwarfare.<br />
Third, we discussed issue indivisibility, imagining a future scenario in
which individual parties must compete for access to a world-dominating
AI. We reasoned that since dividing this AI between many stakeholders
would reduce its power, parties may find it instrumentally rational to
conflict for access to it.<br />
Fourth, we discussed how AIs may make wars more uncertain, increasing
the probability of conflict. We expect that AI weaponry innovation will
present an opportunity for superpowers to consolidate their dominance,
whereas weaker states may be able to quickly increase their power by
taking advantage of these technologies early on. This dynamic may create
a future in which power shifts are uncertain, which may lead states to
incorrectly expect that there is something to gain from going to
war.<br />
Finally, we explored the relationship between inequality and conflict.
We considered how even under conditions of abundance facilitated by
widespread automation and advanced AI implementation, relative
deprivation, and therefore conflict may persist. We also explored the
possibility that AIs may be motivated by social envy to compete with
other humans or AIs for desired social standing. This may result in a
global landscape in which the majority of humanity’s resources are
controlled by selfish, power-seeking AIs.<br />
Though AIs could evolve cooperative tendencies similarly to humans and
other animals, the possibility that they pursue interests and goals that
promote conflict, no matter how small, could pose catastrophic risks to
humans. It is thus important that we understand the drivers of conflict,
especially in the context of advanced future AIs.<br />
</p>
<h2 id="cooperation">Cooperation</h2>
<h3 id="direct-reciprocity">Direct Reciprocity</h3>
<p><strong>Direct reciprocity overview.</strong> One way agents may
cooperate is through <em>direct reciprocity</em>: when one agent
performs a favor for another because they expect the recipient to return
this favor in the future <span class="citation"
data-cites="trivers1971evolution"></span>. We capture this core idea in
idioms like “quid pro quo,” or “you scratch my back, I’ll scratch
yours.” Direct reciprocity requires repeated interaction between the
agents: the more likely they are to meet again in the future, the
greater the incentive for them to cooperate in the present. We have
already encountered this in the iterated Prisoner’s Dilemma: how an
agent behaves in a present interaction can influence the behavior of
others in future interactions . Game theorists sometimes refer to this
phenomenon as the “shadow of the future.” When individuals know that
future cooperation is valuable, they have increased incentives to behave
in ways that benefit both themselves and others, fostering trust,
reciprocity, and cooperation over time. Cooperation can only evolve as a
consequence of direct reciprocity when the probability, <span
class="math inline"><em>w</em></span>, of subsequent encounters between
the same two individuals is greater than the cost-benefit ratio of the
helpful act. In other words, if agent A decided to help agent B at some
cost to themselves, they will only do so when the expected benefit of
agent B returning the favor outweighs the cost of Agent A’s initially
providing it. Thus, we have the rule <span
class="math inline"><em>w</em> &gt; <em>c</em>/<em>b</em></span>; see
Table <a href="#tab:reciprocity" data-reference-type="ref"
data-reference="tab:reciprocity">5</a> below.<br />
</p>
<div id="tab:reciprocity">
<table>
<caption>Direct Reciprocity Matrix</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Cooperate</th>
<th style="text-align: center;">Defect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Cooperate</td>
<td style="text-align: center;"><span
class="math inline"><em>b</em> − <em>c</em>/(1−<em>w</em>)</span></td>
<td style="text-align: center;"><span
class="math inline"> − <em>c</em></span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Defect</td>
<td style="text-align: center;"><span
class="math inline"><em>b</em></span></td>
<td style="text-align: center;"><span class="math inline">0</span></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Natural examples of direct reciprocity.</strong> Trees and
fungi have evolved symbiotic relationships where they exchange sugars
and nutrients for mutual benefit. Dolphins use cooperative hunting
strategies where one dolphin herds schools of fish while the others form
barriers to encircle them. The dynamics of the role reversal are decided
by an expectation that other dolphins in the group will reciprocate this
behavior during subsequent hunts. Similarly, chimpanzees engage in
reciprocal grooming, where they exchange grooming services with one
another with the expectation that they will be returned during a later
session <span class="citation"
data-cites="schino2007grooming"></span>.<br />
Direct reciprocity in human society. Among humans, one prominent example
of direct reciprocity is commerce. Commerce is a form of direct
reciprocity "which offers positive-sum benefits for both parties and
gives each a selfish stake in the well-being of the other" <span
class="citation" data-cites="pinker2012better"></span>; commerce can be
a win-win scenario for all parties involved. For instance, if Alice
produces wine and Bob produces cheese, but neither Alice nor Bob has the
resources to produce what the other can, both may realize they are
better off trading. Different parties might both need the good the other
has when they can’t produce it themselves, so it is mutually beneficial
for them to trade, especially when they know they will encounter each
other again in the future. If Alice and Bob both rely on each other for
wine and cheese respectively, then they will naturally seek to prevent
harm to one another because it is in their rational best interest. To
this point, commerce can foster <em>complex interdependencies</em>
between economies, which enhances the benefits gained through mutual
exchange while decreasing the probability of conflict or war.</p>
<p><strong>Direct reciprocity and AIs.</strong> The future may contain
multiple AI agents, many of which might interact with one another to
achieve different functions in human society. Such AI agents may
automate parts of our economy and infrastructures, take over mundane and
time-consuming tasks, or provide humans and other AIs with daily
assistance. In a multi-AI agent system, where the probability that
individual AIs would meet again is high, AIs might evolve cooperative
behaviors through direct reciprocity. If one AI in this system has
access to important resources that other AIs need to meet their
objectives, it may decide to share these resources accordingly. However,
since providing this favor would be costly to the given AI, it will do
so only when the probability of meeting the recipient AIs (those that
received the favor) outweighs the cost-benefit ratio of the favor
itself.</p>
<p><strong>Direct reciprocity can backfire: AIs may disfavor cooperation
with humans.</strong> AIs may favor cooperation with other AIs over
humans. As AIs become substantially more capable and efficient than
humans, the benefit of interacting with humans may decrease. It may take
a human several hours to reciprocate a favor provided by an AI, whereas
it may take an AI only seconds to do so. It may therefore become
extremely difficult to formulate exchanges between AIs and humans that
benefit AIs more than exchanges with other AIs would. In other words,
from an AIs perspective, the cost-benefit ratio for cooperation with
humans is not worth it.</p>
<h3 id="indirect-reciprocity">Indirect Reciprocity</h3>
<p><strong>Indirect reciprocity overview.</strong> When someone judges
whether to provide a favor to someone else, they may consider the
recipient’s reputation. If the recipient is known to be generous, this
would encourage the donor (the one that provides the favor) to offer
their assistance. On the other hand, if the recipient has a stingy or
selfish reputation, this could discourage the donor from offering a
favor. In considering whether to provide a favor, donors may also
consider the favor’s effect on their own reputation. If a donor gains a
“helpful and trustworthy” reputation by providing a favor, this may
motivate others to cooperate with them more often. We call this
reputation-based mechanism of cooperation <em>indirect reciprocity</em>
<span class="citation" data-cites="nowak1998evolution"></span>. Agents
may cooperate to develop and maintain good reputations since doing so is
likely to benefit them in the long-term. Indirect reciprocity is
particularly useful in larger groups, where the probability that the
same two agents will encounter one another again is lower. It provides a
mechanism for leveraging collective knowledge to promote cooperation.
Where personal interactions are limited, reputation-based evaluations
provide a way to assess the cooperative tendencies of others.
Importantly, cooperation can only emerge within a population as a
consequence of indirect reciprocity when the probability, <span
class="math inline"><em>q</em></span>, that any agent can discern
another agent’s reputation (whether they are cooperative or not),
outweighs the cost-benefit ratio of the helpful behavior to the donor.
Thus, we have the rule <span
class="math inline"><em>q</em> &gt; <em>c</em>/<em>b</em></span>; see
Table <a href="#tab:indirect-repr" data-reference-type="ref"
data-reference="tab:indirect-repr">6</a> below.<br />
</p>
<p><span id="tab:indirect-repr" label="tab:indirect-repr"></span></p>
<div id="tab:indirect-repr">
<table>
<caption>Indirect Reciprocity Matrix</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Discern</th>
<th style="text-align: center;">Defect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Discern</td>
<td style="text-align: center;"><span
class="math inline"><em>b</em> − <em>c</em></span></td>
<td style="text-align: center;">-c(1-q)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Defect</td>
<td style="text-align: center;"><span
class="math inline"><em>b</em>(1−<em>q</em>)</span></td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Natural examples of indirect reciprocity.</strong> Cleaner
fish (fish that feed on parasites or mucus on the bodies of other fish)
can either cooperate with client fish (fish that receive the “services”
of cleaner fish) by feeding on parasites that live on their bodies, or
cheat, by feeding on the mucus that client fish excrete <span
class="citation" data-cites="bshary2006image"></span>. Client fish tend
to cooperate more frequently with cleaner fish that have a “good
reputation,” which are those that feed on parasites rather than mucus.
Similarly, while vampire bats are known to share food with their kin,
they also share food with unrelated members of their group. Vampire bats
more readily share food with unrelated bats when they know the
recipients of food sharing also have a reputation for being consistent
and reliable food donors <span class="citation"
data-cites="carter2013food"></span>.</p>
<p><strong>Indirect reciprocity in human society.</strong> Language
provides a way to obtain information about others without ever having
interacted with them, allowing humans to adjust reputations accordingly
and facilitate conditional cooperation. Consider sites like Yelp and
TripAdvisor, which allow internet users to gauge the reputations of
businesses through reviews provided by other consumers. Similarly,
gossip is a complex universal human trait that plays an important role
in indirect reciprocity. Through gossip, individuals reveal the nature
of their past interactions with others as well as exchanges they observe
between others but are not a part of. Gossip allows us to track each
others’ reputations and enforce cooperative social norms, reducing the
probability that cooperative efforts are exploited by others with
reputations for dishonesty <span class="citation"
data-cites="balliet2020indirect"></span>.</p>
<p><strong>Indirect reciprocity in AIs.</strong> AIs could develop a
reputation system where they observe and evaluate each others’
behaviors, with each accumulating a reputation score based on their
cooperative actions. AIs with higher reputation scores may be more
likely to receive assistance and cooperation from others, thereby
developing a reputation for reliability. Moreover, sharing insights and
knowledge with <em>reliable</em> partners may establish a network of
cooperative AIs, promoting future reciprocation.</p>
<p><strong>Indirect reciprocity can backfire: extortionists can threaten
reputational damage.</strong> The pressure to maintain a good reputation
can make agents vulnerable to extortion. Other agents may be able to
leverage the fear of reputational harm to extract benefits or force
compliance. For example, political smear campaigns manipulate public
opinion by spreading false information or damaging rumors about
opponents. Similarly, blackmail often involves leveraging damaging
information about others to extort benefits. AIs may manipulate or
extort humans in order to better pursue their objectives. For instance,
an AI might threaten to expose the sensitive, personal information it
has accessed about a human target unless specific demands are met.</p>
<p><strong>Indirect reciprocity can backfire: ruthless reputations may
also work.</strong> Indirect reciprocity can also favor the emergence of
“ruthless” reputations. A reputation for ruthlessness can sometimes be
extremely successful in motivating compliance through fear. For
instance, in military contexts, projecting a reputation for ruthlessness
may deter potential adversaries or enemies. If others perceive an
individual or group as willing to employ extreme measures without
hesitation, they may be less likely to challenge or provoke them. Some
AIs might similarly evolve ruthless reputations, perhaps as a defensive
strategy to discourage potential attempts at exploitation, or control by
others.</p>
<h3 id="group-selection">Group Selection</h3>
<p><strong>Group selection overview.</strong> When there is competition
between groups, groups with more cooperators may outcompete those with
fewer cooperators. Under such conditions, selection at the group level
influences selection at the individual level (traits that benefit the
group may not necessarily benefit the individual), and we refer to this
mechanism as <em>group selection</em> <span class="citation"
data-cites="west2007social"></span>. Importantly, between groups, groups
with a higher proportion of cooperators have an advantage. Cooperative
groups are better able to coordinate their allocation of resources,
establish channels for reciprocal exchange, and maintain steady
communication, making them less likely to go extinct. Moreover,
cooperative groups more frequently split in two: as cooperative groups
grow in size, social tensions may emerge and threaten the cohesion of
the group, leading members to break off into their own cooperative
groups. It so happens that, if <span
class="math inline"><em>m</em></span> is large and is the number of
groups and n is the maximum group size, group selection can only promote
cooperation when <span
class="math inline"><em>b</em>/<em>c</em> &gt; 1 + <em>n</em>/<em>m</em></span>;
see Table <a href="#tab:group" data-reference-type="ref"
data-reference="tab:group">7</a> below.<br />
</p>
<div id="tab:group">
<table>
<caption>Group Selection Matrix</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Cooperate</th>
<th style="text-align: center;">Defect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Cooperate</td>
<td style="text-align: center;"><span
class="math inline">(<em>n</em>+<em>m</em>)(<em>b</em>+<em>c</em>)</span></td>
<td style="text-align: center;"><span
class="math inline"><em>n</em>(−<em>c</em>) + <em>m</em>(<em>b</em>−<em>c</em>)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Defect</td>
<td style="text-align: center;"><span
class="math inline"><em>n</em><em>b</em></span></td>
<td style="text-align: center;"><span class="math inline">0</span></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Natural examples of group selection.</strong> Group selection
is widely viewed as a mechanism involved in the process of natural
selection, however, direct causal attribution is difficult for many
species. Nonetheless, we may view chimpanzees that engage in lethal
intergroup conflict as a likely example of group selection. Chimpanzees
can be remarkably violent toward outgroups, such as by killing the
offspring of rival males or engaging in brutal fights over territory.
Such behaviors can help groups of chimpanzees secure competitive
advantages over other groups of chimpanzees, by either reducing their
abilities to mate successfully through infanticide, or by securing
larger portions of available territory.</p>
<p><strong>Group selection in human society.</strong> Among humans,
warfare is a salient example of group selection. Imagine two armies: A
and B. The majority of soldiers in army A are brave, while the majority
of soldiers in army B are cowardly. For soldiers in army A, bravery may
be individually costly, since brave soldiers are more willing to risk
losing their lives on the battlefield. For soldiers in army B, cowardice
may be individually beneficial, since cowardly soldiers will take fewer
life-threatening risks on the battlefield. However, at the group-level,
when these two armies conflict, army A will defeat army B: brave
soldiers will be more willing to fight alongside each other for victory,
while cowardly soldiers will not.</p>
<p><strong>Group selection in AIs.</strong> Consider a future in which
the majority of human labor has been fully automated by AIs, such that
AIs are now running most companies. Under these circumstances, AIs may
form corporations with other AIs, creating an economic landscape in
which multiple AI corporations must compete with each other to produce
economic value. AI corporations in which individual AIs work well
together may outcompete those in which individual AIs do not work as
well together. The more cooperative individual AIs within AI
corporations are, the more economic value their corporations will be
able to produce; AI corporations with less cooperative AIs may
eventually run out of resources and lose the ability to sustain
themselves.</p>
<p><strong>Group selection can backfire: in-group favoritism can promote
out-group hostility.</strong> Group selection can inspire in-group
favoritism, which might lead to cruelty toward out-groups. Chimpanzees
will readily cooperate with members of their own groups. However, when
interacting with chimpanzees from other groups, they are often vicious
and merciless. Moreover, when groups gain a competitive advantage, they
may attempt to preserve it by mistreating, exploiting, or marginalizing
outgroups such as people with different political or ideological
beliefs. AIs may be more likely to see other AIs as part of their group,
and this could promote antagonism between AIs and humans.</p>
<h3 id="kin-selection">Kin Selection</h3>
<p><strong>Kin selection overview.</strong> When driven by <em>kin
selection</em>, agents are more likely to cooperate with others with
whom they share a higher degree of genetic relatedness <span
class="citation" data-cites="hamilton1964genetical"></span>. The more
closely related agents are, the more inclined to cooperate they will be.
Thus, kin selection favors cooperation under the following conditions:
an agent will help their relative only when the benefit to their
relative “<span class="math inline"><em>b</em></span>,” multiplied by
the relatedness between the two “<span
class="math inline"><em>r</em></span>,” outweighs the cost to the agent
“<span class="math inline"><em>c</em></span>.” This is known as
Hamilton’s rule: <span
class="math inline"><em>r</em><em>b</em> &gt; <em>c</em></span>, or
equivalently <span
class="math inline"><em>r</em> &gt; <em>c</em>/<em>b</em></span> <span
class="citation" data-cites="hamilton1964genetical"></span>; see Table
<a href="#tab:kin" data-reference-type="ref"
data-reference="tab:kin">8</a> below.</p>
<div id="tab:kin">
<table>
<caption>Kin Selection Matrix</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Cooperate</th>
<th style="text-align: center;">Defect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Cooperate</td>
<td style="text-align: center;"><span
class="math inline">(<em>b</em>−<em>c</em>)(1+<em>r</em>)</span></td>
<td style="text-align: center;"><span
class="math inline">(−<em>c</em>+<em>b</em><em>r</em>)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Defect</td>
<td style="text-align: center;"><span
class="math inline"><em>b</em> − <em>r</em><em>c</em></span></td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Natural examples of kin selection.</strong> In social insect
colonies, such as bees and ants, colony members are closely related.
Such insects often assist their kin in raising and producing offspring
while “workers” relinquish their reproductive potential, devoting their
lives to foraging and other means required to sustain the colony as a
whole. Similarly, naked mole rats live in colonies with a single
reproductive queen and non-reproductive workers. The workers are sterile
but still assist in tasks such as foraging, nest building, and
protecting the colony. This behavior benefits the queen’s offspring,
which are their siblings, and enhances the colony’s overall survival
capabilities. Alternatively, some bird species engage in cooperative
breeding practices where older offspring delay breeding to help parents
raise their siblings.</p>
<p><strong>Kin selection in human society.</strong> Some evolutionary
psychologists claim that we can see evidence of kin selection in many
commonplace traditions and activities. For example, in humans, we might
identify the mechanism of kin selection in the way that we treat our
immediate relatives. For instance, people often leave wealth, property,
and other resources to direct relatives upon their deaths. Leaving
behind an inheritance offers no direct benefit to individuals, but it
does help ensure the survival and success of their lineage in subsequent
generations. Similarly, grandparents often care for their grandchildren,
which increases the probability that their lineages will persist.</p>
<p><strong>Kin selection in AIs.</strong> AIs that are similar could
exhibit cooperative tendencies towards each other, similar to genetic
relatedness in biological systems. For instance, AIs may create back ups
or variants of themselves. They may then favor cooperation with these
versions of themselves over other AIs or humans. Variant AIs may
prioritize resource allocation and sharing among themselves, developing
preferential mechanisms for sharing computational resources with other
versions of themselves.</p>
<p><strong>Kin selection can backfire: nepotism.</strong> Kin selection
can lead to nepotism: prioritizing the interests of relatives above
others. For instance, some bird species exhibit differential feeding and
provisioning. When chicks hatch asynchronously, parents may allocate
more resources to those that are older, and therefore more likely to be
their genetic offspring, since smaller chicks are more likely to be the
result of brood parasitism (when birds lay their eggs in other birds’
nests). In humans, too, we often encounter nepotism. Company executives
may hire their sons or daughters, even though they lack the experience
required for the role, which can harm companies and their employees in
the long-run. Similarly, parents often protect their children from the
law, especially when they have committed serious criminal acts that can
result in extended jail time. Such tendencies could apply to AIs as
well: AIs might favor cooperation only with other similar AIs. This
could be especially troubling for humans –— as the differences between
humans and AIs increase, AIs may be increasingly less inclined to
cooperate with humans.</p>
<h3 id="individual-stakes-to-common-stakes">Individual stakes to common
stakes</h3>
<p><strong>Individual stakes to common stakes overview.</strong> The
Nobel laureate economist John Harsanyi suggested a concept like the veil
of ignorance to explore how we should structure our societies. Behind
the veil of ignorance, agents are unaware of their personal
characteristics and roles in society. This forces them to act as
impartial observers when envisioning how the group or society should
interact with each other. (For a more detailed discussion of the veil of
ignorance, see <em>Section of the chapter</em>: A Brief Introduction).
Because agents do not know what place they will hold (e.g., whether they
will get an average position in society or a different position) in the
society they envision, they often act more impartially and cooperatively
when constructing a group or society. When rational agents are ignorant
of their future position in society—–when they are forced to make
decisions behind the veil of ignorance-—-they are more likely to make
decisions that maximize collective wellbeing, enabling cooperation
through the mechanism of <em>individual stakes to common stakes</em>
<span class="citation"
data-cites="nowak2011supercooperators"></span>.</p>
<p><strong>Natural examples of individual stakes to common
stakes.</strong> Intragenomic conflict arises when an individual
organism’s genes have differing interests with respect to their
transmission to the next generation. For example, some genes may have
evolved mechanisms that increase their own replication at the expense of
the organism’s wellbeing. The process of meiosis, however, can resolve
the problem of intragenomic conflict through randomization or the
creation of a “Darwinian veil of ignorance.” Meiosis is a process of
cell division that occurs during the formation of reproductive cells in
sexually reproducing organisms. Meiosis typically results in a 50-50
distribution of genetic material from both parents to offspring,
fostering genetic diversity. While it would be “better” for any
individual gene to make more copies of itself in offspring, it cannot
“know” whether doing so will actually increase the fitness of the
organism, allowing it to pass on its genes to subsequent generations.
However, by fostering genetic diversity, the process of meiosis opens
the door to randomized success, allowing the forces of natural selection
to impartially dictate which genes and their respective traits propagate
<span class="citation" data-cites="okasha2012social"></span>.</p>
<p><strong>Individual stakes to common stakes in human society.</strong>
The “Hutterites” are a religious group known for their cooperative and
communal lifestyle <span class="citation"
data-cites="Harsanyi1955CardinalWI Dennett1995DarwinsDI"></span>. They
represent a prime example of how the mechanism of individual stakes to
common stakes can be used to promote cooperation. Within their
communities, the Hutterites distribute all resources among themselves,
and relinquish any personal wealth or possessions. This allows the
Hutterites to live under a collective identity, where focus on the
common good often overrides individual interests. As another example,
when their group gets too large, Hutterites set up another place for
people to live, and once it is finished, they randomly select members
from their community to go live there. Alternatively, consider a
scenario in which a group of people must row to shore. The ship does not
have enough food for everyone, so the group will have to throw some
rowers overboard at the halfway point to ensure the rest of the group’s
survival. However, no one in the group knows that they may be thrown
overboard, but they do know that to make it to shore, they must all row
now. If people knew that they would be thrown overboard at some point,
they would be far less likely to row.</p>
<p><strong>Individual stakes to common stakes and AIs.</strong> People
might perceive advanced future AIs as an outgroup threat, and this may
motivate humans to cooperate against AIs. As AI’s become more widespread
and powerful, they may begin to threaten human values at the global
scale, motivating nations—–for instance, the US and China–—to set aside
their differences and come together to address a common threat. If AI is
someday viewed by humanity as an “invasive species,” it may promote
global solidarity.</p>
<h3 id="simons-selection-mechanism">Simon’s Selection Mechanism</h3>
<p><strong>Simon’s selection mechanism overview.</strong> Humans and
other animals typically do not have access to all available information,
time, and cognitive abilities required to make the best possible
decision. Individuals are restricted by their “bounded rationality.” To
overcome their bounded rationality and arrive at better solutions,
people and organisms can benefit, on average, from relying on and
receiving information through cooperative social channels. Individuals
that take advantage of cooperative social channels have an increased
ability to acquire socially transmitted skills and conform to socially
established norms, thereby incurring a fitness advantage over those that
do not participate in such channels. Moreover, when individuals choose
to contribute to cooperative social channels, the information they
provide can be freely utilized by anyone who participates in the system.
If this information is widely used and benefits the entire social
structure, social norms may emerge that compel others participating in
the cooperative social channel to conform their behavior appropriately.
In other words, when individuals participate in cooperative social
channels, they may benefit from the collective intelligence they
provide, but they may also face a cost from the group which can
influence their behaviors through norms or rules. This idea was
pioneered by the political scientist and Nobel laureate, Herbert Simon,
and we refer to it as <strong>Simon’s selection mechanism</strong> <span
class="citation" data-cites="simon1990mechanism"></span>.</p>
<p><strong>Simon’s selection mechanism in human society.</strong> Humans
have a tendency to believe in facts and propositions that they have not
had the opportunity to independently verify. Such knowledge is typically
disseminated through established cooperative social channels, such as
the internet or culture. For instance, many people believe that
consuming too much of certain kinds of cholesterol is bad for one’s
health, or that touching a hot stove should always be avoided. People
generally agree upon these beliefs not because they are skilled medical
literature reviewers or seasoned chefs, but because such beliefs have
become socially transmitted common knowledge. In general, people are
better off being a part of society, rather than isolating themselves
from it. Societal isolation may prevent individuals from accessing
critical information that is disseminated through social channels. For
instance, a hermit living in the woods with no internet access might be
unaware that there is steadily expanding forest fire in the
vicinity.</p>
<p><strong>Simon’s selection mechanism and free riding.</strong> One
drawback of Simon’s selection mechanism is that it may enable free
riding. Individuals may benefit from the information contained in
cooperative social channels without themselves contributing to it.
Consider the open-source knowledge base, Wikipedia, in this respect.
Though anyone can, in theory, contribute to Wikipedia, few people
actually do, because doing so can require extensive time and effort.
This can result in a knowledge base that has much outdated content.</p>
<p><strong>Simon’s selection mechanism and AIs.</strong> In a
multi-agent setting, AIs may be interacting with one another directly.
They may create new communication channels and protocols among
themselves, and from their interactions, norms and information channels
may emerge. Such dynamics can give rise to very complex social systems
from which AIs are more able to benefit than humans are. In other words,
AIs may incur fitness advantages over humans by developing and
participating in their own cooperative social channels, which themselves
may preclude human understanding. Moreover, such cooperative social
channels may evolve into AI collective intelligences, just as the
internet now represents a form of human collective intelligence.
However, within these collective intelligences, AIs could maintain large
numbers of complex relationships with other AIs simultaneously, arriving
at potentially new forms of self-organization that increase AIs ability
to achieve their goals. AI collective intelligences would be vastly
superior to human collective intelligences, and as a consequence, humans
may be prevented from participating in and understanding the decisions
AIs make within such systems.</p>
<div class="storybox">
<p><span>A Note on Morality as Cooperation</span> The theory of
“Morality as Cooperation” (MAC) proposes that human morality was
generated by evolutionary pressures to solve our most salient
cooperation problems <span class="citation"
data-cites="curry2016morality"></span>. Natural selection has discovered
several mechanisms by which rational and self-interested agents may
cooperate with one another, and MAC theory suggests that some of these
mechanisms have driven the formation of our moral intuitions and
customs. Here, we examine four cooperation problems, the mechanisms
humans have evolved to solve them, and how these mechanisms may have
generated our ideas of morality. These are overviewed in Table <a
href="#tab:cooperation" data-reference-type="ref"
data-reference="tab:cooperation">9</a>.</p>
<div id="tab:cooperation">
<table>
<caption>Mapping cooperation mechanisms to morality components - Adapted
from <span class="citation"
data-cites="curry2016morality"></span></caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Cooperation Problem</strong></th>
<th style="text-align: left;"><strong>Solutions/Mechanism</strong></th>
<th style="text-align: left;"><strong>Component of
Morality</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">Kin selection</td>
<td style="text-align: left;">Parental duties, family values</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span>2-3</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>Agents can benefit by treating genetic
relatives preferentially</em></td>
<td style="text-align: left;">Avoiding inbreeding</td>
<td style="text-align: left;">Incest aversion</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Forming alliances and collaborating</td>
<td style="text-align: left;">Friendship, loyalty, commitment, team
players</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span>2-3</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>Agents must coordinate their behavior
to profit from mutually-benefical situations</em></td>
<td style="text-align: left;">Developing theory-of-mind</td>
<td style="text-align: left;">Understanding intention, not merely
action</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">Direct reciprocity (e.g. tit-for-tat)</td>
<td style="text-align: left;">Trust, gratitude, revenge, punishment,
forgiveness</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span>2-3</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>Agents need each other to reciprocate
and contribute despite incentives to free ride</em></td>
<td style="text-align: left;">Indirect reciprocity (e.g. forming
reputations)</td>
<td style="text-align: left;">Patience, guilt, gratitude</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Division</td>
<td style="text-align: left;">Fairness, negotiation, compromise</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span>2-3</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>Agents can benefit from avoiding
conflict, which is mutually costly</em></td>
<td style="text-align: left;">Deference to prior ownership</td>
<td style="text-align: left;">Respecting others’ property, punishing
theft</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Kinship.</strong> Natural selection can favor agents who
cooperate with their genetic relatives. This is because there may be
copies of these agents’ genes in their relatives’ genomes, and so
helping them may further propagate their own genes. We call this
mechanism “kin selection” <span class="citation"
data-cites="hamilton1964genetical"></span>: an agent can gain a fitness
advantage by treating their genetic relatives preferentially, so long as
the cost-benefit ratio of helping is less than the relatedness between
the agent and their kin. Similarly, repeated inbreeding can reduce an
agent’s fitness by increasing the probability of producing offspring
with both copies of any recessive, deleterious alleles in the parents’
genomes <span class="citation"
data-cites="charlesworth2009genetics"></span>.<br />
MAC theory proposes that the solutions to this cooperation problem
(preferentially helping genetic relatives), such as kin selection and
inbreeding avoidance, underpin several major moral ideas and customs.
Evidence for this includes the fact that human societies are usually
built around family units <span class="citation"
data-cites="chagnon1979kin"></span>, in which “family values” are
generally considered highly moral. Loyalty to one’s close relatives and
duties to one’s offspring are ubiquitous moral values across human
cultures <span class="citation"
data-cites="westermarck2022origin"></span>. Our laws regarding
inheritance <span class="citation"
data-cites="smith1987inheritance"></span> and our naming traditions
<span class="citation" data-cites="oates2002nominal"></span> similarly
reflect these moral intuitions, as do our rules and social taboos
against incest <span class="citation"
data-cites="lieberman2003does thornhill1991evolutionary"></span>.</p>
<p><strong>Mutualism.</strong> In game theory, some games are “positive
sum” and “win-win”: the agents involved can increase the total available
value by interacting with one another in particular ways, and all the
agents can then benefit from this additional value. Sometimes, securing
these mutual benefits requires that the agents coordinate their behavior
with each other. To solve this cooperation problem, agents may form
alliances and coalitions <span class="citation"
data-cites="connor1995benefits"></span>. This may require the capacity
for basic communication, rule-following <span class="citation"
data-cites="vanvugt2008leadership"></span>, and perhaps theory-of-mind
<span class="citation"
data-cites="carruthers1996theories"></span>.<br />
MAC theory proposes that these cooperative mechanisms comprise important
components of human morality. Examples include the formation of—and
loyalty to—friendships, commitments to collaborative activities, and a
certain degree of in-group favoritism and conformation to local
conventions. Similarly, we often consider the agent’s intentions when
judging the morality of their actions, which requires a certain degree
of theory-of-mind.</p>
<p><strong>Exchange.</strong> Sometimes, benefiting from “win-win”
situations requires more than mere coordination. If the payoffs are
structured so as to incentivize “free riding” behaviors, the cooperation
problem becomes how to ensure that others will reciprocate help and
contribute to group efforts. To solve this problem, agents can enforce
cooperation via systems of reward, punishment, policing, and reciprocity
<span class="citation" data-cites="west2007evolutionary"></span>. Direct
reciprocity concerns doing someone a favor out of the expectation that
they will reciprocate at a later date <span class="citation"
data-cites="trivers1971evolution"></span>. Indirect reciprocity concerns
doing someone a favor to boost your reputation in the group, out of the
expectation that this will increase the probability of a third party
helping you in the future <span class="citation"
data-cites="nowak1998evolution"></span>.<br />
Once again, MAC theory proposes that these mechanisms are found in our
moral systems. Moral ideas such as trust, gratitude, patience, guilt,
and forgiveness can all help to assure against free riding behaviors.
Likewise, punishment and revenge, both ideas with strong moral
dimensions, can serve to enforce cooperation more assertively. Idioms
such as “an eye for an eye”, or the “Golden Rule” of treating others as
we would like to be treated ourselves, reflect the solutions we evolved
to this cooperation problem.</p>
<p><strong>Conflict resolution.</strong> Conflict is very often
“negative sum”: the interaction of the agents themselves can destroy
some amount of the total value available. Examples span from the wounds
of rutting deer to the casualties of human wars. If the agents instead
manage to cooperate with each other, they may both be able to benefit—a
“win-win” outcome. One way to resolve conflict situations is division
<span class="citation" data-cites="nash1950bargaining"></span>: dividing
up the value between the agents, such as through striking a bargain.
Another solution is to respect prior ownership, deferring to the
original “owner” of the valuable item <span class="citation"
data-cites="gintis2007evolution"></span>.<br />
According to MAC theory, we can see both of these solutions in our ideas
of morality. The cross-culturally ubiquitous notions of fairness,
equality, and compromise help us resolve conflict by promoting the
division of value between competitors <span class="citation"
data-cites="henrich2005economic"></span>. We see this in ideas such as
“taking turns” and “I cut, you choose” <span class="citation"
data-cites="brams1996fair"></span>: mechanisms for turning a negative
sum situation (conflict) into a zero sum one (negotiation), to mutual
benefit. Likewise, condemnation of theft and respect for others’
property are extremely important and common moral values <span
class="citation"
data-cites="herskovits1952economic westermarck2022origin"></span>. This
set of moral rules may stem from the conflict resolution mechanism of
deferring to prior ownership.</p>
<p><strong>Conclusion.</strong> MAC theory argues that morality is
composed of biological and cultural solutions humans evolved to the most
salient cooperation problems of our ancestral social environment. Here,
we explored four examples of cooperation problems, and how the solutions
to them discovered by natural selection may have produced our moral
values.</p>
</div>
<h3 id="institutions">Institutions</h3>
<p><strong>Institutions overview.</strong> Agents are more likely to be
cooperative when there are laws or externally imposed incentives that
reward cooperation and punish defection. We define an
<strong>institution</strong> as an intentionally designed large-scale
structure that is publicly accepted and recognized, has a centralized
logic, and serves to mediate human interaction. Some examples of
institutions include governments, the UN, IAEA, and so on; in this
section, by “institutions,” we do not mean widespread or standardized
social customs such as the “institution” of marriage. Institutions
typically aim to establish collective goals which require collaboration
and engagement from large or diverse groups. Therefore, a possible way
of representing many institutions, such as governments, is with the
concept of a “Leviathan”: a powerful entity that can exert control or
influence over other actors in a system.</p>
<p><strong>The Pacifist’s dilemma and social control.</strong> When
one’s opponent is potentially aggressive, pacifism can be irrational. In
his book, “The Better Angels of Our Nature,” Steven Pinker refers to
this as the “Pacifist’s dilemma” <span class="citation"
data-cites="pinker2012better"></span>. In potential conflict scenarios,
agents have little to gain and a lot to lose when they respond to
aggression with pacifism; see Table <a href="#tab:pacifist"
data-reference-type="ref" data-reference="tab:pacifist">10</a> below.
This dynamic often inspires rational agents to choose conflict over
peace.<br />
</p>
<div id="tab:pacifist">
<table>
<caption>Pacifist’s Dilemma Payoff Matrix (No leviathan) </caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;"><span
style="color: blue">Pacifist</span></th>
<th style="text-align: center;"><span
style="color: blue">Aggressor</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span
style="color: red">Pacifist</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span style="color: blue">Peace + Profit
<span class="math inline">(100+5) = 105</span></span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span style="color: blue">Victory (<span
class="math inline">10</span>)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span
style="color: red">Aggressor</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span style="color: blue">Defeat(<span
class="math inline"> − 100</span>)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span style="color: blue">War(<span
class="math inline"> − 50</span>)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<p>However, we can shift the interests of agents in this context in
favor of peace by introducing a Leviathan, in the form of a third-party
peacekeeping or balancing mission, which establishes an authoritative
presence that maintains order and prevents conflict escalation.
Peacekeeping missions can take several forms, but they often involve the
deployment of peacekeeping forces such as military, police, and civilian
personnel. These forces work to deter potential aggressors, enhance
security, and set the stage for peaceful resolutions and negotiations as
impartial mediators, usually by penalizing aggression and rewarding
pacifism; see Table <a href="#tab:leviathan" data-reference-type="ref"
data-reference="tab:leviathan">11</a> below.<br />
</p>
<div id="tab:leviathan">
<table>
<caption>Leviathan Payoff Matrix - <span class="citation"
data-cites="pinker2012better"></span></caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;"><span
style="color: blue">Pacifist</span></th>
<th style="text-align: center;"><span
style="color: blue">Aggressor</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span
style="color: red">Pacifist</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span style="color: blue">Peace (<span
class="math inline">5</span>)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span style="color: blue">Victory -
Penalty (<span class="math inline">10 − 15 =  − 5</span>)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span
style="color: red">Aggressor</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span style="color: blue">Defeat (<span
class="math inline"> − 100</span>)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span style="color: blue">War - Penalty
(<span class="math inline"> − 50 − 200 =  − 250</span>)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Institutions in human society.</strong> Institutions play a
central role in promoting cooperation in international relations.
Institutions, such as the UN, can broker agreements or treaties between
nations and across cultures through balancing and peacekeeping
operations. The goal of such operations is to hold nations accountable
on the international scale; when nations break treaties, other nations
may punish them by refusing to cooperate, such as by cutting off trade
routes or imposing sanctions and tariffs. On the other hand, when
nations readily adhere to treaties, other nations may reward them, such
as by fostering trade or providing foreign aid. Similarly, institutions
can incentivize cooperation at the national scale by creating laws and
regulations that reward cooperative behaviors and punish non-cooperative
ones. For example, many nations attempt to prevent criminal behavior by
leveraging the threat of extended jail-time as a legal deterrent to
crime. On the other hand, some nations incentivize cooperative behaviors
through tax breaks, such as those afforded to citizens that make
philanthropic donations or use renewable energy resources like solar
power.</p>
<p><strong>Institutions and AI.</strong> Institutions are crucial in the
context of international AI development. By establishing laws and
regulations concerning AI development, institutions may be able to
reduce AI races, lowering competitive pressures and the probability that
countries cut corners on safety. Moreover, we may want concrete
international agreements on AI development that serve to hold nations
accountable; institutions could play a central role in helping us broker
these kinds of agreements. Ultimately, institutions could improve
coordination mechanisms and international standards for AI development,
which would correspondingly improve AI safety.<br />
Recall that institutions can punish defectors. In the future, harmful
AIs or harmful malicious actors could be punished with or by AIs. In the
future, humanity alone may not possess the power required to combat
advanced dominance-seeking AIs, and existing laws and regulations may be
insufficient if there is no way to enforce them. Such an AI Leviathan
could help regulate other AIs and affect their evolution, in which
selfish AIs are counteracted or domesticated.</p>
<p><strong>How institutions can backfire: corruption, free riding,
inefficiency.</strong> Institutions sometimes fail to achieve the goals
they set for themselves, even if they are well-intended. Failure to
achieve such goals is often the result of corruption, free riding, and
inefficiency at the institutional scale. Some examples of corruption
include bribery, misappropriation of public funds for private interests,
voter fraud and manipulation, and price fixing, among many others.
Examples of free-riding include scenarios like welfare fraud, where
individuals fraudulently receive benefits they may not be entitled to,
reducing the available supply of resources for those genuinely in need.
Institutions can also struggle with inefficiency, which may stem from
factors such as the satisfaction of bureaucratic requirements, the
emergence of natural monopolies, or the development of diseconomies of
scale, which may cause organizations to pay a higher average cost to
produce more goods and services. Institutions can be undermined,
corrupted, and poorly designed or outdated: they do not guarantee that
we will be able to fix cooperation problems.<br />
Like humans, AIs may be motivated to corrupt existing institutions.
Advanced AIs might learn to leverage the institutions we have in place
for their benefit, and might do so in ways that are virtually
undetectable to us. Moreover, as we discussed previously, AIs might form
an AI Leviathan. However, if humanity’s relationship with this Leviathan
is not symbiotic and transparent, humans risk losing control of AIs. For
instance, if groups of AIs within the Leviathan collude behind the
scenes to further their own interests, or power and resources become
concentrated with a few AIs at the “top,” humanity’s collective
wellbeing could be threatened.</p>
<h3 id="summary-7">Summary</h3>
<p>Throughout this section, we discussed a variety of mechanisms that
promote cooperation. These mechanisms included direct reciprocity,
indirect reciprocity, group selection, kin selection, individual stakes
to common stakes, Simon’s selection mechanism, and institutions.<br />
First, we discussed how direct reciprocity may motivate AI agents in a
multi-agent setting to cooperate with each other, if the probability
that the same two AIs meet again is sufficiently high. However, we also
considered the possibility that AIs may disfavor cooperation with humans
as they become progressively more advanced: the cost-benefit ratio for
cooperation with humans may simply be bad from an AI’s
perspective.<br />
Second, we explored how indirect reciprocity may promote cooperation in
AIs that develop a reputation system where they observe and score each
others’ behaviors. AIs with higher reputation scores may be more likely
to receive assistance and cooperation from others. Still, this does not
guarantee that AIs will be cooperative: AIs might leverage the fear of
reputational harm to extort benefits from others, or themselves develop
ruthless reputations to inspire cooperation through fear.<br />
Third, we considered how group selection –– in a future where labor has
been automated such that AIs now run the majority of companies –– could
promote cooperation on a multi-agent scale. AIs may form corporate
coalitions with other AIs to protect their interests; AI groups with a
cooperative AI minority may be outcompeted by AI groups with a
cooperative AI majority. Under such conditions, however, AIs may learn
to favor in-group members and antagonize out-group members, in order to
maintain group solidarity. AIs may be more likely to see other AIs as
part of their group, and this could lead to conflict between AIs and
humans.<br />
Fourth, we explored the possibility that AIs may create variants of
themselves, and the forces of kin selection may drive these related
variants to cooperate with each other. However, this could also give
rise to nepotism, where AIs prioritize the interests of their variants
over other AIs and humans. As the differences between humans and AIs
increase, AIs may be increasingly less inclined to cooperate with
humans.<br />
Fifth, we discussed the mechanism of individual stakes to common stakes.
As an example, we explored the possibility that AIs may someday be
viewed as an out-group, an existential threat by humanity. This may
inspire humanity to set aside its differences, promoting global
solidarity as a means to address catastrophic threats posed by AI.<br />
Sixth, we broke down Simon’s selection mechanism, highlighting that to
overcome their bounded rationality, agents may be better off
establishing and relying on cooperative social channels. We extended
this idea to AI agents as well, supposing that in a multi-agent setting,
AIs may develop their own communication channels and protocols, which
may give them a fitness advantage over humans. Moreover, we also
considered the possibility that AIs might use their cooperative social
channels to build collective intelligences, which, being vastly superior
to human collective intelligences, would preclude humans from taking
part in and understanding the decisions AIs make within such
systems.<br />
Finally, we concluded with a discussion of how institutions can
incentivize cooperation through externally imposed incentives that
enforce cooperation and punish defection <span class="citation"
data-cites="buterin2022institution"></span>. We related this concept to
the idea of an AI Leviathan used to counteract selfish, powerful AIs.
However, we also stressed that humans should take care to ensure their
relationship with the AI Leviathan is symbiotic and transparent,
otherwise humans risk losing control of AIs.<br />
In our discussion of these mechanisms, we not only illustrated their
prevalence in our world, but also showed how they might influence
cooperation with AI agents. In several cases, the mechanisms we discuss
could promote cooperation in AI. However, no single mechanism provides a
foolproof method for ensuring cooperation. In the following section, we
discuss the nature of conflict, namely the various factors that may give
rise to it. In doing so, we enhance our understanding of what might
motivate conflict in AI, and subsequently, our abilities to predict and
address AI-driven conflict scenarios.</p>
<h1 id="discussion">Discussion</h1>
<h2 id="chapter-review">Chapter Review</h2>
<p>In this chapter, we considered a variety of multi-agent dynamics in
biological and social systems to explore the emergent risks associated
with the development and interactions of multiple AI agents. Our
underlying thesis was that multi-agent AI dynamics might foster
undesirable outcomes, mirroring patterns observable in nature and
society. The dangers posed by multiple interacting AI agents extend
beyond the individual threat levels; they can be catastrophic or even
existentially threatening to humans. The total risk is not merely the
sum of individual parts but a manifestation of their interactions. In
other words, the safety of AI systems cannot be guaranteed solely by the
alignment of each AI agent to well-intentioned operators. This calls for
heightened attention and strategic intervention to avoid disastrous
futures.<br />
We began this chapter with a simple game: the Prisoner’s Dilemma. Here,
we observed that even rational agents may reach equilibrium states that
are detrimental to all. In this scenario, defection becomes the dominant
strategy, leading to a Nash equilibrium that is Pareto inefficient.
Intelligence and rationality does not guarantee that good decisions will
always be made; AI agents may be similarly subject to the dynamics
predicted by the Prisoner’s Dilemma. Next, we considered the effects of
extending this game through time using the Iterated Prisoner’s Dilemma.
Here, we found that uncertainty could foster rational cooperation, even
when defection remains the dominant strategy in fixed rounds of
interaction.<br />
When we expanded the playing field by adding more agents. In multi-agent
“tournaments,” we found that “nice, forgiving and retaliatory”
strategies such as <span style="color: gray">tit-for-tat</span> were
often successful. However, in tournaments where agents could change
their strategies, the dynamics more closely resembled those of
biological evolution. In these contexts, there may be no equilibrium
state at all: the population may simply undulate through different
strategy compositions unpredictably. The emergence of <span
style="color: gray">extortion</span> as a strategy illustrated a grim
possibility for future AI systems: AI extortion could be a source of
monumental disvalue, particularly if it were to involve digital minds of
moral significance. Moreover, AI extortion might persist stably
throughout populations of AI agents, which could make it difficult to
eradicate, especially if AIs learn to deceive or manipulate humans to
obscure their true intentions.<br />
Moreover, adding interactions that involved more than two agents
simultaneously introduced collective action problems, highlighting the
friction between individual and group interests. In society, these
dynamics manifest in challenges like climate change, public health
issues, voting dilemmas, and deforestation. The central lesson here can
be summarized in the words of Thomas Schelling, “Micromotives <span
class="math inline">≠</span> macrobehavior.” Even when individual agents
share similar goals, the system-level dynamics can override their
intentions and create undesirable outcomes. We suppose that individual
AI agents would also be sensitive to these system-level dynamics, and as
a consequence, may shift their behaviors and strategies in ways that
favor individual interests, oftentimes at the cost of the common
good.<br />
Examining these natural dynamics also allowed us to draw parallels with
AI development, deployment, and adoption. Observations of “AI races” in
corporate and military contexts serve as a clear reminder that such
dynamics can exacerbate AI risks, potentially resulting in catastrophes
in the form of autonomous economies or flash wars.<br />
Our exploration of frequency-dependent selection within the Hawk-Dove
game demonstrated how a Nash equilibrium might involve a range of
strategies, even within identical agents in identical environments. This
phenomenon might enable the persistence of undesirable AI behaviors such
as deception, free-riding, and extortion. In considering Generalized
Darwinism, we found that AI development could be subject to evolution by
natural selection, much like biological entities, as it plausibly meets
Lewontin’s conditions. This reveals the stark possibility of AI systems
naturally evolving towards selfish behavior.<br />
We ended the chapter by closely examining the drivers of conflict and
cooperation in the real world. Drawing from biological systems and human
societies, we illustrated an array of mechanisms that may promote
cooperation in AI. However, we also highlighted the possibility that
such mechanisms may not, in fact, ensure that AIs develop cooperative
tendencies; AIs may learn to favor behaviors such as nepotism, in-group
favoritism, as well as extortion and ruthlessness, the consequences of
which could be catastrophic to humanity, especially as AIs become more
proliferous and capable. Moreover, by drawing from bargaining theory, we
discussed the possibility that AIs may find it instrumentally rational
to pursue conflict—just as humans do—–even when bargaining to arrive at
a peaceful settlement is possible. Given that AIs may someday be able to
wield superhuman threats, due to vastly superior abilities for goal
preservation and longevity (among many others), it is imperative that we
understand what may drive future AIs to instigate conflict or cooperate
with other AIs and humans.<br />
AIs will not cooperate with humans or other AIs by default. By
identifying this stark reality, we underline the importance of aligning
the whole system, rather than merely individual components. Individually
aligning each AI agent to a desired goal is not sufficient to guarantee
macrobehavior in line with this goal. Undesirable outcomes can result
from system-level dynamics such as feedback loops, critical mass
requirements, emergence, and self-organization.</p>
<h1 id="conclusion">Conclusion</h1>
<p>This chapter has focused on multi-agent dynamics in biological and
social systems, and how these bear similarity to the dynamics which
govern AI development, adoption, and interaction. We have seen how these
natural dynamics can produce undesirable outcomes. Dynamics between
multiple humans or corporations generate “races” in corporate and
military settings. Dynamics between multiple AIs may generate
evolutionary pressure for immoral behaviors, particularly selfishness,
free-riding, deception, conflict, and extortion. Our future will likely
be one in which individual humans, corporations, and AI agents must all
engage with one another. We cannot address all the risks posed by AI
simply by focusing on the outcomes of agents acting in isolation. It is
an essential component of ensuring our safety, and a valuable future,
that we consider these multi-agent AI dynamics carefully. These dynamics
represent a common problem—clashes between individual and collective
interests. We must find innovative solutions to ensure that the
development and interaction of AI agents lead to beneficial outcomes for
all.</p>
