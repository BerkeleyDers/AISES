<h2 id="reducing-risks-of-tail-events-and-black-swans">Reducing Risks of
Tail Events and Black Swans</h2>
<p>In the last two sections, we have explored the implications for risk
management of tail events and black swans. Since risk estimation models
often break down in the face of tail events and black swans, it might
not be worth spending many resources trying to precisely calculate
risks. Such calculations might not be very informative or useful, and
might simply waste time, during which we would continue to be exposed to
risks of high-impact events while we are unprepared. Even if we lack
information about tail events and black swans, there are still measures
we can take to reduce the risks they present. We will now explore six
ideas to reduce risks from black swans, especially focused on risks from
AIs:</p>
<ol>
<li><p>Find potential black swans.</p></li>
<li><p>Incorporate safe design principles.</p></li>
<li><p>Improve systemic factors.</p></li>
<li><p>Reduce our exposure to risk by following the precautionary
principle.</p></li>
<li><p>Improve institutional decision-making.</p></li>
<li><p>Reduce moral hazard.</p></li>
</ol>
<p><strong>There are techniques for turning some black swans into known
unknowns.</strong> As discussed earlier, under our practical definition,
not all black swans are completely unpredictable, especially not for
people who have the relevant expertise. Ways of putting more black swans
on our radar include expanding our safety imagination, conducting stress
tests, and red-teaming <span class="citation"
data-cites="Marsden2017blackswan"></span>.<br />
Expanding our “safety imagination” can help us envision a wider range of
possibilities. We can do this by playing a game of “what if” to increase
the range of possible scenarios we can imagine unfolding. Brainstorming
sessions can also help to rapidly generate lots of new ideas about
potential failure modes in a system. We can identify and question our
assumptions–—about what the nature of a hazard will be, what might cause
it, and what procedures we will be able to follow to deal with it—–in
order to imagine a richer set of eventualities.<br />
Some HROs use a technique called horizon scanning, which involves
monitoring potential future threats and opportunities before they
arrive, to minimize the risk of unknown unknowns <span class="citation"
data-cites="boult2018horizon"></span>. AI systems could be used to
enhance horizon-scanning capabilities by simulating situations that
mirror the real world with a high degree of complexity. The simulations
might generate data that reveal potential black swan risks to be aware
of when deploying a new system. As well as conducting horizon scanning,
HROs also contemplate near-misses and speculate about how they might
have turned into catastrophes, so that lessons can be learned.<br />
“Red teams” can find more black swans by adopting a mindset of malicious
intent. Red teams should try to think of as many ways as they can to
misuse or sabotage the system. They can then challenge the organization
on how it would respond to such attacks. Finally, stress tests such as
dry-running hypothetical scenarios and evaluating how well the system
copes with them, and thinking about how it could be improved can improve
a system’s resilience to unexpected events.</p>
<p><strong>Incorporating safe design principles can improve general
safety.</strong> While it might be possible to find some black swans, it
would be difficult to ever ensure that there are no more unknown
unknowns posing a risk to us. Nevertheless, there are some actions that
robustly improve safety, even if we are not aware of all the risks.
Following the safe design principles listed earlier in this chapter can
be a good first step, with the caveat that we should think carefully
about which defense features are appropriate, and avoid too much
complexity. In particular, focusing on increasing the controllability of
the system might be a good idea. This can be done by adding loose
coupling into the system, by supporting human operators to notice
hazards and act on them early, and by devising negative feedback
mechanisms that will down-regulate processes if control is lost.<br />
Consider in detail the principle of least privilege. For one, it tells
us that we should be cautious about giving AIs too much power, to limit
the extent to which we are exposed to their tail risks. We might be
concerned that AIs become enmeshed within society with the capacity to
make big changes in the world when they do not need such access to
perform their assigned duties. Additionally, for particularly powerful
AI systems that have useful capabilities, it might be reasonable to keep
them relatively isolated from wider society, and accessible only to
verified individuals who have demonstrable and specific needs for such
AIs. In general, being conservative about if and how we unleash
technologies can reduce our exposure to black swans.</p>
<p><strong>Improving systemic factors can substantially reduce overall
risk.</strong> As we discussed, tackling systemic safety issues can be
more effective than focusing on details in complex systems. This can
reduce the risk of both foreseeable accidents and black swans. Raising
general awareness of risks associated with technologies can produce
social pressures, and bring organizations operating those technologies
under greater scrutiny. Developing and enforcing industry regulations
can help ensure organizations maintain appropriate safety standards, as
can encouraging best practices that improve safety culture. If there are
ways of reducing the safety costs, this can make it more likely that an
organization will adopt them, also improving general safety.<br />
Other systemic factors to pay attention to include competitive
pressures. These can undermine general safety by compelling management
and employees to cut corners, whether to increase rates of production or
to reach a goal before competitors. If there are ways of reducing these
pressures and encouraging organizations to prioritize safety, this could
substantially lessen overall risk.</p>
<p><strong>Reduce our exposure to tail risks by being cautious <span
class="citation" data-cites="Marsden2007blackswan"></span>.</strong> The
precautionary principle states that it is generally better to err on the
side of caution. In some situations, we can be extremely wrong and
things can still end up being fine; in others, we can be just slightly
wrong but suffer disastrous consequences. Some decisions require vastly
more caution than others: for instance, paraphrasing Richard Danzig, you
should not ‘need evidence’ that a gun is loaded to avoid playing Russian
roulette <span class="citation"
data-cites="danzig2018technology"></span>. Instead, you should need
evidence of safety.<br />
In situations where we are subject to the possibility of tail events and
black swans, this evidence might be impossible to find. One element of
good decision making is to exercise more caution than we would under
thin-tailed scenarios. In the case of new technologies such as AI
systems, this might mean not deploying them rashly on a large scale. We
should err on the side of caution instead.<br />
We must also be cautious while trying to solve our problems. For
example, while climate change poses a serious threat, many experts
believe it would be unwise to attempt to fix it quickly by rushing into
geoengineering solutions like spraying sulfur particles into the
atmosphere. There may be an urgent need to solve the problem, but we
should take care that we are not pursuing solutions that could cause
many other problems.</p>
<p><strong>Improve decision-making about AI.</strong> We can improve
decision-making about AI in two ways. Firstly, we can help policymakers
improve their decision-making processes, such as by developing better
methods of collecting, analyzing, and understanding information, as well
as improving their awareness of common cognitive biases to avoid. This
approach can help those in power to understand a situation more
thoroughly and improve the likelihood that they will make good
decisions.<br />
Besides improving the decision-making of those in power, another method
of improving decision making might be to change who has a say in
decisions, perhaps by including citizens in decision-making processes,
not only officials and scientists <span class="citation"
data-cites="Marsden2017blackswan"></span>. This reduces moral hazard by
including the stakeholders that have “skin in the game.” It can also
lead to better decisions in general due to the wisdom of crowds, the
phenomenon where crowds composed of diverse individuals make much better
decisions collectively than most members within it, when the conditions
are right.</p>
<p><strong>Reduce moral hazard.</strong> Lastly, we might want to
influence the incentives of researchers developing AI. Researchers might
currently be focused on increasing profits and reaching goals before
competitors, pursuing scientific curiosity and a desire for
technological progress, or developing the best capabilities in deep
learning models to find out what is possible. In this sense, these
researchers might be somewhat disconnected from the risks they could be
creating and the externalities they are imposing on the rest of society,
creating a moral hazard. Encouraging more consideration of the possible
risks, perhaps by making researchers liable for any consequences of the
technologies they develop, could therefore improve general safety.</p>
