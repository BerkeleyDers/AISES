<!-- Appendix B - Utility Functions -->

<h1 id="utility-and-utility-functions">B.1 Utility and Utility
Functions</h1>
<h2 id="fundamentals">B.1.1 Fundamentals</h2>
<p><strong>A utility function is a mathematical representation of
preferences.</strong> A utility function, <span
class="math inline"><em>u</em></span>, takes inputs like goods or
situations and outputs a value called <em>utility</em>. Utility is a
measure of how much an agent prefers goods and situations relative to
other goods and situations.<p>
Suppose we offer Alice some apples, bananas, and cherries. She might
have the following utility function for fruits:<p>
<span
class="math display"><em>u</em>(fruits) = 12<em>a</em> + 10<em>b</em> + 2<em>c</em>,</span>
where <span class="math inline"><em>a</em></span> is the number of
apples, <span class="math inline"><em>b</em></span> is the number of
bananas, and <span class="math inline"><em>c</em></span> is the number
of cherries that she consumes. Suppose Alice consumes no apples, one
banana, and five cherries. The amount of utility she gains from her
consumption is calculated as <span
class="math display"><em>u</em>(0 apples,1 banana,5 cherries) = (12⋅0) + (10⋅1) + (2⋅5) = 20.</span>
The output of this function is read as “20 units of utility” for short.
These units are arbitrary and reflect the level of Alice’s utility. We
can use utility functions to quantitatively represent preferences over
different combinations of goods and situations. For example, we can rank
Alice’s preferences over fruits as <span
class="math display">apple ≻ banana ≻ cherry,</span> where <span
class="math inline">≻</span> represents <em>preference</em>, such that
what comes before the symbol is preferred to what comes after it. This
follows from the fact that Alice gains 12 units from an apple, 10 units
from a banana, and 2 units from a cherry. The advantage of having a
utility function as opposed to just an explicit ranking of goods is that
we can directly infer information about more complex goods. For example,
we know <span class="math display"><em>u</em>(1 banana,5
cherries) = 20 &gt; <em>u</em>(1 apple) = 12 &gt; <em>u</em>(1
banana) = 10.</span> <strong>Utility functions, if accurate, reveal what
options agents would prefer and choose.</strong> If told to choose only
one of the three fruits, Alice would pick the apple, since it gives her
the most utility. Her preference follows from <em>rational choice
theory</em>, which proposes that individuals, acting in their own
self-interest, make decisions that maximize their self-interest. This
view is only an approximation to human behavior. In this chapter we will
discuss how rational choice theory is an imperfect but useful way to
model choices. We will also refer to individuals who behave in coherent
ways that help maximize utility as <em>agents</em>.</p>
<p><strong>We explore concepts about utility functions that are useful
for thinking about AIs, humans, and organizations like companies and
states.</strong> First, we introduce <em>Bernoulli utility
functions</em>, which are conventional utility functions that define
preferences over certain outcomes like the example above. We later
discuss <em>von Neumann-Morgenstern utility functions</em>, which extend
preferences to probabilistic situations, in which we cannot be sure
which outcome will occur. <em>Expected utility theory</em> suggests that
rationality is the ability to maximize preferences. We consider the
relevance of utility functions to <em>AI corrigibility</em>—the property
of being receptive to corrections—and see how this might be a source of
tail risk. Much of this chapter focuses on how utility functions help
understand and model agents’ <em>attitudes toward risk</em>. Finally, we
examine <em>non-expected utility theories</em>, which seek to rectify
some shortcomings of conventional expected utility theory when modeling
real-life behavior.</p>
<h2 id="motivations-for-learning-about-utility-functions">B.1.2 Motivations
for Learning About Utility Functions</h2>
<p><strong>Utility functions are a central concept in economics and
decision theory.</strong> Utility functions can be applied to a wide
range of problems and agents, from rats finding cheese in a maze to
humans making investment decisions to countries stockpiling nuclear
weapons. Conventional economic theory assumes that people are rational
and well-informed, and make decisions that maximize their self-interest,
as represented by their utility function. The view that individuals will
choose options that are likely to maximize their utility functions,
referred to as <em>expected utility theory</em>, has been the major
paradigm in real-world decision making since the Second World War <span
class="citation" data-cites="schoemaker1982expected">[1]</span>. It is
useful for modeling, predicting, and encouraging desired behavior in a
wide range of situations. However, as we will discuss, this view does
not perfectly capture reality, because individuals can often be
irrational, lack relevant knowledge, and frequently make mistakes.</p>
<p><strong>The objective of maximizing a utility function can cause
intelligence.</strong> The <em>reward hypothesis</em> suggests that the
objective of maximizing some reward is sufficient to drive behavior that
exhibits intelligent traits like learning, knowledge, perception, social
awareness, language, generalization, and more <span class="citation"
data-cites="silver2021reward">[2]</span>. The reward hypothesis implies
that artificial agents in rich environments with simple rewards could
develop sophisticated general intelligence. For example, an artificial
agent deployed with the goal of maximizing the number of successful food
deliveries may develop relevant geographical knowledge, an understanding
of how to move between destinations efficiently, and the ability to
perceive potential dangers. Therefore, the construction and properties
of the utility function that agents maximize are central to guiding
intelligent behavior.</p>
<p><strong>Certain artificial agents may be approximated as expected
utility maximizers.</strong> Some artificial intelligences are
agent-like. They are programmed to consider the potential outcomes of
different actions and to choose the option that is most likely to lead
to the optimal result. It is a reasonable approximation to say that many
artificial agents make choices that they predict will give them the
highest utility. For instance, in reinforcement learning (introduced in
the previous chapter), artificial agents explore their environment and
are rewarded for desirable behavior. These agents are explicitly
constructed to maximize reward functions, which strongly shape an
agent’s internal utility function, should it exist, and its
dispositions. This view of AI has implications for how we design and
evaluate these systems—we need to ensure that their value functions
promote human values. Utility functions can help us reason about the
behavior of AIs, as well as the behavior of powerful actors that direct
AIs, such as corporations or governments.</p>
<p><strong>Utility functions are a key concept in AI safety.</strong>
Utility functions come up explicitly and implicitly at various times
throughout this book, and are useful for understanding the behavior of
reward-maximizing agents, as well as humans and organizations involved
in the AI ecosystem. They will also come up in our chapter on Machine Ethics, when we
consider that some advanced AIs may have utility functions make up the
social welfare function they seek to increase. In the Collective Action Problems chapter, we will
continue our discussion of rational agents that seek to maximize their
own utility.</p>

<br>
<br>
<h3>References</h3>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-schoemaker1982expected" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] P.
Schoemaker, <span>“The expected utility model: Its variants, purposes,
evidence and limitations,”</span> <em>Journal of Economic
Literature</em>, vol. 20, pp. 529–63, Feb. 1982.</div>
</div>
<div id="ref-silver2021reward" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] D.
Silver, S. Singh, D. Precup, and R. S. Sutton, <span>“Reward is
enough,”</span> <em>Artificial Intelligence</em>, vol. 299, p. 103535,
2021, doi: <a
href="https://doi.org/10.1016/j.artint.2021.103535">https://doi.org/10.1016/j.artint.2021.103535</a>.</div>
</div>
</div>
